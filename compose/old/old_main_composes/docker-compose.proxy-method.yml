

x-common-hostname-aliases: &common-hostname-aliases
  - host.docker.internal:host-gateway  # doesn't exist with docker desktop, let's create it.
# Tailscale
  - boden-iphone:100.97.148.14
  - beatapostapita:100.99.45.35
  - micklethefickle:100.72.149.123
  - micklethefickle3:100.72.149.123
  - wizard:100.119.187.62
  - wizard-pc:100.119.187.62
# Warp
  - warp:${WARP_IPV4_ADDRESS:-10.76.128.97}
#  - aiostreams:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - comet:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - jackett:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - jackettio:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - localhost:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - mediafusion:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - prowlarr:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - stremio-jackett:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - zurg:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - mediaflow-proxy:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - deluge:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - stremio-web:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - stremio-https-server:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - stremio-fallback:${WARP_IPV4_ADDRESS:-10.76.128.97}
# Gluetun Premiumize NL
  - gluetun-premiumize-nl:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
  - transmission:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
  - qbittorrent:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
#  - riven:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
#  - riven-backend:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
#  - riven-db:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
#  - riven-frontend:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
  - rclone:${RCLONE_IPV4_ADDRESS:-10.76.128.111}

x-gluetun-hosts-workaround: &gluetun-hosts-workaround
  - host.docker.internal:host-gateway  # doesn't exist with docker desktop, let's create it.
# Tailscale
  - boden-iphone:100.97.148.14
  - beatapostapita:100.99.45.35
  - micklethefickle:100.72.149.123
  - micklethefickle3:100.72.149.123
  - wizard:100.119.187.62
  - wizard-pc:100.119.187.62
# Warp
  - warp:${WARP_IPV4_ADDRESS:-10.76.128.97}
#  - aiostreams:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - comet:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - jackett:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - jackettio:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - mediafusion:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - prowlarr:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - stremio-jackett:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - zurg:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - mediaflow-proxy:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - deluge:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - stremio-web:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - stremio-https-server:${WARP_IPV4_ADDRESS:-10.76.128.97}
  - stremio-fallback:${WARP_IPV4_ADDRESS:-10.76.128.97}
# Gluetun Premiumize NL
  - gluetun-premiumize-nl:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
  - transmission:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
  - qbittorrent:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
#  - riven:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
#  - riven-backend:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
#  - riven-db:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
#  - riven-frontend:${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
# Docker containers:
  - dash:${DASH_IPV4_ADDRESS:-10.76.128.23}
  - gpt-researcher:${GPT_RESEARCHER_IPV4_ADDRESS:-10.76.128.43}
  - gptr:${GPT_RESEARCHER_IPV4_ADDRESS:-10.76.128.43}
  - qdrant:${QDRANT_IPV4_ADDRESS:-10.76.128.44}
  - bolabaden-nextjs:${BOLABADEN_NEXTJS_IPV4_ADDRESS:-10.76.128.45}
  - lobechat:${LOBECHAT_IPV4_ADDRESS:-10.76.128.46}
  - mongodb:${MONGODB_IPV4_ADDRESS:-10.76.0.50}
  - code-demo:${CODE_DEMO_IPV4_ADDRESS:-10.76.128.80}
  - whoami:${WHOAMI_IPV4_ADDRESS:-10.76.128.81}
  - tinyauth:${TINYAUTH_IPV4_ADDRESS:-10.76.128.82}
  - watchtower:${WATCHTOWER_IPV4_ADDRESS:-10.76.128.83}
  - maintainerr:${MAINTAINERR_IPV4_ADDRESS:-10.76.128.84}
  - traefik:${TRAEFIK_IPV4_ADDRESS:-10.76.128.85}
  - speedtest:${SPEEDTEST_IPV4_ADDRESS:-10.76.128.86}
  - redis:${REDIS_IPV4_ADDRESS:-10.76.128.87}
  - homepage:${HOMEPAGE_IPV4_ADDRESS:-10.76.128.88}
  - dozzle:${DOZZLE_IPV4_ADDRESS:-10.76.128.89}
  - searxng:${SEARXNG_IPV4_ADDRESS:-10.76.128.90}
  - code-server:${CODE_SERVER_IPV4_ADDRESS:-10.76.128.92}
  - byparr:${FLARESOLVERR_IPV4_ADDRESS:-10.76.128.93}
  - flaresolverr:${FLARESOLVERR_IPV4_ADDRESS:-10.76.128.93}
  - nginx-auth:${NGINX_AUTH_IPV4_ADDRESS:-10.76.128.94}
  - plex:${PLEX_IPV4_ADDRESS:-10.76.128.95}
  - code:${CODE_DEMO_IPV4_ADDRESS:-10.76.128.96}
  - stremthru:${STREMTHRU_IPV4_ADDRESS:-10.76.128.99}
  - riven-frontend:${RIVEN_IPV4_ADDRESS:-10.76.128.100}
  - riven:${RIVEN_CORE_IPV4_ADDRESS:-10.76.128.101}
  - riven-backend:${RIVEN_CORE_IPV4_ADDRESS:-10.76.128.101}
  - riven-db:${RIVEN_DB_IPV4_ADDRESS:-10.76.128.102}
  - tautulli:${TAUTULLI_IPV4_ADDRESS:-10.76.128.103}
  - crowdsec:${CROWDSEC_IPV4_ADDRESS:-10.76.128.104}
  - gatus:${GATUS_IPV4_ADDRESS:-10.76.128.105}
  - homer:${HOMER_IPV4_ADDRESS:-10.76.0.106}
  - wizarr:${WIZARR_IPV4_ADDRESS:-10.76.128.107}
#  - zurg:${ZURG_IPV4_ADDRESS:-10.76.128.108}
  - plex-watchlist:${PLEX_WATCHLIST_IPV4_ADDRESS:-10.76.52.97}
  - realdebrid-account-monitor:${REALDEBRID_ACCOUNT_MONITOR_IPV4_ADDRESS:-10.76.128.109}
  - rclone:${RCLONE_IPV4_ADDRESS:-10.76.128.111}
  - plex-authentication:${PLEX_AUTHENTICATION_IPV4_ADDRESS:-10.76.128.112}
  - plex-request:${PLEX_REQUEST_IPV4_ADDRESS:-10.76.128.113}
  - jellyseerr:${JELLYSEERR_IPV4_ADDRESS:-10.76.128.115}
  - overseerr:${OVERSEERR_IPV4_ADDRESS:-10.76.128.116}
  - jellyfin:${JELLYFIN_IPV4_ADDRESS:-10.76.128.120}
  - radarr:${RADARR_IPV4_ADDRESS:-10.76.128.121}
  - sonarr:${SONARR_IPV4_ADDRESS:-10.76.128.122}
  - sonarr-anime:${SONARR_ANIME_IPV4_ADDRESS:-10.76.128.123}
  - firecrawl-api:${FIRECRAWL_API_IPV4_ADDRESS:-10.76.128.127}
  - firecrawl-worker:${FIRECRAWL_WORKER_IPV4_ADDRESS:-10.76.128.128}
  - firecrawl-playwright:${FIRECRAWL_PLAYWRIGHT_IPV4_ADDRESS:-10.76.128.130}
  - decluttarr:${DECLUTTARR_IPV4_ADDRESS:-10.76.128.21}
  - whisparr:${WHISPARR_IPV4_ADDRESS:-10.76.0.120}
  - aiostreams:${AIOSTREAMS_IPV4_ADDRESS:-10.76.128.200}
  - plex-repair:${PLEX_REPAIR_IPV4_ADDRESS:-10.76.0.115}
  - script-runner:${SCRIPT_RUNNER_IPV4_ADDRESS:-10.76.0.116}
  - recyclarr:${RECYCLARR_IPV4_ADDRESS:-10.76.0.117}
  - open-webui:${OPEN_WEBUI_IPV4_ADDRESS:-10.76.128.118}
  - autoscan:${AUTOSCAN_IPV4_ADDRESS:-10.76.0.119}
  - prometheus:${PROMETHEUS_IPV4_ADDRESS:-10.76.127.80}
#  - qbittorrent:${QB_IPV4_ADDRESS:-10.76.128.201}
  - litellm-postgres:${LITELLM_POSTGRES_IPV4_ADDRESS:-10.76.127.90}
  - litellm:${LITELLM_IPV4_ADDRESS:-10.76.126.90}
  - mcpo:${MCPO_IPV4_ADDRESS:-10.76.127.3}

configs:
  #### Prometheus config ####
  prometheus.yml:
    file: ./static-docker-configmaps/prometheus.yml

  alert.rules:
    file: ./static-docker-configmaps/alert.rules

  #### Gluetun config ####
  # Premiumize authentication file (shared across all Premiumize VPN endpoints)
  premiumize-auth.txt:
    content: |
      $PREMIUMIZE_CUSTOMER_ID
      $PREMIUMIZE_API_KEY

  # Gluetun auth configuration
  gluetun-auth-config.toml:
    file: ./static-docker-configmaps/premiumize_ovpn/gluetun-auth-config.toml

  # Premiumize CA certificate (shared across all Premiumize VPN endpoints)
  premiumize-ca.cert:
    file: ./static-docker-configmaps/premiumize_ovpn/premiumize-ca.cert

  vpn-nl.premiumize.me:
    file: ./static-docker-configmaps/premiumize_ovpn/vpn-nl.premiumize.me
  mediafusion-nginx-config:
    content: |
      user  nginx;
      worker_processes  1;

      events {
          worker_connections  1024;
      }

      http {
          include       mime.types;
          default_type  application/octet-stream;
          sendfile        on;
          keepalive_timeout  65;

          server {
              listen 443 ssl;
              server_name mediafusion.local;

              ssl_certificate     /etc/ssl/certs/mediafusion.local.pem;
              ssl_certificate_key /etc/ssl/private/mediafusion.local-key.pem;

              ssl_session_cache   shared:SSL:10m;
              ssl_session_timeout 10m;

              location / {
                  proxy_pass ${MEDIAFUSION_INTERNAL_URL:-http://mediafusion:8000};
                  proxy_set_header Host $$host;
                  proxy_set_header X-Real-IP $$remote_addr;
                  proxy_set_header X-Forwarded-For $$proxy_add_x_forwarded_for;
                  proxy_set_header X-Forwarded-Proto $$scheme;
              }
          }
      }
  nginx-proxy-config.conf:
    file: ./static-docker-configmaps/nginx-proxy-config.conf
  #######################################################################################
  # RcloneFM Configuration
  #######################################################################################
  rclone-rclone.conf:
    content: |
      [rdhttp]
      type = http
      url = https://my.real-debrid.com/$REALDEBRID_WEBDAV_PASSWORD/
      no_head = false
      no_slash = false

      [rdwebdav]
      type = webdav
      url = https://dav.real-debrid.com/
      vendor = other
      user = ${REALDEBRID_USER:-brunner56}
      pass = $REALDEBRID_RCLONE_OBSCURE_PASS

      [rdhttp2]
      type = http
      url = https://my.real-debrid.com/$REALDEBRID_WEBDAV_PASSWORD_2/
      no_head = false
      no_slash = false

      [rdwebdav2]
      type = webdav
      url = https://dav.real-debrid.com/
      vendor = other
      user = ${REALDEBRID_USER2:-null}
      pass = ${REALDEBRID_PASS2:-null}

      [zurg]
      type = webdav
      url = http://zurg:9999/dav
      vendor = other
      pacer_min_sleep = 0

      [zurghttp]
      type = http
      url = http://zurg:9999/http/
      no_head = false
      no_slash = false

      [zurg-remote]
      type = webdav
      url = https://zurg.$DOMAIN/dav
      vendor = other
      pacer_min_sleep = 0

      [zurghttp-remote]
      type = http
      url = https://zurg.$DOMAIN/http/
      no_head = false
      no_slash = false
  rclone-mounts.json:
    content: |
      [
        {
          "fs": "zurg:",
          "mountPoint": "${ZURG_MOUNT_PATH:-/mnt/remote/realdebrid}",
          "mainOpt": {
            "LogLevel": "INFO"
          },
          "LogLevel": "INFO",
          "mountOpt": {
            "GID": ${PGID:-988},
            "UID": ${PUID:-1001},
            "AllowNonEmpty": true,
            "AllowOther": true,
            "AttrTimeout": "87600h",
            "DirCacheTime": "60s",
            "DirPerms": "0777",
            "FilePerms": "0666",
            "PollInterval": "30s",
            "ExtraFlags": [
              "--config=/config/rclone/rclone.conf",
              "--log-level=INFO",
              "--log-file=/config/rclone/rclone.log"
            ]
          },
          "vfsOpt": {
            "BufferSize": "128M",
            "CacheMaxAge": "2m",
            "CacheMaxSize": "100G",
            "CacheMode": "full",
            "CachePollInterval": "30s",
            "ChunkSize": "2M",
            "ChunkSizeLimit": "64M",
            "DiskSpaceTotalSize": "1T",
            "FastFingerprint": true,
            "MinFreeSpace": "1G",
            "ReadAhead": "128M",
            "ReadWait": "40ms",
            "ExtraOptions": [
              "--vfs-refresh",
              "--transfers=16",
              "--checkers=16",
              "--multi-thread-streams=${CPU_COUNT:-4}",
              "--cache-dir=${ZURG_CACHE_DIR:-/mnt/remote/cache/zurg}"
            ]
          }
        }
      ]

  rclonefm-settings.js:
    content: |
      export const guiVersion = "0.4.0";
      export const rcloneSettings = {
          host: "https://rclonefm.$DOMAIN",
          // null if --rc-no-auth, otherwise what is set in --rc-user
          user: ${RCLONE_USER:-null},
          // null if --rc-no-auth, otherwise what is set in --rc-pass
          pass: ${RCLONE_PASS:-null},
          // null if there is no login_token in URL query parameters,
          // otherwise is set from there and takes over user/pass
          loginToken: ${RCLONE_LOGIN_TOKEN:-null}
      };
      export const asyncOperations = [
          "/sync/copy",
          "/sync/move",
          "/operations/purge",
          "/operations/copyfile",
          "/operations/movefile",
          "/operations/deletefile"
      ];
      export const remotes = {
          "storage": {
              "startingFolder": "${RCLONE_BASE_FOLDER:-/mnt/remote}",
              "canQueryDisk": true,
              "pathToQueryDisk": ""
          }
      };
      export const userSettings = {
          timerRefreshEnabled: true,
          timerRefreshView: 2,
          timerRefreshViewInterval: undefined,
          timerProcessQueue: 5,
          timerProcessQueueInterval: undefined
      };
  prowlarr_indexers.json:
    file: ./static-docker-configmaps/prowlarr_indexers.json
  prowlarr_indexer_proxy.json:
    content: |
      {
        "onHealthIssue": false,
        "supportsOnHealthIssue": false,
        "includeHealthWarnings": false,
        "name": "FlareSolverr",
        "fields": [
          {
            "name": "host",
            "value": "${FLARESOLVERR_INTERNAL_URL:-http://flaresolverr:8191}"
          },
          {
            "name": "requestTimeout",
            "value": 60
          }
        ],
        "implementationName": "FlareSolverr",
        "implementation": "FlareSolverr",
        "configContract": "FlareSolverrSettings",
        "infoLink": "https://wiki.servarr.com/prowlarr/supported#flaresolverr",
        "tags": [
          1
        ]
      }
  scraper_config.json:
    file: ./static-docker-configmaps/scraper_config.json

  litellm_config.yaml:
    file: ./static-docker-configmaps/litellm_config.yaml


  #######################################################################################
  # ZURG CONFIGURATION
  #######################################################################################
  zurg-cli_debrid_update.sh:
    file: ./static-docker-configmaps/zurg-cli_debrid_update.sh

  zurg-config.yml:
    content: |
      # Zurg configuration version
      zurg: v1

      # Get your token for the next line from https://real-debrid.com/apitoken
      token: "${REALDEBRID_TOKEN:?}"

      # additional download tokens - uncomment the lines below, and populate this with your additional tokens to auto-overcome bandwidth limitation issues
      download_tokens:
        - "$REALDEBRID_TOKEN_2"

      # basic functionality
      host: "[::]" # do not change this if you are running it inside a docker container
      port: 9999 # do not change this if you are running it inside a docker container
      concurrent_workers: 32
      check_for_changes_every_secs: 30

      # misc configs
      retain_folder_name_extension: true # if true, zurg won't modify the filenames from real-debrid
      retain_rd_torrent_name: true # if true, it will strictly follow RD API torrent name property w/c should make this more compatible with rdt-client
      auto_delete_rar_torrents: false # if true, zurg will delete unstreamable rar files (these torrents will always be compressed in a rar archive no matter what files you select)
      use_download_cache: false # if true, during zurg initialization, it will fetch all downloads to unrestrict links faster

      enable_repair: true
      repair_every_mins: 60

      ###on_library_update: sh plex_update.sh "$@"
      ignore_renames: true

      # do a RD speed test on every start
      cache_network_test_results: false

      # network configs
      network_buffer_size: 1048576 # 1 MiB
      serve_from_rclone: false
      verify_download_link: true # if true, zurg will check if the link is truly streamable; only relevant if serve_from_rclone is set to true (as it already does this all the time if serve_from_rclone is false)
      force_ipv6: false # force connect to real-debrid ipv6 addresses
      rate_limit_sleep_secs: 6 # wait time after getting a 429 from Real-Debrid API
      realdebrid_timeout_secs: 60 # api timeout
      retries_until_failed: 5 # api failures until considered failed
      get_torrents_count: 5000

      rar_action: extract

      addl_playable_extensions:
      - avi
      - flac
      - m4b
      - mkv
      - mp3
      - mp4

      # List of directory definitions and their filtering rules
      directories:
        audiobooks:
          group_order: 5
          group: media
          filters:
            - and:
              - is_music: true
              - media_info_duration_gte: 600

        music:
          group_order: 10
          group: media
          filters:
            - is_music: true

        anime:
          group_order: 15
          group: media
          filters:
            - regex: /\b[a-fA-F0-9]{8}\b/
            - any_file_inside_regex: /\b[a-fA-F0-9]{8}\b/

        shows:
          group_order: 20
          group: media
          filters:
            - has_episodes: true

        movies:
          group_order: 25
          group: media
          only_show_the_biggest_file: true
          filters:
            - regex: /.*/

      restrict_repair_to_cached: true
  zurg-setup.sh:
    content: |
      #!/bin/bash
      # Disable native auth (we do it with traefik instead when necessary)
      sed -i  "s/^username:/#username/" /config/config.yml
      sed -i  "s/^password:/#password/" /config/config.yml
      # Socks via WARP
      # # Enable proxy only if gluetun vars are not present
      # if [[ ! -z "$$ZURG_WARP_ENABLED" ]]; then
      #   echo "not enabling warp, we may no longer need it"
      #   # grep proxy /config/config.yml || echo 'proxy: socks5://127.0.0.1:1080' >> /config/config.yml
      #   # sed -i  "s/^#proxy:/proxy/" /config/config.yml
      # else
      #   # remove proxy if found
      #   sed -i  "s/^proxy:/#proxy/" /config/config.yml
      # fi
      # revert to repairing every hour
      grep repair_every_mins /config/config.yml \
        || echo 'repair_every_mins: 60' >> /config/config.yml
      # Enable rar extraction if it's not already there
      grep rar_action /config/config.yml || echo 'rar_action: extract' >> /config/config.yml
      # Check for changes every 10 sec - this is too aggressive on large libraries
      # sed -i  "s/check_for_changes_every_secs:.*/check_for_changes_every_secs: 10/" /config/config.yml
      # don't repair uncached
      grep restrict_repair_to_cached /config/config.yml \
        || echo 'restrict_repair_to_cached: true' >> /config/config.yml
      # # Don't serve from rclone - leave this up to the user now
      # sed -i  "s/serve_from_rclone:.*/serve_from_rclone: false/" /config/config.yml
      # Disable plex_update.sh unless user uses plexdebrid
        # we don't need the script
        sed -i  "s/on_library_update/#on_library_update/" /config/config.yml
      # Enable repair everywhere
      # disabled on 13 nov 2024 to debug
      # sed -i  "s/enable_repair:.*/enable_repair: true/" /config/config.yml
  #######################################################################################
  # Tooling Scripts
  #######################################################################################
  tooling-clean-up-dns-on-termination.sh:
    content: |
      #!/bin/bash
      function cleanup_dns_on_shutdown {
          echo "Received SIGTERM, cleaning up DNS entries..."
          source /tooling-scripts/cloudflare-scripts.sh
          DNS_DOMAIN=elfhosted.com
          delete_cloudflare_record $$DNS_DOMAIN $$ELF_TENANT_NAME-plex.$$DNS_DOMAIN CNAME
      }
      # When we terminate, delete the DNS record
      trap cleanup_dns_on_shutdown SIGTERM
      # Hang around doing nothing until terminated
      while true
      do
          echo "Waiting for SIGTERM to remove DNS entry"
          sleep infinity
      done
  tooling-cloudflare-scripts.sh:
    file: ./static-docker-configmaps/tooling-cloudflare-scripts.sh
  tooling-update-dns-on-init.sh:
    file: ./static-docker-configmaps/tooling-update-dns-on-init.sh
  #######################################################################################
  # Homer Configuration
  #######################################################################################
  homer-config.yml:
    content: |
      # Homepage configuration
      # See https://fontawesome.com/icons for icons options

      # Optional: Use external configuration file.
      # Using this will ignore remaining config in this file
      # externalConfig: https://$DOMAIN/server-luci/config.yaml
      subtitle: "You are ..."
      title: "ElfHosted!"
      documentTitle: "$DOMAIN - $TS_HOSTNAME" # Customize the browser tab text
      # Alternatively a fa icon can be provided:
      # icon: "fas fa-wand-magic-sparkles"
      logo: https://elfhosted.com/images/logo.png
      header: true # Set to false to hide the header
      footer: Version 1.417.0
      columns: "3" # "auto" or number (must be a factor of 12: 1, 2, 3, 4, 6, 12)
      connectivityCheck: false # whether you want to display a message when the apps are not accessible anymore (VPN disconnected for example)
      # Optional theming
      # theme: default # 'default' or one of the themes available in 'src/assets/themes'.
      # Optional custom stylesheet
      # Will load custom CSS files. Especially useful for custom icon sets.
      #stylesheet:
      #  - "assets/custom.css"
      # Here is the exhaustive list of customization parameters
      # However all value are optional and will fallback to default if not set.
      # if you want to change only some of the colors, feel free to remove all unused key.
      colors:
        light:
          highlight-primary: "#222526"
          highlight-secondary: "#4285F4"
          highlight-hover: "#5A95F5"
          background: "#222526"
          card-background: "#FFFFFF"
          text: "#363636"
          text-header: "#FFFFFF"
          text-title: "#303030"
          text-subtitle: "#424242"
          card-shadow: "rgba(0, 0, 0, 0.1)"
          link-hover: "#363636"
          background-image: /assets/backgrounds/background-light.jpg
        dark:
          highlight-primary: "#222526"
          highlight-secondary: "#4285F4"
          highlight-hover: "#5A95F5"
          background: "#222526"
          card-background: "#2B2B2B"
          text: "#EAEAEA"
          text-header: "#FFFFFF"
          text-title: "#FAFAFA"
          text-subtitle: "#F5F5F5"
          card-shadow: "rgba(0, 0, 0, 0.4)"
          link-hover: "#FFDD57"
          background-image: /assets/backgrounds/background-dark.jpg
      # Optional message
      message:
        url: "https://$DOMAIN/assets/message/message.json"
        # mapping:
        #   content: 'message'
        #   style: 'style'
        #   title: 'title'
        refreshInterval: 10000
        icon: "fas fa-door-open"
      # Optional navbar
      # links: [] # Allows for navbar (dark mode, layout, and search) without any links
      links:
        - name: "Apps"
          icon: "fas fa-couch"
          url: "#config"
        - name: "Health"
          icon: "fas fa-heartbeat"
          url: "https://homer-health.$DOMAIN"
          target: "_blank" # optional html tag target attribute
      # Services
      # First level array represents a group.
      # Leave only a "items" key if not using group (group name, icon & tagstyle are optional, section separation will not be displayed).
      services:
        - name: "Consume Media"
          icon: "fas fa-couch"
          # A path to an image can also be provided. Note that icon take precedence if both icon and logo are set.
          # logo: "path/to/logo"
          items:
            - name: "Plex"
              logo: "assets/png/plex.png"
              # Alternatively a fa icon can be provided:
              # icon: "fab fa-photo-video"
              subtitle: "Watch Movies/TV"
              tag: "public"
              tagstyle: "is-danger"
              url: "https://plex.$DOMAIN"
              target: "_blank"  # Optional html tag target attribute
            - name: "Jellyfin"
              logo: "assets/png/jellyfin.png"
              subtitle: "Watch Movies/TV"
              tag: "public"
              tagstyle: "is-danger"
              url: "https://jellyfin.$DOMAIN"
              target: "_blank"  # Optional html tag target attribute
        - name: "Manage Media"
          icon: "fas fa-photo-video"
          items:
            - name: "Wizarr"
              icon: "fas fa-hat-wizard"
              subtitle: "Manage Plex / Emby / Jellyfin Accounts / Invites"
              tag: "public"
              tagstyle: "is-danger"
              url: "https://wizarr.$DOMAIN"
              target: "_blank"  # Optional html tag target attribute
            - name: "Riven"
              icon: "fas fa-mountain"
              subtitle: "All-in-one Debrid Media Management"
              tag: "protected"
              tagstyle: "is-success"
              url: "https://riven.$DOMAIN"
              target: "_blank"  # Optional html tag target attribute
            - name: "Riven (backend)"
              icon: "fas fa-mountain"
              subtitle: "Geek out over Riven's backend"
              tag: "protected"
              tagstyle: "is-success"
              url: "https://riven-backend.$DOMAIN"
              target: "_blank"  # Optional html tag target attribute
        - name: "Download Media"
          icon: "fas fa-cloud-arrow-down"
          items:
            - name: "Zurg"
              logo: "assets/png/real-debrid.png"
              subtitle: "Manage RealDebrid Mount"
              tag: "protected"
              tagstyle: "is-success"
              url: "https://zurg.$DOMAIN/http/"
              target: "_blank"  # Optional html tag target attribute
        - name: "Tools"
          icon: "fas fa-toolbox"
          items:
            - name: "FileBrowser"
              logo: "assets/png/filebrowser.png"
              subtitle: "Manage Files"
              tag: "protected"
              tagstyle: "is-success"
              url: "https://filebrowser.$DOMAIN"
              target: "_blank"  # Optional html tag target attribute
            - name: "code-server"
              icon: "fas fa-terminal"
              subtitle: "Enter the code-server.. 🪄"
              tag: "protected"
              tagstyle: "is-success"
              url: "https://code-server.$DOMAIN"
              target: "_blank"  # Optional html tag target attribute
        - name: "Rclone"
          icon: "fas fa-truck"
          items:
            - name: "Rclone Remotes"
              logo: "assets/png/rclone.png"
              subtitle: "Rclone remotes"
              tag: "protected"
              tagstyle: "is-success"
              url: "https://filebrowser.$DOMAIN/files/config/rclone/rclone.conf"
              target: "_blank"  # Optional html tag target attribute
            - name: "Rclone UI"
              logo: "assets/png/rclone.png"
              subtitle: "Rclone UI"
              tag: "protected"
              tagstyle: "is-success"
              url: "https://rclone.$DOMAIN"
              target: "_blank"  # Optional html tag target attribute
  custom.css:
    file: ./static-docker-configmaps/custom.css
  #######################################################################################
  # Plex Configuration
  #######################################################################################
  plex-Preferences.xml:
    content: |
      <?xml version="1.0" encoding="utf-8"?>
      <Preferences
        AcceptedEULA="1"
        AnonymousMachineIdentifier="a02f47cd-9111-41fb-837d-dc6be25ba6d3"
        CertificateUUID="56d3167058f948448c4564df78239440"
        CertificateVersion="3"
        customConnections="https://plex.$DOMAIN:443"
        DlnaEnabled="1"
        FriendlyName="$DOMAIN"
        GlobalMusicVideoPathMigrated="1"
        LanguageInCloud="1"
        LastAutomaticMappedPort="0"
        MachineIdentifier="a36c4347-359e-408a-93a4-04948a773c57"
        ManualPortMappingMode="1"
        MergedRecentlyAdded="0"
        MetricsEpoch="1"
        OldestPreviousVersion="legacy"
        PlexOnlineHome="1"
        PlexOnlineMail="${PLEX_EMAIL:-boden.crouch@gmail.com}"
        PlexOnlineToken="${PLEX_API_KEY:-tzn6xvA4d8ygGXyR4sND}"
        PlexOnlineUsername="${PLEX_USERNAME:-brunner56}"
        ProcessedMachineIdentifier="${PLEX_MACHINE_IDENTIFIER:-508f1e66f9ebdecae95ab71afb36f051aed8bccc}"
        PublishServerOnPlexOnlineKey="1"
        ScheduledLibraryUpdateInterval="86400"
        ScheduledLibraryUpdatesEnabled="1"
        TranscoderTempDirectory="/transcode"/>
  #######################################################################################
  # Traefik Configuration
  #######################################################################################
  crowdsec-appsec.yaml:
    content: |
      appsec_config: crowdsecurity/appsec-default
      labels:
        type: appsec
      listen_addr: 0.0.0.0:${CROWDSEC_APPSEC_PORT:-7422}
      source: appsec

  traefik-dynamic.yaml:
    content: |
      # yaml-language-server: $$schema=https://www.schemastore.org/traefik-v3-file-provider.json
      http:
        middlewares:
          # This middleware allows services to be embedded in an iframe on the main domain
          # and its subdomains.
          allow-iframe-from-domain:
            headers:
              # X-Frame-Options is an older header, but still good for compatibility.
              # We are removing it because the Content-Security-Policy header is more modern and powerful.
              # Sending both can have unexpected results in some browsers.
              customResponseHeaders:
                X-Frame-Options: "" # An empty value removes the header
                # This is the modern replacement for X-Frame-Options.
                # frame-ancestors specifies which domains are allowed to embed this content in an iframe.
                # 'self' allows the service to be framed by pages from its own origin.
                # You may need to adjust this depending on your dashboard's final URL.
                Content-Security-Policy: "frame-ancestors 'self' https://*.$DOMAIN https://$DOMAIN"
          crowdsec:
            plugin:
              crowdsec-bouncer-traefik-plugin:
                enabled: true
                defaultDecisionSeconds: 60
                crowdsecMode: live
                crowdsecAppsecEnabled: false  # <--- Here you can enable appsec waf
                crowdsecAppsecHost: ${CROWDSEC_APPSEC_INTERNAL_HOST:-crowdsec:7422}
                crowdsecAppsecFailureBlock: true
                crowdsecAppsecUnreachableBlock: true
                crowdsecLapiKey: $CROWDSEC_BOUNCER_API_TOKEN
                crowdsecLapiHost: ${CROWDSEC_INTERNAL_HOST:-crowdsec:8080}
                crowdsecLapiScheme: http
                crowdsecLapiTLSInsecureVerify: false
                forwardedHeadersTrustedIPs: *local-ips
                clientTrustedIPs: *local-ips

  traefik-logrotate:
    content: |
      # place this example code at /etc/logrotate.d/traefik on your docker host server
      # please adjust the custom file path below, where your traefik logs are stored
      # please adjust the below traefik container name to send the USR1 signal for log rotation

      compress
      ${CONFIG_PATH:-./volumes}/traefik/logs/*.log {
        size 20M
        daily
        rotate 14
        missingok
        notifempty postrotate
        docker kill --signal="USR1" traefik-${STACK_NAME:-my-media-stack}
        endscript
      }

  acquis.yaml:
    content: |
      filenames:
        - /var/log/auth.log
        - /var/log/syslog
      labels:
        type: syslog
      ---
      poll_without_inotify: false
      filenames:
        - /var/log/traefik/*.log
      labels:
        type: traefik
      ---
      listen_addr: 0.0.0.0:${CROWDSEC_APPSEC_PORT:-7422}
      appsec_config: crowdsecurity/appsec-default
      name: myAppSecComponent
      source: appsec
      labels:
        type: appsec

  #######################################################################################
  # Mattermost Backup Configuration
  #######################################################################################
  mattermost-backup-daily:
    content: |
      #!/bin/sh
      now=$$(date +"%s_%Y-%m-%d")
      /usr/bin/mysqldump --opt -h $$MYSQL_HOST -u $$MYSQL_USER -p $$MYSQL_PASSWORD $$MYSQL_DATABASE > "/backup/$$now_$$MYSQL_DATABASE.sql"

  #######################################################################################
  # Recyclarr Configuration
  #######################################################################################
  recyclarr-recyclarr.yaml:
    content: |
      # yaml-language-server: $$schema=https://raw.githubusercontent.com/recyclarr/recyclarr/master/schemas/config-schema.json

      radarr:
        radarr_main:
          base_url: ${RADARR_INTERNAL_URL:-http://radarr:7878}
          api_key: $RADARR_API_KEY
          delete_old_custom_formats: true
          quality_definition:
            type: movie
            preferred_ratio: 0.5
          include:
            - template: radarr-quality-definition-hd-bluray-web
            - template: radarr-custom-formats-hd-bluray-web
            - template: radarr-custom-formats-audio
            - template: radarr-custom-formats-hdr
            - template: radarr-custom-formats-misc
            - template: radarr-custom-formats-movie-versions
            - template: radarr-custom-formats-streaming
            - template: radarr-custom-formats-unwanted
          quality_profiles:
            - name: HD Bluray + WEB
              reset_unmatched_scores: true
              min_format_score: 0
              upgrade_until_score: 10000
              qualities:
                - Bluray-1080p
                - WEBDL-1080p
                - WEBRip-1080p
                - Bluray-720p
                - WEBDL-720p
                - WEBRip-720p
              custom_formats:
                - trash_ids:
                    - ed27ebfef2f323e964fb1f61391bcb35  # HD Bluray Tier 01
                    - c20c8647f2746a1f4c4262b0fbbeeeae  # HD Bluray Tier 02
                    - 5608c71bcebba0a5e666223bae8c9227  # HD Bluray Tier 03
                    - c20f169ef63c5f40c2def54abaf4438e  # WEB Tier 01
                    - 403816d65392c79236dcb6dd591aeda4  # WEB Tier 02
                    - af94e0fe497124d1f9ce732069ec8c3b  # WEB Tier 03
                    - e7718d7a3ce595f289bfee26adc178f5  # Repack/Proper
                    - ae43b294509409a6a13919dedd4764c4  # Repack2
                    - 5caaaa1c08c1742aa4342d8c4cc463f2  # Repack3
                    - ed38b889b31be83fda192888e2286d83  # BR-DISK
                    - e6886871085226c3da1830830146846c  # Generated Dynamic HDR
                    - 90a6f9a284dff5103f6346090e6280c8  # LQ
                    - e204b80c87be9497a8a6eaff48f72905  # LQ (Release Title)
                    - dc98083864ea246d05a42df0d05f81cc  # x265 (HD)
                    - b8cd450cbfa689c0259a01d9e29ba3d6  # 3D
                    - 0a3f082873eb454bde444150b70253cc  # Extras
                    - 712d74cd88bceb883ee32f773656b1f5  # Sing-Along Versions
                    - cae4ca30163749b891686f95532519bd  # AV1
                  score: auto
          custom_formats:
            # Advanced Audio
            - trash_ids:
                - 496f355514737f7d83bf7aa4d24f8169 # TrueHD ATMOS
                - 2f22d89048b01681dde8afe203bf2e95 # DTS X
                - 417804f7f2c4308c1f4c5d380d4c4475 # ATMOS (undefined)
                - 1af239278386be2919e1bcee0bde047e # DD+ ATMOS
                - 3cafb66171b47f226146a0770576870f # TrueHD
                - dcf3ec6938fa32445f590a4da84256cd # DTS-HD MA
                - a570d4a0e56a2874b64e5bfa55202a1b # FLAC
                - e7c2fcae07cbada050a0af3357491d7b # PCM
                - 8e109e50e0a0b83a5098b056e13bf6db # DTS-HD HRA
                - 185f1dd7264c4562b9022d963ac37424 # DD+
                - f9f847ac70a0af62ea4a08280b859636 # DTS-ES
                - 1c1a4c5e823891c75bc50380a6866f73 # DTS
                - 240770601cc226190c367ef59aba7463 # AAC
                - c2998bd0d90ed5621d8df281e839436e # DD
              assign_scores_to:
                - name: HD Bluray + WEB

      sonarr:
        sonarr_main:
          base_url: ${SONARR_INTERNAL_URL:-http://sonarr:8989}
          api_key: ${SONARR_API_KEY:?}
          delete_old_custom_formats: true
          quality_definition:
            type: series
          include:
            - template: sonarr-quality-definition-series
            - template: sonarr-v4-quality-profile-web-1080p
            - template: sonarr-v4-custom-formats-web-1080p
            - template: sonarr-v4-custom-formats-hd-bluray
            - template: sonarr-v4-custom-formats-audio
            - template: sonarr-v4-custom-formats-hdr
            - template: sonarr-v4-custom-formats-misc
            - template: sonarr-v4-custom-formats-unwanted
          quality_profiles:
            - name: WEB-1080p
              reset_unmatched_scores: true
              min_format_score: 0
              upgrade_until_score: 10000
              qualities:
                - WEBDL-1080p
                - WEBRip-1080p
                - Bluray-1080p
                - Bluray-720p
                - WEBDL-720p
                - WEBRip-720p
              custom_formats:
                - trash_ids:
                    - 85c61753df5da1fb2aab6f2a47426b09 # BR-DISK
                    - 9c11cd3f07101cdba90a2d81cf0e56b4 # LQ
                    - e2315f990da2e2cbfc9fa5b7a6fcfe48 # LQ (Release Title)
                    - 47435ece6b99a0b477caf360e79ba0bb # x265 (HD)
                    - fbcb31d8dabd2a319072b84fc0b7249c # Extras
                  score: auto
          custom_formats:
            # Advanced Audio
            - trash_ids:
                - 496f355514737f7d83bf7aa4d24f8169 # TrueHD ATMOS
                - 2f22d89048b01681dde8afe203bf2e95 # DTS X
                - 417804f7f2c4308c1f4c5d380d4c4475 # ATMOS (undefined)
                - 1af239278386be2919e1bcee0bde047e # DD+ ATMOS
                - 3cafb66171b47f226146a0770576870f # TrueHD
                - dcf3ec6938fa32445f590a4da84256cd # DTS-HD MA
                - a570d4a0e56a2874b64e5bfa55202a1b # FLAC
                - e7c2fcae07cbada050a0af3357491d7b # PCM
                - 8e109e50e0a0b83a5098b056e13bf6db # DTS-HD HRA
                - 185f1dd7264c4562b9022d963ac37424 # DD+
                - f9f847ac70a0af62ea4a08280b859636 # DTS-ES
                - 1c1a4c5e823891c75bc50380a6866f73 # DTS
                - 240770601cc226190c367ef59aba7463 # AAC
                - c2998bd0d90ed5621d8df281e839436e # DD
              assign_scores_to:
                - name: WEB-1080p

  recyclarr-settings.yaml:
    content: |
      # yaml-language-server: $$schema=https://raw.githubusercontent.com/recyclarr/recyclarr/master/schemas/settings-schema.json

      # Edit this file to customize the behavior of Recyclarr beyond its defaults
      # For the settings file reference guide, visit the link to the wiki below:
      # https://recyclarr.dev/wiki/yaml/settings-reference/

      notifications:
        verbosity: normal # Options: normal, detailed, minimal
        apprise:
          mode: stateless
          base_url: ${APPRISE_BASE_URL:-http://apprise:8000}
          urls:
            - ${APPRISE_URL:-https://discord.com/api/webhooks/secret}

      repositories:
        config_templates:
            clone_url: ${CONFIG_TEMPLATES_REPO:-https://github.com/recyclarr/config-templates.git}
            branch: ${CONFIG_TEMPLATES_BRANCH:-master}
        trash_guides:
            clone_url: ${TRASH_GUIDES_REPO:-https://github.com/TRaSH-/Guides.git}
            branch: ${TRASH_GUIDES_BRANCH:-master}

  #########################################################
  # Notifiarr
  #########################################################
  notifiarrConfig:
    content: |-
      ###############################################
      # Notifiarr Client Example Configuration File #
      # Created by Notifiarr v0.4.4   @ 232801T0734 #
      ###############################################
      ## This API key must be copied from your notifiarr.com account.
      api_key = "$NOTIFIARR_API_KEY"

      ## Setting a UI password enables the human accessible web GUI. Must be at least 9 characters.
      ## The default username is admin; change it by setting ui_password to "username:password"
      ## Set to "webauth" to disable the login form and use only proxy authentication. See upstreams, below.
      ## Your auth proxy must pass the x-webauth-user header if you set this to "webauth".
      ## You may also set a custom auth header by setting to "webauth:<header>" e.g. "webauth:remote-user"
      ## Disable auth by setting this to "noauth". Not recommended. Requires "upstreams" being set.
      ui_password = "webauth"

      ## The ip:port to listen on for incoming HTTP requests. 0.0.0.0 means all/any IP and is recommended!
      ## You may use "127.0.0.1:5454" to listen only on localhost; good if using a local proxy.
      ## This is used to receive Plex webhooks and Media Request commands.
      ##
      bind_addr = "0.0.0.0:${NOTIFIARR_PORT:-5454}"

      ## This application can update itself on Windows systems.
      ## Set this to "daily" to check GitHub every day for updates.
      ## You may also set it to a Go duration like "12h" or "72h".
      ## THIS ONLY WORKS ON WINDOWS
      auto_update = "daily"
      ## Quiet makes the app not log anything to output.
      ## Recommend setting log files if you make the app quiet.
      ## This is always true on Windows and macOS app.
      ## Log files are automatically written on those platforms.
      ##
      quiet = false

      ## Debug prints more data and json payloads. This increases application memory usage.
      debug = false

      max_body = 0  # Maximum body size for debug logs. 0 = no limit.
      host_id = "notifiarr.$DOMAIN"

      ## All API paths start with /api. This does not affect incoming /plex webhooks.
      ## Change it to /somethingelse/api by setting urlbase to "/somethingelse"
      ##
      urlbase = "/"

      ## Allowed upstream networks. Networks here are allowed to send two special headers:
      ## (1) x-forwarded-for (2) x-webauth-user
      ## The first header sets the IPs in logs.
      ## The second header allows an auth proxy to set a logged-in username. Be careful.
      ##
      ## Set this to your reverse proxy server's IP or network. If you leave off the mask,
      ## then /32 or /128 is assumed depending on IP version. Empty by default. Example:
      ##
      upstreams = [ "${TAILSCALE_CIDR:-100.64.0.0/10}" ] # for ElfHosted's Traefik pods

      ## If you provide a cert and key file (pem) paths, this app will listen with SSL/TLS.
      ## Uncomment both lines and add valid file paths. Make sure this app can read them.
      ##
      #ssl_key_file  = '/path/to/cert.key'
      #ssl_cert_file = '/path/to/cert.key'
      ## If you set these, logs will be written to these files.
      ## If blank on windows or macOS, log file paths are chosen for you.
      log_file = '/logs/notifiarr.log'
      http_log = '/logs/notifiarr.http.log'

      ##
      ## Set this to the number of megabytes to rotate files.
      log_file_mb = 100

      ##
      ## How many files to keep? 0 = all.
      log_files = 5

      ##
      ## Unix file mode for new log files. Umask also affects this.
      ## Missing, blank or 0 uses default of 0600. Permissive is 0644. Ignored by Windows.
      file_mode = "0600"

      ## Web server and website timeout.
      ##
      timeout = "1m"

      ## This application can integrate with apt on Debian-based OSes.
      ## Set apt to true to enable this integration. A true setting causes
      ## notifiarr to relay apt package install/update hooks to notifiarr.com.
      ##
      apt = false

      ## Setting serial to true makes the app use fewer threads when polling apps.
      ## This spreads CPU usage out and uses a bit less memory.
      serial = false

      ## Retries controls how many times to retry requests to notifiarr.com.
      ## Sometimes cloudflare returns a 521, and this mitigates those problems.
      ## Setting this to 0 will take the default of 4. Use 1 to disable retrying.
      retries = 4

      ##################
      # Starr Settings #
      ##################
      ## The API keys are specific to the app. Get it from Settings -> General.
      ## Configurations for unused apps are harmless. Set URL and API key for
      ## apps you have and want to make requests to using Media Bot.
      ## See the Service Checks section below for information about setting the names.
      ##
      ## Examples follow. UNCOMMENT (REMOVE #), AT MINIMUM: [[header]], url, api_key
      ## Setting any application timeout to "-1s" will disable that application.
      [[lidarr]]
      name     = "Lidarr | $DOMAIN" # Set a name to enable checks of your service.
      url      = "${LIDARR_INTERNAL_URL:-http://lidarr:8686}"
      api_key  = "$LIDARR_API_KEY"

      [[prowlarr]]
      name     = "Prowlarr | $DOMAIN" # Set a name to enable checks of your service.
      url      = "${PROWLARR_INTERNAL_URL:-http://prowlarr:9696}"
      api_key  = "$PROWLARR_API_KEY"

      [[radarr]]
      name      = "Radarr | $DOMAIN" # Set a name to enable checks of your service.
      url       = "${RADARR_INTERNAL_URL:-http://radarr:7878}"
      api_key   = "$RADARR_API_KEY"

      [[readarr]]
      name      = "Readarr | $DOMAIN" # Set a name to enable checks of your service.
      url       = "${READARR_INTERNAL_URL:-http://readarr:8787}"
      api_key   = "$READARR_API_KEY"

      [[sonarr]]
      name      = "Sonarr | $DOMAIN"  # Set a name to enable checks of your service.
      url       = "${SONARR_INTERNAL_URL:-http://sonarr:8989}"
      api_key   = "$SONARR_API_KEY"

      # Download Client Configs (below) are used for dashboard state and service checks.
      [[deluge]]
      name     = "Deluge | $DOMAIN"  # Set a name to enable checks of your service.
      url      = "${DELUGE_INTERNAL_URL:-http://deluge:8112}"
      password = "${DELUGE_PASSWORD:-deluge}"

      [[qbit]]
      name     = "Qbittorrent | $DOMAIN"  # Set a name to enable checks of your service.
      url      = "${QBITTORRENT_INTERNAL_URL:-http://qbittorrent:8084}"
      user     = "${QBITTORRENT_USERNAME:-admin}"
      pass     = "${QBITTORRENT_PASSWORD:-adminadmin}"

      #################
      # Plex Settings #
      #################
      ## Find your token: https://support.plex.tv/articles/204059436-finding-an-authentication-token-x-plex-token/
      ##
      [plex]
      url     = "http://plex:32400/" # Your plex URL
      token   = "${PLEX_TOKEN:?}" # your plex token; get this from a web inspector
      #####################
      # Tautulli Settings #
      #####################
      # Enables email=>username map. Set a name to enable service checks.
      # Must uncomment [tautulli], 'api_key' and 'url' at a minimum.
      [tautulli]
        name    = "Tautulli | $DOMAIN" # only set a name to enable service checks.
        url     = "${TAUTULLI_INTERNAL_URL:-http://tautulli:8181/}" # Your Tautulli URL
        api_key = "$TAUTULLI_API_KEY" # your tautulli api key; get this from settings
      ##################
      # MySQL Snapshot #
      ##################
      # Enables MySQL process list in snapshot output.
      # Adding a name to a server enables TCP service checks.
      # Example Grant:
      # GRANT PROCESS ON *.* to 'notifiarr'@'localhost'
      #[[snapshot.mysql]]
      #name = "MySQL | $DOMAIN" # only set a name to enable service checks.
      #host = "mysql:3306"
      #user = "notifiarr"
      #pass = "$NOTIFIARR_MYSQL_PASSWORD"
      ###################
      # Nvidia Snapshot #
      ###################
      # The app will automatically collect Nvidia data if nvidia-smi is present.
      # Use the settings below to disable Nvidia GPU collection, or restrict collection to only specific Bus IDs.
      # SMI Path is found automatically if left blank. Set it to path to nvidia-smi (nvidia-smi.exe on Windows).
      #[snapshot.nvidia]
      #disabled = true
      #smi_path = ''''''
      #bus_ids  = []
      ##################
      # Service Checks #
      ##################
      ## This application performs service checks on configured services at the specified interval.
      ## The service states are sent to Notifiarr.com. Failed services generate a notification.
      ## Setting names on Starr apps (above) enables service checks for that app.
      ## Setting the Interval to "-1s" (Disabled in UI) will disable service checks on that named instance.
      ## Use the [[service]] directive to add more service checks. Example below.
      [services]
        disabled = false  # Setting this to true disables all service checking routines.
        parallel = 1     # How many services to check concurrently. 1 should be enough.
        interval = "10m" # How often to send service states to Notifiarr.com. Minimum = 5m.
        log_file = '/tmp/notifiarr.services.log'    # Service Check logs go to the app log by default. Change that by setting a services.log file here. # TODO: Change to /logs/notifiarr.services.log
      ## Uncomment the following section to create a service check on a URL or IP:port.
      ## You may include as many [[service]] sections as you have services to check.
      ## Do not add Radarr, Sonarr, Readarr, Prowlarr, or Lidarr here! Add a name to enable their checks.
      ##
      ## Example with comments follows.
      #[[service]]
      #  name     = "MyServer"          # name must be unique
      #  type     = "http"              # type can be "http" or "tcp"
      #  check    = 'http://127.0.0.1/'  # url for 'http', host/IP:port for 'tcp'
      #  expect   = "200"               # return code to expect (for http only)
      #  timeout  = "10s"               # how long to wait for tcp or http checks.
      #  interval = "5m"                # how often to check this service.
      ## Another example. Remember to uncomment [[service]] if you use this!
      ##
      [[service]]
        name    = "Bazarr"
        type    = "http"
        check   = 'http://${BAZARR_INTERNAL_URL:-http://bazarr:6767}/series/'
        expect  = "200"
        timeout = "10s"
      ######################
      # File & Log Watcher #
      ######################
      ## Tail a log file, regex match lines, and send notifications.
      ## Example:
      #[[watch_file]]
      #  path  = '/var/log/system.log'
      #  skip  = '''error'''
      #  regex = '''[Ee]rror'''
      #  poll  = false
      #  pipe  = false
      #  must_exist = false
      #  log_match  = true
      ###################
      # Custom Commands #
      ###################
      ## Run and trigger custom commands.
      ## Commands may have required arguments thet can be passed in when the command is run.
      ## These use the format ({regex}) - a regular expression wrapped by curly braces and parens.
      ## The example below allows a user to run any combination of ls -la on /usr, /home, or /tmp:
      ## command = "/bin/ls ({-la|-al|-l|-a}) ({/usr|/home|/tmp})"
      ##
      ## Full Example (remove the leading # hashes to use it):
      #[[command]]
      #  name    = 'some-notifiarr-syslog-thing'
      #  command = '/var/log/system.log'
      #  shell   = false
      #  log     = true
      #  notify  = true
      #  timeout = "10s"

  #######################################################################################
  # Blackhole Script Configuration
  #######################################################################################

  # Container management script
  manage-containers.sh:
    file: ./static-docker-configmaps/manage-containers.sh

  # Symlink cleanup script
  symlink-cleanup.sh:
    content: |
      #!/bin/bash
      # Cleanup broken symlinks

      echo "Cleaning up broken symlinks..."

      # Find and remove broken symlinks in media directories
      find /mnt/media -type l -exec test ! -e {} \; -print -delete
      find /mnt/symlinks -type l -exec test ! -e {} \; -print -delete

      echo "Symlink cleanup complete"

  # Backup configuration script
  backup-configs.sh:
    content: |
      #!/bin/bash
      # Backup configuration files

      BACKUP_DIR="/mnt/local/backups/$(date +%Y%m%d_%H%M%S)"
      CONFIG_PATH="${CONFIG_PATH:-./volumes}"

      echo "Creating backup directory: $BACKUP_DIR"
      mkdir -p "$BACKUP_DIR"

      echo "Backing up configuration files..."
      if [ -d "$CONFIG_PATH" ]; then
        cp -rv "$CONFIG_PATH" "$BACKUP_DIR/"
        echo "Backup completed: $BACKUP_DIR"
      else
        echo "Config directory not found: $CONFIG_PATH"
        exit 1
      fi

      # Keep only last 7 days of backups
      find /mnt/local/backups -type d -mtime +7 -exec rm -rf {} + 2>/dev/null || true

      echo "Backup process complete"
  plex_update.sh:
    file: ./static-docker-configmaps/plex_update.sh
  i2pd.conf:
    content: |
      # Minimal i2pd config
      [http]
      enabled = true
      address = 0.0.0.0
      port = 7070

      [logging]
      destination = stdout
      level = info
  tunnels.conf:
    content: ""
  watcher.py:
    content: |
      #!/usr/bin/env python3
      import os
      import time
      from pathlib import Path

      import docker

      TUNNELS_FILE = "/app/config/tunnels.conf"
      KEYS_DIR = "/app/keys"
      I2PD_CONTAINER = "i2pd"
      CHECK_INTERVAL = 10

      client = docker.from_env()

      def get_running_containers():
          return [c for c in client.containers.list() if c.name != I2PD_CONTAINER]

      def generate_tunnel_entry(name, ip, port, key_file):
          return f"""
      [{name}]
      type = http
      host = {ip}
      port = {port}
      keys = {key_file}
      inbound.length = 2
      outbound.length = 2
      """

      def get_container_ip(container):
          return container.attrs["NetworkSettings"]["Networks"]["i2pnet"]["IPAddress"]

      def generate_tunnels():
          entries = []
          containers = get_running_containers()
          for c in containers:
              name = c.name
              ip = get_container_ip(c)
              port = 80  # Default web port; change if needed
              key_file = f"{name}.dat"
              entry = generate_tunnel_entry(name, ip, port, key_file)
              entries.append(entry)
          return "\n".join(entries)

      def write_tunnels_config(config):
          with open(TUNNELS_FILE, "w") as f:
              f.write(config)

      def reload_i2pd():
          i2pd = client.containers.get(I2PD_CONTAINER)
          i2pd.exec_run("pkill -HUP i2pd")  # Send SIGHUP to reload tunnels

      def ensure_key_files(containers):
          Path(KEYS_DIR).mkdir(parents=True, exist_ok=True)
          for c in containers:
              key_path = os.path.join(KEYS_DIR, f"{c.name}.dat")
              if not os.path.exists(key_path):
                  # Force i2pd to generate keys by creating dummy config and reloading
                  with open(TUNNELS_FILE, "a") as f:
                      f.write(generate_tunnel_entry(c.name, "127.0.0.1", 65535, f"{c.name}.dat"))
                  reload_i2pd()
                  print(f"Generated key for {c.name}")
                  time.sleep(3)  # Allow i2pd to generate keys

      def main():
          print("Starting I2P tunnel watcher...")
          while True:
              try:
                  containers = get_running_containers()
                  ensure_key_files(containers)
                  config = generate_tunnels()
                  write_tunnels_config(config)
                  reload_i2pd()
                  print("Updated tunnels.conf and reloaded i2pd.")
              except Exception as e:
                  print(f"Error: {e}")
              time.sleep(CHECK_INTERVAL)

      if __name__ == "__main__":
          main()


x-common-env: &common-env
  TZ: ${TZ:-America/Chicago}
  PUID: ${PUID:-1001}
  PGID: ${PGID:-988}
  UMASK: ${UMASK:-002}

x-common-uidgid: &common-uidgid
  user: ${PUID:-1001}:${PGID:-988}

services:
  riven-frontend:
    # 🔹🔹 Riven Frontend 🔹🔹
    depends_on:
      riven:
        condition: service_healthy
    image: spoked/riven-frontend:latest
    container_name: riven-frontend
    hostname: riven-frontend
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${RIVEN_IPV4_ADDRESS:-10.76.128.100}
    volumes:
      - ${CONFIG_PATH:-./volumes}/riven/config:/riven/config
      - ${CONFIG_PATH:-./volumes}/riven/tmp:/tmp
    environment:
      <<: *common-env
    tty: true
    labels:
      traefik.enable: "true"
      # Riven Frontend
      traefik.http.routers.riven-frontend.service: riven-frontend
      traefik.http.routers.riven-frontend.middlewares: nginx-auth@file
      traefik.http.routers.riven-frontend.rule: Host(`riven.$DOMAIN`) || Host(`riven.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.riven-frontend.loadbalancer.server.port: 3000
      homepage.group: Media Management
      homepage.name: Riven
      homepage.icon: riven.png
      homepage.href: https://riven.$DOMAIN/
      homepage.description: Modern frontend for Plex Debrid
    healthcheck:  # docker exec riven-frontend ls -la /bin /usr/bin | grep -E 'curl|wget|nc|telnet|http|python|ncat|nmap'
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:3000 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: always
  riven:
    # 🔹🔹 Riven 🔹🔹
    depends_on:
      riven-db:
        condition: service_healthy
    image: docker.io/spoked/riven:latest
    container_name: riven
    hostname: riven
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${RIVEN_CORE_IPV4_ADDRESS:-10.76.128.101}
    tty: true
    volumes:
      - ${CONFIG_PATH:-./volumes}/riven/data:/riven/data
      - ${ZURG_MOUNT_PATH:-/mnt/remote/realdebrid}:${ZURG_MOUNT_PATH:-/mnt/remote/realdebrid}:rshared
      - ${RIVEN_SYMLINK_LIBRARY_PATH:-/mnt/symlinks}:${RIVEN_SYMLINK_LIBRARY_PATH:-/mnt/symlinks}
#    configs:
#      - source: riven-settings.json
#        target: /riven/data/settings.json
#        mode: 0777
    environment:
      - PUID=${PUID:-1001}
      - PGID=${PGID:-988}
      - TZ=${TZ:-America/Chicago}
      - UMASK=${UMASK:-002}
      - DIALECT=postgres
      - RIVEN_DATABASE_HOST=postgresql+psycopg2://postgres:postgres@riven-db:5432/riven
      # This is used to force the use of the env variables all the time!
      # By default, this is false, which only new setting files will be created
      # using the env variables you specified.
      - RIVEN_FORCE_ENV=true
      - SETTINGS_FILENAME=settings.json  # This is used to specify settings filename. By default it's settings.json.
      - SKIP_TRAKT_CACHE=false  # Whether to use trakt cache or not.
      - HARD_RESET=false  # This will reset the database and recreate all tables, and then exit after running!
      - REPAIR_SYMLINKS=false  # This will attempt to fix broken symlinks in the library, and then exit after running!
      - API_KEY=${RIVEN_API_KEY:-3e0be750b16ff363727d05d18ba364fd}  # Manual api key, must be 32 characters long
      - TRAKT_API_CLIENT_ID=  # Custom trakt client id used by trakt indexer
      # This is the number of workers to use for reindexing symlinks after a database reset.
      # More workers = faster symlinking but uses more memory.
      # For lower end machines, stick to around 1-3.
      # For higher end machines, you can do as many as you want, or set it to the number of cores.
      # If you are experiencing indexing issues after a database reset, try lowering this to 1.
      - SYMLINK_MAX_WORKERS=2
      - RIVEN_VERSION=0.21.21
      - RIVEN_DEBUG=false
      - RIVEN_LOG=true
      - RIVEN_FORCE_REFRESH=false
      - RIVEN_MAP_METADATA=true
      - RIVEN_TRACEMALLOC=false
      - RIVEN_SYMLINK_RCLONE_PATH=${RIVEN_SYMLINK_RCLONE_PATH:-/mnt/remote/realdebrid/__all__}  # The path to your rclone dir. If using Zurg, this should point to the `__all__` directory.
      - RIVEN_SYMLINK_LIBRARY_PATH=${RIVEN_SYMLINK_LIBRARY_PATH:-/mnt/symlinks}  # The path you want your symlinks to be placed in.
      - RIVEN_SYMLINK_SEPARATE_ANIME_DIRS=true  # If you want to separate your anime symlinks into a separate directory.
      - RIVEN_SYMLINK_REPAIR_SYMLINKS=true  # If you want to repair broken symlinks after a database reset.
      - RIVEN_SYMLINK_REPAIR_INTERVAL=6.0  # The interval at which to check for broken symlinks, in hours.
      - RIVEN_UPDATER_INTERVAL=120
      - RIVEN_UPDATERS_PLEX_TOKEN=$PLEX_TOKEN
      - RIVEN_PLEX_TOKEN=$PLEX_TOKEN
      - RIVEN_PLEX_ENABLED=true
      - RIVEN_UPDATERS_PLEX_ENABLED=true
      - RIVEN_PLEX_URL=${PLEX_INTERNAL_URL:-http://plex:32400}
      - RIVEN_JELLYFIN_ENABLED=false
      - RIVEN_JELLYFIN_API_KEY=$JELLYFIN_API_KEY
      - RIVEN_JELLYFIN_URL=${JELLYFIN_INTERNAL_URL:-http://jellyfin:8096}
      - RIVEN_EMBY_ENABLED=false
      - RIVEN_EMBY_API_KEY=$EMBY_API_KEY
      - RIVEN_EMBY_URL=${EMBY_INTERNAL_URL:-http://emby:8096}
      - RIVEN_DOWNLOADERS_VIDEO_EXTENSIONS=["avi","mkv","mp4"]
      - RIVEN_DOWNLOADERS_PROXY_URL=http://warp:1080
      - RIVEN_DOWNLOADERS_REAL_DEBRID_ENABLED=true
      - RIVEN_DOWNLOADERS_REAL_DEBRID_API_KEY=${REALDEBRID_TOKEN:?}
      - RIVEN_DOWNLOADERS_ALL_DEBRID_ENABLED=false
      - RIVEN_DOWNLOADERS_ALL_DEBRID_API_KEY=$ALL_DEBRID_API_KEY
      - RIVEN_DOWNLOADERS_TORBOX_ENABLED=false
      - RIVEN_DOWNLOADERS_TORBOX_API_KEY=$TORBOX_API_KEY
      - RIVEN_CONTENT_OVERSEERR_ENABLED=true
      - RIVEN_CONTENT_OVERSEERR_URL=${OVERSEERR_INTERNAL_URL:-http://overseerr:5055}
      - RIVEN_CONTENT_OVERSEERR_API_KEY=$OVERSEERR_API_KEY
      - RIVEN_CONTENT_OVERSEERR_USE_WEBHOOK=false
      - RIVEN_CONTENT_OVERSEERR_UPDATE_INTERVAL=30
      - RIVEN_CONTENT_PLEX_WATCHLIST_ENABLED=true
      - RIVEN_CONTENT_PLEX_WATCHLIST_RSS=${RIVEN_CONTENT_PLEX_WATCHLIST_RSS:-[]}
      - RIVEN_CONTENT_PLEX_WATCHLIST_UPDATE_INTERVAL=60
      - RIVEN_CONTENT_MDBLIST_ENABLED=false
      - RIVEN_CONTENT_MDBLIST_API_KEY=$MDBLIST_API_KEY
      - RIVEN_CONTENT_MDBLIST_UPDATE_INTERVAL=300
      - RIVEN_CONTENT_MDBLIST_LISTS=[]
      - RIVEN_CONTENT_LISTRR_ENABLED=false
      - RIVEN_CONTENT_LISTRR_API_KEY=$LISTRR_API_KEY
      - RIVEN_CONTENT_LISTRR_UPDATE_INTERVAL=300
      - RIVEN_CONTENT_LISTRR_MOVIE_LISTS=[]
      - RIVEN_CONTENT_LISTRR_SHOW_LISTS=[]
      - RIVEN_CONTENT_TRAKT_ENABLED=true
      - RIVEN_CONTENT_TRAKT_API_KEY=$TRAKT_API_KEY
      - RIVEN_CONTENT_TRAKT_UPDATE_INTERVAL=3600
      # The interval to retry scraping an item after it's failed, in hours.
      - RIVEN_SCRAPING_AFTER_2=2
      - RIVEN_SCRAPING_AFTER_5=6
      - RIVEN_SCRAPING_AFTER_10=24
      - RIVEN_SCRAPING_PARSE_DEBUG=false  # This will enable debug mode for the scraper, which will log more information about the scraping process.
      # This will enable aliases for the scraper, which will allow for more accurate scraping.
      # This can sometimes cause issues if there are alias titles that aren't exactly the same as the title in the library.
      - RIVEN_SCRAPING_ENABLE_ALIASES=true
      - RIVEN_SCRAPING_TORRENTIO_ENABLED=true
      - RIVEN_SCRAPING_TORRENTIO_FILTER=sort=qualitysize%7Cqualityfilter=480p,scr,cam,unknown
      - RIVEN_SCRAPING_TORRENTIO_URL=${TORRENTIO_URL:-https://torrentio.strem.fun}
      - RIVEN_SCRAPING_TORRENTIO_TIMEOUT=30
      - RIVEN_SCRAPING_TORRENTIO_RATELIMIT=true
      - RIVEN_SCRAPING_TORRENTIO_PROXY_URL=http://warp:1080
      - RIVEN_SCRAPING_KNIGHTCRAWLER_ENABLED=true
      - RIVEN_SCRAPING_KNIGHTCRAWLER_FILTER=sort=qualitysize%7Cqualityfilter=480p,scr,cam,unknown
      - RIVEN_SCRAPING_KNIGHTCRAWLER_URL=${KNIGHTCRAWLER_URL:-https://knightcrawler.elfhosted.com}
      - RIVEN_SCRAPING_KNIGHTCRAWLER_TIMEOUT=30
      - RIVEN_SCRAPING_KNIGHTCRAWLER_RATELIMIT=${KNIGHTCRAWLER_RATELIMIT:-true}
      - RIVEN_SCRAPING_JACKETT_ENABLED=true
      - RIVEN_SCRAPING_JACKETT_URL=${JACKETT_INTERNAL_URL:-http://jackett:9117}
      - RIVEN_SCRAPING_JACKETT_API_KEY=$JACKETT_API_KEY
      - RIVEN_SCRAPING_JACKETT_TIMEOUT=10
      - RIVEN_SCRAPING_JACKETT_RATELIMIT=true
      - RIVEN_SCRAPING_PROWLARR_ENABLED=false
      - RIVEN_SCRAPING_PROWLARR_URL=${PROWLARR_INTERNAL_URL:-http://prowlarr:9696}
      - RIVEN_SCRAPING_PROWLARR_API_KEY=$PROWLARR_API_KEY
      - RIVEN_SCRAPING_PROWLARR_TIMEOUT=10
      - RIVEN_SCRAPING_PROWLARR_RATELIMIT=true
      - RIVEN_SCRAPING_PROWLARR_LIMITER_SECONDS=60
      - RIVEN_SCRAPING_ORIONOID_ENABLED=false
      - RIVEN_SCRAPING_ORIONOID_API_KEY=$ORIONOID_API_KEY
      - RIVEN_SCRAPING_ORIONOID_CACHED_RESULTS_ONLY=true
      - RIVEN_SCRAPING_ORIONOID_PARAMETERS_VIDEO3D=false  # See the Orionoid API docs for more information on these parameters.
      - RIVEN_SCRAPING_ORIONOID_PARAMETERS_VIDEOQUALITY=sd_hd8k  # See the Orionoid API docs for more information on these parameters.
      - RIVEN_SCRAPING_ORIONOID_PARAMETERS_LIMITCOUNT=5  # See the Orionoid API docs for more information on these parameters.
      - RIVEN_SCRAPING_ORIONOID_TIMEOUT=10
      - RIVEN_SCRAPING_ORIONOID_RATELIMIT=true
      - RIVEN_SCRAPING_MEDIAFUSION_ENABLED=true
      - RIVEN_SCRAPING_MEDIAFUSION_URL=${MEDIAFUSION_INTERNAL_URL:-http://mediafusion:8000}
      - RIVEN_SCRAPING_MEDIAFUSION_TIMEOUT=10
      - RIVEN_SCRAPING_MEDIAFUSION_RATELIMIT=${MEDIAFUSION_RATELIMIT:-false}
      - RIVEN_SCRAPING_MEDIAFUSION_CATALOGS=prowlarr_streams,torrentio_streams
      - RIVEN_SCRAPING_ZILEAN_ENABLED=true
      - RIVEN_SCRAPING_ZILEAN_URL=${ZILEAN_URL:-http://zilean.elfhosted.com}
      - RIVEN_SCRAPING_ZILEAN_TIMEOUT=30
      - RIVEN_SCRAPING_ZILEAN_RATELIMIT=${MEDIAFUSION_RATELIMIT:-false}
      - RIVEN_SCRAPING_COMET_ENABLED=true
      - RIVEN_SCRAPING_COMET_URL=http://comet:${COMET_PORT:-2020}
      - RIVEN_SCRAPING_COMET_INDEXERS=${COMET_INDEXERS:-animetosho,anirena,eztv,limetorrents,nyaasi,solidtorrents,thepiratebay,torlock,yts}
      - RIVEN_SCRAPING_COMET_TIMEOUT=30
      - RIVEN_SCRAPING_COMET_RATELIMIT=${COMET_RATELIMIT:-false}
      - RIVEN_RANKING_PROFILE=default
      - RIVEN_RANKING_REQUIRE=[]
      - RIVEN_RANKING_EXCLUDE=[]
      - RIVEN_RANKING_PREFERRED=[]
      - RIVEN_RANKING_RESOLUTIONS_2160P=false
      - RIVEN_RANKING_RESOLUTIONS_1080P=true
      - RIVEN_RANKING_RESOLUTIONS_720P=true
      - RIVEN_RANKING_RESOLUTIONS_480P=false
      - RIVEN_RANKING_RESOLUTIONS_360P=false
      - RIVEN_RANKING_RESOLUTIONS_UNKNOWN=true
      - RIVEN_RANKING_OPTIONS_TITLE_SIMILARITY=0.85
      - RIVEN_RANKING_OPTIONS_REMOVE_ALL_TRASH=true
      - RIVEN_RANKING_OPTIONS_REMOVE_RANKS_UNDER=-10000
      - RIVEN_RANKING_OPTIONS_REMOVE_UNKNOWN_LANGUAGES=false
      - RIVEN_RANKING_OPTIONS_ALLOW_ENGLISH_IN_LANGUAGES=true
      - RIVEN_RANKING_LANGUAGES_REQUIRED=["en"]
      - RIVEN_RANKING_LANGUAGES_EXCLUDE=[]
      - RIVEN_RANKING_LANGUAGES_PREFERRED=["en"]
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_AV1_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_AV1_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_AV1_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_AVC_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_AVC_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_AVC_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_BLURAY_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_BLURAY_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_BLURAY_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_DVD_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_DVD_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_DVD_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_HDTV_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_HDTV_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_HDTV_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_HEVC_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_HEVC_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_HEVC_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_MPEG_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_MPEG_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_MPEG_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_REMUX_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_REMUX_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_REMUX_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_VHS_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_VHS_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_VHS_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_WEB_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_WEB_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_WEB_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_WEBDL_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_WEBDL_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_WEBDL_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_WEBMUX_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_WEBMUX_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_WEBMUX_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_XVID_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_XVID_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_QUALITY_XVID_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_BDRIP_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_BDRIP_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_BDRIP_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_BRRIP_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_BRRIP_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_BRRIP_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_DVDRIP_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_DVDRIP_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_DVDRIP_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_HDRIP_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_HDRIP_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_HDRIP_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_PPVRIP_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_PPVRIP_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_PPVRIP_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_SATRIP_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_SATRIP_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_SATRIP_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_TVRIP_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_TVRIP_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_TVRIP_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_UHDRIP_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_UHDRIP_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_UHDRIP_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_VHSRIP_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_VHSRIP_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_VHSRIP_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_WEBDLRIP_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_WEBDLRIP_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_WEBDLRIP_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_WEBRIP_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_WEBRIP_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_RIPS_WEBRIP_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_10BIT_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_10BIT_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_10BIT_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_DOLBY_VISION_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_DOLBY_VISION_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_DOLBY_VISION_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_HDR_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_HDR_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_HDR_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_HDR10PLUS_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_HDR10PLUS_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_HDR10PLUS_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_SDR_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_SDR_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_HDR_SDR_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_AAC_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_AAC_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_AAC_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_AC3_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_AC3_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_AC3_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_ATMOS_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_ATMOS_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_ATMOS_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_DOLBY_DIGITAL_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_DOLBY_DIGITAL_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_DOLBY_DIGITAL_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_DOLBY_DIGITAL_PLUS_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_DOLBY_DIGITAL_PLUS_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_DOLBY_DIGITAL_PLUS_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_DTS_LOSSY_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_DTS_LOSSY_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_DTS_LOSSY_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_DTS_LOSSLESS_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_DTS_LOSSLESS_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_DTS_LOSSLESS_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_EAC3_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_EAC3_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_EAC3_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_FLAC_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_FLAC_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_FLAC_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_MONO_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_MONO_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_MONO_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_MP3_FETCH=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_MP3_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_MP3_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_STEREO_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_STEREO_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_STEREO_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_SURROUND_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_SURROUND_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_SURROUND_RANK=0
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_TRUEHD_FETCH=true
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_TRUEHD_USE_CUSTOM_RANK=false
      - RIVEN_RANKING_CUSTOM_RANKS_AUDIO_TRUEHD_RANK=0
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.riven.middlewares: nginx-auth@file
      traefik.http.routers.riven.rule: Host(`riven-backend.$DOMAIN`) || Host(`riven-backend.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.riven.loadbalancer.server.port: 8080
    healthcheck:  # docker exec riven ls -la /bin /usr/bin | grep -E 'curl|wget|nc|telnet|http|python|ncat|nmap'
      test: ["CMD-SHELL", "curl -s http://127.0.0.1:8080 >/dev/null || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 10
    cpu_shares: 10
    mem_reservation: 20M
    deploy:
      resources:
        limits:
          cpus: 1
          memory: 1G
        reservations:
          cpus: 0.5
          memory: 20M
    restart: always

  riven-db:
    # 🔹🔹 Riven DB 🔹🔹
    image: postgres:16.3-alpine3.20
    container_name: riven-db
    hostname: riven-db
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${RIVEN_DB_IPV4_ADDRESS:-10.76.128.102}
    volumes:
      - ${CONFIG_PATH:-./volumes}/riven/pgdata:${RIVEN_PGDATA:-/var/lib/postgresql/data/pgdata}
    environment:
      <<: *common-env
      PGDATA: ${RIVEN_PGDATA:-/var/lib/postgresql/data/pgdata}
      POSTGRES_USER: ${RIVEN_DB_USER:-postgres}
      POSTGRES_PASSWORD: ${RIVEN_DB_PASSWORD:-postgres}
      POSTGRES_DB: ${RIVEN_DB_NAME:-riven}
    labels:
      traefik.enable: "true"
      traefik.http.routers.riven-db.middlewares: nginx-auth@file
      traefik.http.routers.riven-db.rule: Host(`riven-db.$DOMAIN`) || Host(`riven-db.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.riven-db.loadbalancer.server.port: 5432
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 10s
      timeout: 5s
      retries: 5
    cpu_shares: 10
    mem_reservation: 6M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 6M
        limits:
          cpus: 1
          memory: 4G
    restart: always

  ## Zilean (optional scraper service) (https://ipromknight.github.io/zilean/getting-started.html)
  zilean:
    profiles:
      - extras
    depends_on:
      riven-db:
        condition: service_healthy
    image: ipromknight/zilean:latest
    container_name: zilean
    hostname: zilean
    extra_hosts: *common-hostname-aliases
    networks:
      - publicnet
    ports:
      - "8181:8181"
    volumes:
      - /mnt/remote/realdebrid/zilean_data:/app/data
      - ${CONFIG_PATH:-./volumes}/zilean/tmp:/tmp
    environment:
      # You may have to create the zilean database manually with the following command:
      # docker exec -it riven-db createdb -U postgres -W zilean
      Zilean__Database__ConnectionString: "Host=riven-db;Port=5432;Database=zilean;Username=postgres;Password=postgres"
    healthcheck:
      test: ["CMD-SHELL", "curl --connect-timeout 10 --silent --show-error --fail http://localhost:8181/healthchecks/ping"]
      timeout: 60s
      interval: 30s
      retries: 10
    restart: unless-stopped
  decluttarr:
    # 🔹🔹 Decluttarr 🔹🔹  https://github.com/manimatter/decluttarr
    # Watches radarr, sonarr, lidarr, readarr and whisparr download queues and removes downloads if they become stalled or no longer needed.
    profiles:
      - extras
    image: ghcr.io/manimatter/decluttarr
    container_name: decluttarr
    hostname: decluttarr
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${DECLUTTARR_IPV4_ADDRESS:-10.76.128.21}
    <<: *common-uidgid
    environment:
      <<: *common-env
      FAILED_IMPORT_MESSAGE_PATTERNS: '[ "Not a Custom Format upgrade for existing", "Not an upgrade for existing" ]'
      IGNORED_DOWNLOAD_CLIENTS: '["emulerr"]'
      LIDARR_KEY: $LIDARR_API_KEY
      LIDARR_URL: http://lidarr:${LIDARR_PORT:-8686}
      LOG_LEVEL: VERBOSE
      MIN_DOWNLOAD_SPEED: 100 # 100 KB/s
      NO_STALLED_REMOVAL_QBIT_TAG: Don't Kill
      PERMITTED_ATTEMPTS: 3
      QBITTORRENT_PASSWORD: ${QBITTORRENT_PASSWORD:-adminadmin}
      QBITTORRENT_URL: https://qbittorrent.$DOMAIN
      QBITTORRENT_USERNAME: ${QBITTORRENT_USERNAME:-admin}
      RADARR_KEY: $RADARR_API_KEY
      RADARR_URL: ${RADARR_INTERNAL_URL:-http://radarr:7878}
      READARR_KEY: $READARR_API_KEY
      READARR_URL: ${READARR_INTERNAL_URL:-http://readarr:8787}
      REMOVE_FAILED_IMPORTS: true
      REMOVE_FAILED: false
      REMOVE_METADATA_MISSING: false
      REMOVE_MISSING_FILES: false
      REMOVE_ORPHANS: true
      REMOVE_SLOW: true
      REMOVE_STALLED: true
      REMOVE_TIMER: 10 # 10 seconds
      REMOVE_UNMONITORED: false
      SONARR_KEY: $SONARR_API_KEY
      SONARR_URL: ${SONARR_INTERNAL_URL:-http://sonarr:8989}
      SSL_VERIFICATION: false
      TEST_RUN: false
      WHISPARR_KEY: $WHISPARR_API_KEY
      WHISPARR_URL: ${WHISPARR_INTERNAL_URL:-http://whisparr:6969}
      RUN_PERIODIC_RESCANS: '{
        "RADARR": {"MISSING": true, "CUTOFF_UNMET": true, "MAX_CONCURRENT_SCANS": 3, "MIN_DAYS_BEFORE_RESCAN": 1},
        "SONARR": {"MISSING": true, "CUTOFF_UNMET": true, "MAX_CONCURRENT_SCANS": 3, "MIN_DAYS_BEFORE_RESCAN": 1}
        }'
    cpu_shares: 10
    mem_reservation: 64M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 64M
        limits:
          cpus: 1
          memory: 1G
    restart: always
  jellyseerr:
    # 🔹🔹 Jellyseerr 🔹🔹
    profiles:
      - jellyfin
    image: docker.io/fallenbagel/jellyseerr
    container_name: jellyseerr
    hostname: jellyseerr
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${JELLYSEERR_IPV4_ADDRESS:-10.76.128.115}
    expose:
      - 5055
    volumes:
      - ${CONFIG_PATH:-./volumes}/jellyseerr/config:/app/config
      - /mnt:/mnt
    environment:
      <<: *common-env
      LOG_LEVEL: ${LOG_LEVEL:-debug}
    labels:
      traefik.enable: "true"
      traefik.http.routers.jellyseerr.rule: Host(`jellyseerr.$DOMAIN`) || Host(`jellyseerr.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.jellyseerr.loadbalancer.server.port: 5055
      homepage.group: Media Requests
      homepage.name: Jellyseerr Requests
      homepage.icon: jellyseerr.png
      homepage.href: https://jellyseerr.$DOMAIN/
      homepage.description: Allows you and others to request new movies and TV shows to be added to your Jellyfin media server.
      homepage.weight: 3
      homepage.widget.type: jellyseerr
      homepage.widget.url: https://jellyseerr.$DOMAIN
      homepage.widget.key: $JELLYSEERR_API_KEY
    cpu_shares: 10
    mem_reservation: 160M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 160M
        limits:
          cpus: 2
          memory: 1G
    restart: always
  notifiarr:
    profiles:
      - unfinished
    # 🔹🔹 Notifiarr 🔹🔹
    # Notifiarr is a tool for monitoring and managing your media server.
    image: golift/notifiarr
    container_name: notifiarr
    hostname: notifiarr
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${NOTIFIARR_IPV4_ADDRESS:-10.76.128.117}
    expose:
      - 8080  # notifiarr-webui
      - 5454  # notifiarr-api
    volumes:
      - ${CONFIG_PATH:-./volumes}/notifiarr/config:/config
      - /var/run/utmp:/var/run/utmp
      - /etc/machine-id:/etc/machine-id
    environment:
      <<: *common-env
      # Required: Get your API key from https://notifiarr.com/
      DN_API_KEY: $NOTIFIARR_API_KEY
    labels:
      traefik.enable: "true"
      traefik.http.routers.notifiarr.rule: Host(`notifiarr.$DOMAIN`) || Host(`notifiarr.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.notifiarr.loadbalancer.server.port: 8080
      homepage.group: Media Requests
      homepage.name: Notifiarr Requests
      homepage.icon: notifiarr.png
      homepage.href: https://notifiarr.$DOMAIN
      homepage.description: Notifiarr is a tool for monitoring and managing your media server.
      homepage.weight: 3
      homepage.widget.type: notifiarr
      homepage.widget.url: https://notifiarr.$DOMAIN
      homepage.widget.key: $NOTIFIARR_API_KEY
    cpu_shares: 10
    mem_reservation: 16M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 16M
        limits:
          cpus: 2
          memory: 1G
    restart: always

  overseerr:
    # 🔹🔹 Overseerr 🔹🔹
    depends_on:
      plex:
        condition: service_healthy
    image: docker.io/linuxserver/overseerr
    container_name: overseerr
    hostname: overseerr
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${OVERSEERR_IPV4_ADDRESS:-10.76.128.116}
    expose:
      - 5055
    volumes:
      - ${CONFIG_PATH:-./volumes}/overseerr/config:/config
    environment:
      <<: *common-env
    labels:
      traefik.enable: "true"
      traefik.http.routers.overseerr.rule: Host(`overseerr.$DOMAIN`) || Host(`overseerr.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.overseerr.loadbalancer.server.port: 5055
      homepage.group: Media Requests
      homepage.name: Overseerr Requests
      homepage.icon: overseerr.png
      homepage.href: https://overseerr.$DOMAIN/
      homepage.description: Allows users to request movies and TV shows, integrating with your media server for easy content management.
      homepage.weight: 3
      homepage.widget.type: overseerr
      homepage.widget.url: https://overseerr.$DOMAIN
      homepage.widget.key: $OVERSEERR_API_KEY
    cpu_shares: 10
    mem_reservation: 175M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 175M
        limits:
          cpus: 2
          memory: 2G
    restart: always

  radarr:
    # 🔹🔹 Radarr 🔹🔹
    profiles:
      - arrs
    image: docker.io/linuxserver/radarr
    container_name: radarr
    hostname: ${RADARR_HOSTNAME:-radarr}
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${RADARR_IPV4_ADDRESS:-10.76.128.121}
    volumes:
      - ${CONFIG_PATH:-./volumes}/radarr/config:/config
      - /mnt:/mnt
      - /mnt/downloads/qbittorrent/movies:/downloads/movies
      - ${RCLONE_BASE_FOLDER:-/mnt/remote}/radarr:/movies
    environment:
      <<: *common-env
      RADARR_API_KEY: $RADARR_API_KEY
    labels:
      traefik.enable: "true"
      traefik.http.routers.radarr.rule: Host(`radarr.$DOMAIN`) || Host(`radarr.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.radarr.loadbalancer.server.port: 7878
      homepage.group: Movie Management
      homepage.name: Radarr Movies
      homepage.icon: radarr.png
      homepage.href: https://radarr.$DOMAIN/
      homepage.description: Automates your movie downloads, finding and adding new releases to your collection as they become available.
      homepage.weight: 1
      homepage.widget.type: radarr
      homepage.widget.url: ${RADARR_INTERNAL_URL:-http://radarr:7878}
      homepage.widget.key: $RADARR_API_KEY
    healthcheck:
      test: curl -fs http://localhost:7878 > /dev/null || exit 1
      interval: ${RADARR_HEALTHCHECK_INTERVAL:-30s}
      timeout: ${RADARR_HEALTHCHECK_TIMEOUT:-15s}
      retries: ${RADARR_HEALTHCHECK_RETRIES:-4}
      start_period: ${RADARR_HEALTHCHECK_START_PERIOD:-60s}
    cpu_shares: 10
    mem_reservation: 6M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 6M
        limits:
          cpus: 1
          memory: 4G
    restart: always
  sonarr:
    # 🔹🔹 Sonarr 🔹🔹
    # Sonarr is a PVR for Usenet and BitTorrent users. It's a sleek Web UI that integrates seamlessly with your various PVR apps.
    profiles:
      - arrs
    image: docker.io/linuxserver/sonarr
    container_name: sonarr
    hostname: sonarr
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${SONARR_IPV4_ADDRESS:-10.76.128.122}
    volumes:
      - ${CONFIG_PATH:-./volumes}/sonarr/config:/config
      - /mnt:/mnt
      - /mnt/downloads/qbittorrent/tv:/downloads/tv
      - /mnt/downloads/qbittorrent/anime:/downloads/anime
      - ${RCLONE_BASE_FOLDER:-/mnt/remote}/sonarr:/tv
      - ${RCLONE_BASE_FOLDER:-/mnt/remote}/sonarr:/anime
    environment:
      <<: *common-env
      SONARR_API_KEY: $SONARR_API_KEY
    labels:
      traefik.enable: "true"
      traefik.http.routers.sonarr.rule: Host(`sonarr.$DOMAIN`) || Host(`sonarr.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.sonarr.loadbalancer.server.port: 8989
      homepage.group: TV Management
      homepage.name: Sonarr TV
      homepage.icon: sonarr.png
      homepage.href: https://sonarr.$DOMAIN/
      homepage.description: Manages your TV show downloads, automatically finding and adding new episodes as they air, keeping your library organized.
      homepage.weight: 0
      homepage.widget.type: sonarr
      homepage.widget.url: ${SONARR_INTERNAL_URL:-http://sonarr:8989}
      homepage.widget.key: $SONARR_API_KEY
    cpu_shares: 10
    mem_reservation: 6M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 6M
        limits:
          cpus: 1
          memory: 4G
    restart: always
  maintainerr:
    # 🔹🔹 Maintainerr 🔹🔹  https://docs.maintainerr.info/latest/Works/
    # Looks and smells like Overseerr, does the opposite. Maintenance tool for the Plex ecosystem
    profiles:
      - extras
    image: ghcr.io/jorenn92/maintainerr
    container_name: maintainerr
    hostname: maintainerr
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${MAINTAINERR_IPV4_ADDRESS:-10.76.128.84}
    expose:
      - ${MAINTAINERR_API_PORT:-3002}
      - ${MAINTAINERR_UI_PORT:-6246}
    ports:
      - ${MAINTAINERR_API_PORT:-3002}:${MAINTAINERR_API_PORT:-3002}
    volumes:
      - ${CONFIG_PATH:-./volumes}/maintainerr/data:/opt/data
      - /mnt:/mnt
    user: $PUID:$PGID
    environment:
      API_PORT: "${MAINTAINERR_API_PORT:-3002}"
      DEBUG: "${MAINTAINERR_DEBUG:-true}"
      NODE_ENV: "${MAINTAINERR_NODE_ENV:-production}"
      UI_HOSTNAME: "${MAINTAINERR_UI_HOSTNAME:-0.0.0.0}" # "0.0.0.0" for IPv4, "::" for IPv6
      UI_PORT: "${MAINTAINERR_UI_PORT:-6246}" # Default 6246.
      VERSION_TAG: "${MAINTAINERR_VERSION_TAG:-develop}"
      UV_USE_IO_URING: "${MAINTAINERR_UV_USE_IO_URING:-0}" # Temporary workaround for https://github.com/libuv/libuv/pull/4141
    labels:
      traefik.enable: "true"
      traefik.http.routers.maintainerr.rule: Host(`maintainerr.$DOMAIN`) || Host(`maintainerr.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.maintainerr.loadbalancer.server.port: 6246
      homepage.group: Media Server Management
      homepage.name: Maintainerr
      homepage.icon: maintainerr.png
      homepage.href: https://maintainerr.$DOMAIN/
      homepage.description: A comprehensive tool for monitoring and managing your server, providing insights and tools to keep everything running smoothly.
    restart: always

  rclone-zurg:
    depends_on:
      - rclone
    image: ghcr.io/coanghel/rclone-docker-automount/rclone-init:latest
    container_name: rclone-zurg
    hostname: rclone-zurg
    extra_hosts: *common-hostname-aliases
    networks:
      - publicnet
    cap_add:
      - SYS_ADMIN
    devices:
      - /dev/fuse:/dev/fuse
    security_opt:
      - apparmor:unconfined
    privileged: true
    configs:
      - source: rclone-mounts.json
        target: /app/mounts.json
        mode: 0777
    volumes:  # only used by the healthcheck.
      - ${ZURG_MOUNT_PATH:-/mnt/remote/realdebrid}:${ZURG_MOUNT_PATH:-/mnt/remote/realdebrid}:rshared
    environment:
      RCLONE_USERNAME: ${RCLONE_USER:-null}
      RCLONE_PASSWORD: ${RCLONE_PASS:-null}
      RCLONE_PORT: ${RCLONE_PORT:-5572}
      RCLONE_CONFIG_PATH: /config/rclone/rclone.conf
    healthcheck:
      test: ["CMD", "mountpoint", "-q", "/mnt/remote/realdebrid"]
      interval: 5s
      timeout: 3s
      retries: 2
      start_period: 10s
    restart: always
  rclone:
    image: rclone/rclone:latest
    container_name: rclone
    hostname: rclone
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${RCLONE_IPV4_ADDRESS:-10.76.128.111}
    expose:
      - ${RCLONE_PORT:-5572}  # rclone webui
      - 5575  # webdav-local
    configs:
      - source: rclone-rclone.conf
        target: /config/rclone/rclone.conf
        mode: 0777
    devices:
      - /dev/fuse:/dev/fuse:rwm
    cap_add:
      - SYS_ADMIN
    security_opt:
      - apparmor:unconfined
    volumes:
      - /:/hostfs:rshared
      - /etc/fuse.conf:/etc/fuse.conf:ro
      - /etc/group:/etc/group:ro
      - /etc/passwd:/etc/passwd:ro
      - ${ZURG_MOUNT_PATH:-/mnt/remote/realdebrid}:${ZURG_MOUNT_PATH:-/mnt/remote/realdebrid}:rshared
      - ${RIVEN_SYMLINK_LIBRARY_PATH:-/mnt/symlinks}:${RIVEN_SYMLINK_LIBRARY_PATH:-/mnt/symlinks}
      - ${CONFIG_PATH:-./volumes}/rclone/.cache:/.cache
      - ${CONFIG_PATH:-./volumes}/rclone/config/rclone:/config/rclone
      - ${CONFIG_PATH:-./volumes}/rclonefm/js:/var/lib/rclonefm/js
    command:
      - rcd
      - --rc-web-gui
      - --rc-web-gui-no-open-browser
      - --rc-addr=:${RCLONE_PORT:-5572}
      - --log-level=INFO
      - --rc-user=brunner56
      - --rc-pass=$SUDO_PASSWORD
      - --config=/config/rclone/rclone.conf
      - --cache-dir=/.cache/rclone
    labels:
      traefik.enable: "true"
      traefik.http.routers.rclone.service: rclone
      traefik.http.routers.rclone.rule: Host(`rclone.$DOMAIN`) || Host(`rclone.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.rclone.loadbalancer.server.port: ${RCLONE_PORT:-5572}
      homepage.group: Media Management
      homepage.name: Rclone
      homepage.icon: rclone.png
      homepage.href: https://rclone.$DOMAIN/
      homepage.description: A web interface for Rclone.
      homepage.weight: 0
    cpu_shares: 10
    mem_reservation: 60M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 60M
        limits:
          cpus: 1
          memory: 1G
    restart: always
  plex:
    # 🔹🔹 Plex Media Server 🔹🔹
    image: plexinc/pms-docker
    container_name: plex
    hostname: plex
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${PLEX_IPV4_ADDRESS:-10.76.128.95}
    ports:
      - "${PLEX_DLNA_PORT:-1900}:1900/udp"
      - "${PLEX_WEBUI_PORT:-32400}:32400/tcp"
      - "${PLEX_PORT2:-32410}:32410/udp"
      - "${PLEX_PORT3:-8324}:8324/tcp"
      - "${PLEX_PORT4:-32410}:32410/udp"
      - "${PLEX_PORT5:-32412}:32412/udp"
      - "${PLEX_PORT6:-32413}:32413/udp"
      - "${PLEX_PORT7:-32414}:32414/udp"
      - "${PLEX_PORT8:-32469}:32469/tcp"
      - "${PLEX_PORT9:-8324}:8324/tcp"
    # Uncomment below line if you have an embedded intel GPU
    #devices:
    #  - /dev/dri:/dev/dri
    volumes:
      - ${CONFIG_PATH:-./volumes}/plex:/config
      - ${CONFIG_PATH:-./volumes}/plex/config:/config
      - ${CONFIG_PATH:-./volumes}/plex/config/osx-config:/config/Library/Application Support/Plex Media Server
      - /dev/shm:/dev/shm
      - ${PLEX_TRANSCODE_PATH:-/mnt/local/transcodes/plex}:/transcode
      - ${RIVEN_SYMLINK_LIBRARY_PATH:-/mnt/symlinks}:/storage/symlinks
      - ${ZURG_MOUNT_PATH:-/mnt/remote/realdebrid}:${ZURG_MOUNT_PATH:-/mnt/remote/realdebrid}:rshared
#    configs:
#      - source: plex-Preferences.xml
#        target: /config/Library/Application Support/Plex Media Server/Preferences.xml
#        mode: 775
    environment:
      #<<: *common-env
      PLEX_CLAIM: $PLEX_CLAIM
      PLEX_TOKEN: $PLEX_TOKEN
      ADVERTISE_IP: https://plex.$DOMAIN:443
      #ADVERTISE_IP: ${ADVERTISE_IP:-170.9.225.137}
      EMAIL_TO: ${EMAIL_TO:-bolabaden.duckdns@gmail.com}
      SMTP_FROM: transcode-killer@$DOMAIN
      SMTP_PORT: ${SMTP_PORT:-587}
      WAIT_FOR_MOUNT_PATHS: ${ZURG_MOUNT_PATH:-/mnt/remote/realdebrid}
      WAIT_FOR_URLS: ${WAIT_FOR_URLS:-http://zurg:9999/http/version.txt}
#      PLEX_CACHE_DIR: ${PLEX_CACHE_DIR:-/config/Library/Application Support/Plex Media Server/Cache}
    labels:
      traefik.enable: "true"
      traefik.http.routers.plex.rule: Host(`plex.$DOMAIN`) || Host(`plex.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.plex.loadbalancer.server.port: 32400
      homepage.group: Media Streaming
      homepage.name: Plex Media Server
      homepage.icon: plex.png
      homepage.href: https://plex.$DOMAIN/
      homepage.description: Stream your movies, TV shows, music, and photos to any device, providing a centralized media hub for all your content.
      homepage.weight: 4
      homepage.widget.type: plex
      homepage.widget.url: https://plex.$DOMAIN
      homepage.widget.key: $PLEX_TOKEN  # see instructions here: https://support.plex.tv/articles/204059436-finding-an-authentication-token-x-plex-token/
    cpu_shares: 10
    mem_reservation: 2G
    deploy:
      resources:
        reservations:
          cpus: 0.5
          memory: 2G
        limits:
          cpus: 2.5
          memory: 4G
    healthcheck:
      test: ["CMD-SHELL", "curl --connect-timeout 15 --silent --show-error --fail http://localhost:32400/identity"]
      interval: 1m
      timeout: 15s
      retries: 3
      start_period: 1m
    restart: always

  recyclarr:
    # 🔹🔹 Recyclarr 🔹🔹
    # command-line application that will automatically synchronize recommended settings from the TRaSH guides to your Sonarr/Radarr instances.
    profiles:
      - arrs
    image: ghcr.io/recyclarr/recyclarr
    container_name: recyclarr
    hostname: recyclarr
    networks:
      - publicnet
    init: true
    configs:
      - source: recyclarr-recyclarr.yaml
        target: /config/recyclarr.yaml
      - source: recyclarr-settings.yaml
        target: /config/settings.yaml
    <<: *common-uidgid
    environment:
      <<: *common-env
      TINI_SUBREAPER: "false"
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.recyclarr.middlewares: nginx-auth@file
      traefik.http.routers.recyclarr.rule: Host(`recyclarr.$DOMAIN`) || Host(`recyclarr.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.recyclarr.loadbalancer.server.port: 32888
      homepage.group: Media Management
      homepage.name: Recyclarr
      homepage.icon: recyclarr.png
      homepage.href: https://recyclarr.$DOMAIN/
      homepage.description: Recyclarr is a tool that will automatically synchronize recommended settings from the TRaSH guides to your Sonarr/Radarr instances.
      homepage.weight: 5
      homepage.widget.type: recyclarr
      homepage.widget.url: https://recyclarr.$DOMAIN
      homepage.widget.key: $RECYCLARR_API_KEY
    cpu_shares: 10
    mem_reservation: 10M
    mem_limit: 512M
    deploy:
      resources:
        reservations:
          cpus: 0.01
          memory: 512M
        limits:
          cpus: 1
          memory: 1G
    restart: always

  jellyfin:
    # 🔹🔹 Jellyfin 🔹🔹
    profiles:
      - jellyfin
    image: linuxserver/jellyfin
    container_name: jellyfin
    hostname: jellyfin
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${JELLYFIN_IPV4_ADDRESS:-10.76.128.120}
    ports:
      - ${JELLYFIN_PORT:-8096}:8096
      - ${JELLYFIN_PORT2:-8920}:8920
      - ${JELLYFIN_DLNA_PORT:-1901}:1900/udp
      - ${JELLYFIN_PORT3:-7359}:7359/udp
    volumes:
      - /mnt:/mnt
      - ${CONFIG_PATH:-./volumes}/jellyfin/config:${JELLYFIN_CONFIG_PATH:-/config}
      - ${CONFIG_PATH:-./volumes}/jellyfin/data:${JELLYFIN_DATA_PATH:-/config/data}
      - ${CONFIG_PATH:-./volumes}/jellyfin/cache:${JELLYFIN_CACHE_DIR:-/config/cache}
      - ${CONFIG_PATH:-./volumes}/jellyfin/log:${JELLYFIN_LOG_DIR:-/config/log}
      - ${CONFIG_PATH:-./volumes}/jellyfin/temp:/tmp/jellyfin
      - ${CONFIG_PATH:-./volumes}/jellyfin/transcode:/transcode
    environment:
      <<: *common-env
      JELLYFIN_PublishedServerUrl: https://jellyfin.$DOMAIN
      JELLYFIN_LOG_DIR: ${JELLYFIN_LOG_DIR:-/config/log}
      JELLYFIN_CONFIG_PATH: ${JELLYFIN_CONFIG_PATH:-/config}
      JELLYFIN_DATA_PATH: ${JELLYFIN_DATA_PATH:-/config/data}
      JELLYFIN_CACHE_DIR: ${JELLYFIN_CACHE_DIR:-/config/cache}
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.jellyfin.middlewares: nginx-auth@file
      traefik.http.routers.jellyfin.rule: Host(`jellyfin.$DOMAIN`) || Host(`jellyfin.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.jellyfin.loadbalancer.server.port: 8096
      homepage.group: Media Streaming
      homepage.name: Jellyfin Media Server
      homepage.icon: jellyfin.png
      homepage.href: https://jellyfin.$DOMAIN/
      homepage.description: Organize and stream your movies, TV shows, and music to any device, creating your own personal media center.
      homepage.weight: 3
      homepage.widget.type: jellyfin
      homepage.widget.url: https://jellyfin.$DOMAIN/
      homepage.widget.key: $JELLYFIN_API_KEY
    cpu_shares: 10
    mem_reservation: 30M
    deploy:
      resources:
        reservations:
          cpus: 0.01
          memory: 30M
        limits:
          cpus: 2
          memory: 4G
    restart: always
  tautulli:
    # 🔹🔹 Tautulli 🔹🔹
    image: ghcr.io/tautulli/tautulli:latest
    container_name: tautulli
    hostname: ${TAUTULLI_HOSTNAME:-tautulli}
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${TAUTULLI_IPV4_ADDRESS:-10.76.128.103}
    expose:
      - 8181
    volumes:
      - ${CONFIG_PATH:-./volumes}/tautulli:/config
    environment:
      <<: *common-env
      TAUTULLI_API_KEY: $TAUTULLI_API_KEY
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.tautulli.rule: Host(`tautulli.$DOMAIN`) || Host(`tautulli.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.tautulli.loadbalancer.server.port: 8181
      homepage.group: Media Maintenance
      homepage.name: Tautulli
      homepage.icon: tautulli.png
      homepage.href: https://tautulli.$DOMAIN/
      homepage.description: Tautulli is a web interface for Plex Media Server.
      homepage.weight: 6
      homepage.widget.type: tautulli
      homepage.widget.url: https://tautulli.$DOMAIN
      homepage.widget.key: $TAUTULLI_API_KEY
    cpu_shares: 10
    mem_reservation: 64M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 64M
        limits:
          cpus: 1
          memory: 1G
    restart: always
  mongodb:
    # 🔹🔹 MongoDB 🔹🔹
    image: mongo:latest
    container_name: mongodb
    hostname: ${MONGODB_HOSTNAME:-mongodb}
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${MONGODB_IPV4_ADDRESS:-10.76.0.50}
    expose:
      - 27017
    volumes:
      - ${CONFIG_PATH:-./volumes}/mongodb/data:/data/db
    environment:
      <<: *common-env
    healthcheck:
      test: ["CMD-SHELL", "mongosh 127.0.0.1:27017/test --quiet --eval 'db.runCommand(\"ping\").ok' > /dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 40s
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.mongodb.middlewares: nginx-auth@file
      traefik.http.routers.mongodb.rule: Host(`mongodb.$DOMAIN`) || Host(`mongodb.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.mongodb.loadbalancer.server.port: 27017
      homepage.group: Databases
      homepage.name: MongoDB
      homepage.icon: mongodb.png
      homepage.href: https://mongodb.$DOMAIN/
      homepage.description: MongoDB is a NoSQL database that uses JSON-like documents with optional schemas.
    restart: always
  ip-checker-naked:
    build:
      context: https://github.com/bolabaden/warp-nat-routing.git#master
      dockerfile: Dockerfile
    container_name: ip-checker-naked
    hostname: ip-checker-naked
    extra_hosts: *common-hostname-aliases
    command: "/bin/sh -c 'while true; do echo \"$(date): $(curl -s ifconfig.me)\"; sleep 60; done'"
    restart: no
  ip-checker-warp:
    build:
      context: https://github.com/bolabaden/warp-nat-routing.git#master
      dockerfile: Dockerfile
    container_name: ip-checker-warp
    hostname: ip-checker-warp
    extra_hosts: *common-hostname-aliases
    networks:
      - warp-network
    command: "/bin/sh -c 'while true; do echo \"$(date): $(curl -s ifconfig.me)\"; sleep 60; done'"
    restart: no
  ip-checker-warp-multi:
    build:
      context: https://github.com/bolabaden/warp-nat-routing.git#master
      dockerfile: Dockerfile
    container_name: ip-checker-warp-multi
    hostname: ip-checker-warp-multi
    extra_hosts: *common-hostname-aliases
    networks:
      - warp-network
      - publicnet
    command: "/bin/sh -c 'while true; do echo \"$(date): $(curl -s ifconfig.me)\"; sleep 60; done'"
    restart: no
  redis:
    # 🔹🔹 Redis 🔹🔹
    # NOTE: If you want to use Valkey (open source) instead of Redis (source available),
    # uncomment the Valkey statement and comment out the Redis statement.
    # Using Valkey with Firecrawl is untested and not guaranteed to work. Use with caution.
    image: redis:alpine
    # image: valkey/valkey:alpine
    container_name: redis
    hostname: redis
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${REDIS_IPV4_ADDRESS:-10.76.128.87}
    expose:
      - 6379
    volumes:
      - ${CONFIG_PATH:-./volumes}/redis:/data
    environment:
      <<: *common-env
      REDIS_HOST: ${REDIS_HOST:-redis}
      REDIS_PORT: ${REDIS_PORT:-6379}
      REDIS_DATABASE: ${REDIS_DATABASE:-0}
      REDIS_USERNAME: ${REDIS_USERNAME:-default}
      REDIS_PASSWORD: ${REDIS_PASSWORD:-redis}
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.redis.middlewares: nginx-auth@file
      traefik.http.routers.redis.rule: Host(`redis.$DOMAIN`) || Host(`redis.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.redis.loadbalancer.server.port: 6379
      homepage.group: Databases
      homepage.name: Redis
      homepage.icon: redis.png
      homepage.href: https://redis.$DOMAIN/
      homepage.description: Redis is an in-memory data structure store, used as a database, cache, and message broker.
    command: redis-server --appendonly yes --save 60 1 --bind 0.0.0.0
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping > /dev/null 2>&1 || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 5
    cpu_shares: 10
    mem_reservation: 200M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 200M
        limits:
          cpus: 0.5
          memory: 4G
    restart: always
  open-webui:
    # 🔹🔹 Open WebUI 🔹🔹
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    hostname: open-webui
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${OPEN_WEBUI_IPV4_ADDRESS:-10.76.128.118}
    expose:
      - ${OPEN_WEBUI_PORT:-8080}
    volumes:
      - ${CONFIG_PATH:-./volumes}/open-webui:/app/backend/data
    environment:
      # ===== GENERAL CONFIGURATION =====
      # Controls whether admin users can export data
      - ENABLE_ADMIN_EXPORT=True

      # Enables admin users to access all chats
      - ENABLE_ADMIN_CHAT_ACCESS=True

      # Bypasses model access control. (default: False)
      - BYPASS_MODEL_ACCESS_CONTROL=True

      # Environment setting (dev/prod) - Docker default is prod
      - ENV=prod

      # If set to False, all PersistentConfig variables are treated as regular variables
      - ENABLE_PERSISTENT_CONFIG=True

      # Sets the port to run Open WebUI from
      - PORT=${OPEN_WEBUI_PORT:-8080}

      # When enabled, saves each chunk of streamed chat data to database in real time
      - ENABLE_REALTIME_CHAT_SAVE=True

      # Bypasses model access control
      - BYPASS_MODEL_ACCESS_CONTROL=False

      # Used for identifying the Git SHA of the build for releases
      - WEBUI_BUILD_HASH=dev-build

      # Builds the Docker image with NVIDIA CUDA support
      - USE_CUDA_DOCKER=${USE_CUDA_DOCKER:-false}

      # ===== AIOHTTP CLIENT CONFIGURATION =====
      # Specifies the timeout duration in seconds for the AIOHTTP client
      - AIOHTTP_CLIENT_TIMEOUT=300

      # Sets the timeout in seconds for fetching the model list
      - AIOHTTP_CLIENT_TIMEOUT_MODEL_LIST=10

      # Sets the timeout in seconds for fetching the OpenAI model list
      - AIOHTTP_CLIENT_TIMEOUT_OPENAI_MODEL_LIST=

      # ===== DIRECTORY CONFIGURATION =====
      # Specifies the base directory for data storage
      - DATA_DIR=./data

      # Specifies the directory for fonts
      - FONTS_DIR=

      # Specifies the location of the built frontend files
      - FRONTEND_BUILD_DIR=../build

      # Specifies the directory for static files
      - STATIC_DIR=./static

      # ===== OLLAMA CONFIGURATION =====
      # Configures the Ollama backend URL
      - OLLAMA_BASE_URL=/ollama

      # Builds the Docker image with a bundled Ollama instance
      - USE_OLLAMA_DOCKER=false

      # If set, assumes Helm chart deployment
      - K8S_FLAG=False

      # ===== SECURITY CONFIGURATION =====
      # Forwards user information as X-headers to OpenAI API and Ollama API
      - ENABLE_FORWARD_USER_INFO_HEADERS=False

      # Sets the SameSite attribute for session cookies (lax/strict/none)
      - WEBUI_SESSION_COOKIE_SAME_SITE=lax

      # Sets the Secure attribute for session cookies
      - WEBUI_SESSION_COOKIE_SECURE=False

      # Sets the SameSite attribute for auth cookies
      - WEBUI_AUTH_COOKIE_SAME_SITE=lax

      # Sets the Secure attribute for auth cookies
      - WEBUI_AUTH_COOKIE_SECURE=False

      # Enables or disables authentication
      - WEBUI_AUTH=${WEBUI_AUTH:-True}

      # Overrides the randomly generated string used for JSON Web Token
      - WEBUI_SECRET_KEY=${OPEN_WEBUI_SECRET_KEY:?}

      # When enabled, makes automatic update checks and notifies about version updates
      - ENABLE_VERSION_UPDATE_CHECK=True

      # Disables Open WebUI's network connections for update checks and automatic model downloads
      - OFFLINE_MODE=False

      # Resets the config.json file on startup
      - RESET_CONFIG_ON_START=False

      # Enables safe mode, which disables potentially unsafe features
      - SAFE_MODE=False

      # Sets the allowed origins for Cross-Origin Resource Sharing (CORS)
      - CORS_ALLOW_ORIGIN=${OPEN_WEBUI_CORS_ALLOWED_ORIGIN:-*}

      # Determines whether to allow custom models defined on the Hub
      - RAG_EMBEDDING_MODEL_TRUST_REMOTE_CODE=True

      # Determines whether to allow custom models for reranking
      - RAG_RERANKING_MODEL_TRUST_REMOTE_CODE=True

      # Toggles automatic update of the Sentence-Transformer model
      - RAG_EMBEDDING_MODEL_AUTO_UPDATE=True

      # Toggles automatic update of the reranking model
      - RAG_RERANKING_MODEL_AUTO_UPDATE=True

      # ===== VECTOR DATABASE CONFIGURATION =====
      # Specifies which vector database system to use
      - VECTOR_DB=chroma

      # ===== RAG CONFIGURATION =====
      # Sets the directory for TikToken cache
      - TIKTOKEN_CACHE_DIR=/app/backend/data/cache/tiktoken

      # Sets the batch size for OpenAI embeddings
      - RAG_EMBEDDING_OPENAI_BATCH_SIZE=1

      # ===== DOCKER-SPECIFIC CONFIGURATION =====
      # Indicates running in Docker environment
      - DOCKER=true

      # Home directory for the container
      - HOME=/root

      # Hugging Face home directory for embedding models
      - HF_HOME=/app/backend/data/cache/embedding/models

      # Sentence Transformers home directory
      - SENTENCE_TRANSFORMERS_HOME=/app/backend/data/cache/embedding/models

      # CUDA Docker version to use
      - USE_CUDA_DOCKER_VER=cu128

      # Embedding model to use in Docker
      - USE_EMBEDDING_MODEL_DOCKER=sentence-transformers/all-MiniLM-L6-v2

      # Reranking model to use in Docker
      - USE_RERANKING_MODEL_DOCKER=

      # ===== TELEMETRY DISABLING =====
      # Disables anonymized telemetry
      - ANONYMIZED_TELEMETRY=false

      # Disables tracking
      - DO_NOT_TRACK=true

      # Disables Scarf analytics
      - SCARF_NO_ANALYTICS=true
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.open-webui.rule: Host(`open-webui.$DOMAIN`) || Host(`open-webui.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.open-webui.loadbalancer.server.port: "${OPEN_WEBUI_PORT:-8080}"
      homepage.group: AI
      homepage.name: Open WebUI
      homepage.icon: open-webui.png
      homepage.href: https://open-webui.$DOMAIN/
      homepage.description: Open WebUI is a feature-rich self-hosted webui for chatting with GenAI.
    command: ["bash", "start.sh"]
    working_dir: /app/backend
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8080"]
      interval: 5s
      timeout: 30s
      retries: 10
    restart: always
  mcpo:
    image: ghcr.io/open-webui/mcpo:main
    container_name: mcpo
    hostname: mcpo
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${MCPO_IPV4_ADDRESS:-10.76.127.3}
    expose:
      - ${MCPO_PORT:-8000}
    volumes:
      - ${CONFIG_PATH:-./volumes}/mcpo/mcp_servers.json:/app/config/mcp_servers.json  # TODO: use configs: syntax for configmap here.
#      - ${CONFIG_PATH:-./volumes}/mcp/mcpo/logs:/app/logs
#      - ${CONFIG_PATH:-./volumes}/mcp/mcpo/data:/app/data
    environment:
      - MCPO_API_KEY=$MCPO_API_KEY
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.mcpo.rule: Host(`mcpo.$DOMAIN`) || Host(`mcpo.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.mcpo.loadbalancer.server.port: ${MCPO_PORT:-8000}
      homepage.group: MCPO
      homepage.name: MCPO
      homepage.icon: mcpo.png
      homepage.href: https://mcpo.$DOMAIN/
      homepage.description: MCPO is a tool for managing and interacting with AI models.
    command: --api-key "$MCPO_API_KEY" --host 0.0.0.0 --port ${MCPO_PORT:-8000} --cors-allow-origins "*" --config /app/config/mcp_servers.json
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:${MCPO_PORT:-8000}/openapi.json"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: always
  litellm:
    image: 'ghcr.io/berriai/litellm-database:main-stable'
#    image: ghcr.io/berriai/litellm:main-stable
    container_name: litellm
    hostname: litellm
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${LITELLM_IPV4_ADDRESS:-10.76.126.90}
    depends_on:
      litellm-postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ${CONFIG_PATH:-./volumes}/litellm:/app/config
    configs:
      - source: litellm_config.yaml
        target: /app/config/litellm_config.yaml
        mode: 0440
    environment:
      - 'LITELLM_LOG=${LITELLM_LOG:-INFO}'
      - 'LITELLM_MODE=${LITELLM_MODE:-PRODUCTION}'
      - 'UI_USERNAME=${LITELLM_UI_USERNAME:-admin}'
      - 'UI_PASSWORD=$LITELLM_MASTER_KEY'
      - 'DATABASE_URL=postgresql://${LITELLM_POSTGRES_USER:-litellm}:${LITELLM_POSTGRES_PASSWORD:-litellm}@${LITELLM_POSTGRES_HOSTNAME:-litellm-postgres}:5432/${LITELLM_POSTGRES_DB:-litellm}'
      - 'REDIS_HOST=${REDIS_HOSTNAME:-redis}'
      - 'REDIS_PORT=${REDIS_PORT:-6379}'
      - 'POSTGRES_USER=${LITELLM_POSTGRES_USER:-litellm}'
      - 'POSTGRES_PASSWORD=${LITELLM_POSTGRES_PASSWORD:-litellm}'
      - 'POSTGRES_DB=${LITELLM_POSTGRES_DB:-litellm}'
      - 'VOYAGE_API_KEY=$VOYAGE_API_KEY'
      - 'AI21_API_KEY=$AI21_API_KEY'
      - 'ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY'
      - 'APIFY_API_TOKEN=$APIFY_API_TOKEN'
      - 'APIPIE_API_KEY=$APIPIE_API_KEY'
      - 'ARLIAI_API_KEY=$ARLIAI_API_KEY'
      - 'AWANLLM_API_KEY=$AWANLLM_API_KEY'
      - 'BASETEN_API_KEY=$BASETEN_API_KEY'
      - 'BITO_ACCESS_KEY=$BITO_ACCESS_KEY'
      - 'BRAVE_API_KEY=$BRAVE_API_KEY'
      - 'CEREBRIUMAI_API_KEY=$CEREBRIUMAI_API_KEY'
      - 'DEEPINFRA_API_KEY=$DEEPINFRA_API_KEY'
      - 'DEEPGRAM_API_KEY=$DEEPGRAM_API_KEY'
      - 'DEEPSEEK_API_KEY=$DEEPSEEK_API_KEY'
      - 'DOCKER_TOKEN=$DOCKER_TOKEN'
      - 'DOCKERHUB_TOKEN=$DOCKERHUB_TOKEN'
      - 'EVERART_API_KEY=$EVERART_API_KEY'
      - 'EXA_API_KEY=$EXA_API_KEY'
      - 'FOREFRONTAI_API_KEY=$FOREFRONTAI_API_KEY'
      - 'GEMINI_API_KEY=$GEMINI_API_KEY'
      - 'GLAMA_API_KEY=$GLAMA_API_KEY'
      - 'GROK_API_KEY=$GROK_API_KEY'
      - 'GROQ_API_KEY=$GROQ_API_KEY'
      - 'HF_TOKEN=$HF_TOKEN'
      - 'HUGGINGFACE_ACCESS_TOKEN=$HUGGINGFACE_ACCESS_TOKEN'
      - 'HUGGINGFACE_API_TOKEN=$HUGGINGFACE_API_TOKEN'
      - 'JINA_API_KEY=$JINA_API_KEY'
      - 'KAGI_API_KEY=$KAGI_API_KEY'
      - 'KLUSTER_API_KEY=$KLUSTER_API_KEY'
      - 'LANGCHAIN_API_KEY=$LANGCHAIN_API_KEY'
      - 'LANGSMITH_API_KEY=$LANGSMITH_API_KEY'
      - 'LITELLM_MASTER_KEY=$LITELLM_MASTER_KEY'
      - 'MISTRAL_API_KEY=$MISTRAL_API_KEY'
      - 'MISTRALAI_API_KEY=$MISTRALAI_API_KEY'
      - 'OPENAI_API_KEY=$OPENAI_API_KEY'
      - 'OPENROUTER_API_KEY=$OPENROUTER_API_KEY'
      - 'PERPLEXITY_API_KEY=$PERPLEXITY_API_KEY'
      - 'PERPLEXITYAI_API_KEY=$PERPLEXITYAI_API_KEY'
      - 'REPLICATE_API_KEY=$REPLICATE_API_KEY'
      - 'SAMBANOVA_API_KEY=$SAMBANOVA_API_KEY'
      - 'SEARCH1API_KEY=$SEARCH1API_KEY'
      - 'SERPAPI_API_KEY=$SERPAPI_API_KEY'
      - 'SMITHERY_API_KEY=$SMITHERY_API_KEY'
      - 'TANDOOR_SECRET_KEY=$TANDOOR_SECRET_KEY'
      - 'TODOIST_API_KEY=$TODOIST_API_KEY'
      - 'TOGETHERAI_API_KEY=$TOGETHERAI_API_KEY'
      - 'UNIFY_API_KEY=$UNIFY_API_KEY'
      - 'UPSTAGE_API_KEY=$UPSTAGE_API_KEY'
      - 'UPSTAGEAI_API_KEY=$UPSTAGEAI_API_KEY'
      - 'VEYRAX_API_KEY=$VEYRAX_API_KEY'
      - 'YANDEX_API_KEY=$YANDEX_API_KEY'
      - 'YOU_API_KEY=$YOU_API_KEY'
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.litellm.rule: Host(`litellm.$DOMAIN`) || Host(`litellm.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.litellm.loadbalancer.server.port: ${LITELLM_PORT:-4000}
      homepage.group: AI
      homepage.name: Litellm
      homepage.icon: litellm.png
      homepage.href: https://litellm.$DOMAIN/
      homepage.description: Litellm is a tool for managing and interacting with AI models.
    command: --config /app/config/litellm_config.yaml --port ${LITELLM_PORT:-4000} --host 0.0.0.0
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:${LITELLM_PORT:-4000}/health/liveliness"]
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 30s
    restart: always
  litellm-postgres:
#    image: 'postgres:16-alpine'
    image: postgres:16.3-alpine3.20
    container_name: litellm-postgres
    hostname: ${LITELLM_POSTGRES_HOSTNAME:-litellm-postgres}
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${LITELLM_POSTGRES_IPV4_ADDRESS:-10.76.127.90}
    environment:
      - 'POSTGRES_DB=${LITELLM_POSTGRES_DB:-litellm}'
      - 'POSTGRES_PASSWORD=${LITELLM_POSTGRES_PASSWORD:-litellm}'
      - 'POSTGRES_USER=${LITELLM_POSTGRES_USER:-litellm}'
    volumes:
      - '${CONFIG_PATH:-./volumes}/litellm/pgdata:/var/lib/postgresql/data'
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 5s
      retries: 3
    restart: always
  searxng:
    # 🔹🔹 SearxNG 🔹🔹  # https://hub.docker.com/r/searxng/searxng
    # SearxNG is a privacy-respecting, hackable, open-source metasearch engine.
    image: docker.io/searxng/searxng:latest
    container_name: searxng
    hostname: ${SEARXNG_HOSTNAME:-searxng}
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${SEARXNG_IPV4_ADDRESS:-10.76.128.90}
    expose:
      - 8080
    volumes:
      # touch ${CONFIG_PATH:-./volumes}/searxng/limiter.toml
      - ${CONFIG_PATH:-./volumes}/searxng:/etc/searxng
    environment:
      <<: *common-env
      SEARXNG_BASE_URL: ${SEARXNG_URL:-http://searxng:8080}
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.searxng.middlewares: nginx-auth@file
      traefik.http.routers.searxng.rule: Host(`searxng.$DOMAIN`) || Host(`searxng.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.searxng.loadbalancer.server.port: 8080
      homepage.group: Search
      homepage.name: SearxNG
      homepage.icon: searxng.png
      homepage.href: https://searxng.$DOMAIN/
      homepage.description: SearxNG is a privacy-respecting, hackable, open-source metasearch engine.
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://127.0.0.1:8080/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: always
  gluetun-premiumize-nl:
    # 🔹🔹 Gluetun 🔹🔹  # https://github.com/qdm12/gluetun
    # Gluetun is a VPN client that supports various VPN protocols, providing secure and private internet access for your applications.
    # use this one with arm:
    # image: ghcr.io/qdm12/gluetun:latest@sha256:f035267182d3928d6d8351eb1b21f74dce0f97c1a521ffe4c18ce6376614806d
    image: ghcr.io/qdm12/gluetun
    container_name: gluetun-premiumize-nl
    hostname: gluetun-premiumize-nl
    extra_hosts: *gluetun-hosts-workaround
    networks:
      publicnet:
        ipv4_address: ${GLUETUN_PREMIUMIZE_NL_IPV4_ADDRESS:-10.76.128.119}
    ports:
      - ${QB_WEBUI_PORT:-8084}:${QB_WEBUI_PORT:-8084}
      - ${QB_TORRENTING_PORT:-6881}:${QB_TORRENTING_PORT:-6881}
      - ${QB_TORRENTING_PORT:-6881}:${QB_TORRENTING_PORT:-6881}/udp
    cap_add:
      - NET_ADMIN
    sysctls:
      net.ipv6.conf.all.disable_ipv6: 1
      net.ipv4.conf.all.src_valid_mark: 1
    expose:
      - ${TRANSMISSION_PEERPORT:-51413}
      - ${QB_WEBUI_PORT:-8084}
      - ${QB_TORRENTING_PORT:-6881}
      - ${QB_TORRENTING_PORT:-6881}/udp
    devices:
      - /dev/net/tun:/dev/net/tun
    environment:
      VPN_SERVICE_PROVIDER: custom
      OPENVPN_CUSTOM_CONFIG: /etc/openvpn/target.ovpn
      OPENVPN_FLAGS: --auth-user-pass /config/auth.conf
      FIREWALL_OUTBOUND_SUBNETS: ${PUBLICNET_SUBNET:-10.76.0.0/16},172.16.0.0/16,172.17.0.0/16,${TAILSCALE_CIDR:-100.64.0.0/10}
      OPENVPN_IPV6: off
      VPN_TYPE: openvpn
    configs:
      - source: premiumize-auth.txt
        target: /config/auth.conf
        mode: 0600
      - source: premiumize-ca.cert
        target: /etc/openvpn/premiumize-ca.cert.pem
        mode: 0600
      - source: vpn-nl.premiumize.me
        target: /etc/openvpn/target.ovpn
        mode: 0600
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      # qBittorrent
      traefik.http.routers.qbittorrent.service: qbittorrent
      traefik.http.routers.qbittorrent.rule: Host(`qbittorrent.$DOMAIN`) || Host(`qbittorrent.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.qbittorrent.loadbalancer.server.port: ${QB_WEBUI_PORT:-8084}
      # Transmission
      traefik.http.routers.transmission.service: transmission
      traefik.http.routers.transmission.rule: Host(`transmission.$DOMAIN`) || Host(`transmission.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.transmission.loadbalancer.server.port: 9091
    restart: always
  warp:
    # 🔹🔹 Cloudflare WARP 🔹🔹
    # Use this for all services that don't need listening ports exposed
#    build:
#      context: https://github.com/cmj2002/warp-docker.git#main
#      dockerfile: Dockerfile
    image: caomingjun/warp:latest
    container_name: warp
    extra_hosts: *gluetun-hosts-workaround
    networks:
      publicnet:
        ipv4_address: ${WARP_IPV4_ADDRESS:-10.76.128.97}
    # add removed rule back (https://github.com/opencontainers/runc/pull/3468)
    device_cgroup_rules:
      - 'c 10:200 rwm'
    expose:
      - 1080  # WARP SOCKS5 Proxy
      - ${COMET_PORT:-2020}  # Comet
      - 3128  # warp-fetch-proxy
      - 8000  # MediaFusion
      - 8112  # Deluge
      - ${MEDIAFLOW_PROXY_PORT:-8888}  # Mediaflow Proxy
      - 9091  # Transmission
      - 9117  # Jackett
      - 9696  # Prowlarr
      - ${ZURG_PORT:-9999}  # Zurg
      - 8080  # Stremio WebUI
      - 11470  # Stremio HTTP Streaming Server
      - 12470  # Stremio HTTPS Streaming Server
    ports:
      - ${ZURG_PORT:-9999}:${ZURG_PORT:-9999}    # Zurg
      - 11470:11470  # Stremio HTTP Streaming Server (HTTP)
      - 12470:12470  # Stremio HTTPS Streaming Server (HTTPS)
    environment:
      - WARP_SLEEP=2
      - WARP_LICENSE_KEY=${WARP_LICENSE_KEY:-}
      - WARP_ENABLE_NAT=1 # enable nat
      - TUNNEL_TOKEN=${WARP_TUNNEL_TOKEN:-}
    cap_add:
      # Docker already have them, these are for podman users
      - MKNOD
      - AUDIT_WRITE
      # additional required cap for warp, both for podman and docker
      - NET_ADMIN
    sysctls:
      - net.ipv6.conf.all.disable_ipv6=1
      - net.ipv4.conf.all.src_valid_mark=1
      # uncomment for nat
      - net.ipv4.ip_forward=1
      - net.ipv6.conf.all.forwarding=1
      - net.ipv6.conf.all.accept_ra=2
    volumes:
      - ${CONFIG_PATH:-./volumes}/warp/data:/var/lib/cloudflare-warp
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      # Middleware for redirecting stremio-web subdomain
      traefik.http.middlewares.stremio-web-redirect.redirectregex.regex: "^https://stremio-web\\.$DOMAIN(.*)"
      traefik.http.middlewares.stremio-web-redirect.redirectregex.replacement: "https://stremio.$DOMAIN$$1"
      traefik.http.middlewares.stremio-web-redirect.redirectregex.permanent: "true"
      # Middleware for redirecting stremio-server subdomain
      traefik.http.middlewares.stremio-server-redirect.redirectregex.regex: "^https://stremio-server\\.$DOMAIN(.*)"
      traefik.http.middlewares.stremio-server-redirect.redirectregex.replacement: "https://stremio.$DOMAIN$$1"
      traefik.http.middlewares.stremio-server-redirect.redirectregex.permanent: "true"
      # Router for redirecting stremio-web subdomain
      traefik.http.routers.stremio-web-redirect.service: noop@internal
      traefik.http.routers.stremio-web-redirect.rule: Host(`stremio-web.$DOMAIN`) || Host(`stremio-web.$TS_HOSTNAME.$DOMAIN`)
      # Router for redirecting stremio-server subdomain
      traefik.http.routers.stremio-server-redirect.service: noop@internal
      traefik.http.routers.stremio-server-redirect.rule: Host(`stremio-server.$DOMAIN`) || Host(`stremio-server.$TS_HOSTNAME.$DOMAIN`)

      # Router for Stremio Web UI (port 8080)
      # This is the fallback router for any other traffic to the domain.
      traefik.http.routers.stremio.service: stremio
      traefik.http.routers.stremio.rule: Host(`stremio.$DOMAIN`) || Host(`stremio.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.routers.stremio.priority: 10
      traefik.http.services.stremio.loadbalancer.server.scheme: https
      traefik.http.services.stremio.loadbalancer.server.port: 8080
      # Router for Stremio API and Streaming Server (port 11470)
      # This has a higher priority to ensure it's matched first.
      traefik.http.routers.stremio-api-http-server.service: stremio-api-http-server
      traefik.http.routers.stremio-api-http-server.rule: Host(`stremio-api.$DOMAIN`) || Host(`stremio-api.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.routers.stremio-api-http-server.priority: 100
      traefik.http.services.stremio-api-http-server.loadbalancer.server.port: 11470
      # Stremio HTTPS Streaming Server (port 12470)
      traefik.http.routers.stremio-api-https-server.service: stremio-api-https-server
      traefik.http.routers.stremio-api-https-server.rule: Host(`stremio.$DOMAIN`) || Host(`stremio.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.routers.stremio-api-https-server.priority: 100
      traefik.http.services.stremio-api-https-server.loadbalancer.server.scheme: https
      traefik.http.services.stremio-api-https-server.loadbalancer.server.port: 12470

      # Router for Stremio Web UI (port 8080)
      # This is the fallback router for any other traffic to the domain.
      traefik.http.routers.stremio-web.service: stremio-web
      traefik.http.routers.stremio-web.rule: Host(`stremio-web.$DOMAIN`) || Host(`stremio-web.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.routers.stremio-web.priority: 10
      traefik.http.services.stremio-web.loadbalancer.server.scheme: https
      traefik.http.services.stremio-web.loadbalancer.server.port: 8080
#      traefik.http.routers.stremio-api-http-server.rule: >
#        'Host(`stremio.$DOMAIN`) || Host(`stremio.$TS_HOSTNAME.$DOMAIN`) || Host(`stremio.$DUCKDNS_DOMAIN`) || Host(`stremio.$TS_HOSTNAME.$DUCKDNS_DOMAIN`)' && PathRegexp(`^/(hlsv2|casting|local-addon|proxy|rar|zip|settings|create|removeAll|samples|probe|subtitlesTracks|opensubHash|subtitles|network-info|device-info|get-https|hwaccel-profiler|status|exec|stream|yt|thumb\.jpg|stats\.json)(/.*)?$|.*\.(m3u8|ts|mp4|json)$`)
#      traefik.http.routers.stremio-api-https-server.rule: >
#        'Host(`stremio.$DOMAIN`) || Host(`stremio.$TS_HOSTNAME.$DOMAIN`) || Host(`stremio.$DUCKDNS_DOMAIN`) || Host(`stremio.$TS_HOSTNAME.$DUCKDNS_DOMAIN`)' && PathRegexp(`^/(hlsv2|casting|local-addon|proxy|rar|zip|settings|create|removeAll|samples|probe|subtitlesTracks|opensubHash|subtitles|network-info|device-info|get-https|hwaccel-profiler|status|exec|stream|yt|thumb\.jpg|stats\.json)(/.*)?$|.*\.(m3u8|ts|mp4|json)$`)
      #traefik.http.routers.stremio-web-redirect.middlewares: "stremio-web-redirect"
      #traefik.http.routers.stremio-server-redirect.middlewares: "stremio-server-redirect"

      # Zurg
      traefik.http.routers.zurg.service: zurg
      traefik.http.routers.zurg.middlewares: nginx-auth@file
      traefik.http.routers.zurg.rule: Host(`zurg.$DOMAIN`) || Host(`zurg.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.zurg.loadbalancer.server.port: 9999
      # Warp Fetch Proxy
      traefik.http.routers.warp-proxy.service: warp-proxy
      traefik.http.routers.warp-proxy.middlewares: nginx-auth@file
      traefik.http.routers.warp-proxy.rule: Host(`warp-proxy.$DOMAIN`) || Host(`warp-proxy.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.warp-proxy.loadbalancer.server.port: 3128
      # Comet
      traefik.http.routers.comet.service: comet
      traefik.http.routers.comet.rule: Host(`comet.$DOMAIN`) || Host(`comet.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.comet.loadbalancer.server.port: ${COMET_PORT:-2020}
      # Jackett
      traefik.http.routers.jackett.service: jackett
      traefik.http.routers.jackett.rule: Host(`jackett.$DOMAIN`) || Host(`jackett.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.jackett.loadbalancer.server.port: 9117
      # MediaFusion
      traefik.http.routers.mediafusion.service: mediafusion
      traefik.http.routers.mediafusion.rule: Host(`mediafusion.$DOMAIN`) || Host(`mediafusion.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.mediafusion.loadbalancer.server.port: 8000
      # Prowlarr
      traefik.http.routers.prowlarr.service: prowlarr
      traefik.http.routers.prowlarr.rule: Host(`prowlarr.$DOMAIN`) || Host(`prowlarr.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.prowlarr.loadbalancer.server.port: 9696
      # Mediaflow Proxy
      traefik.http.routers.mediaflow-proxy.service: mediaflow-proxy
      traefik.http.routers.mediaflow-proxy.rule: Host(`mediaflow-proxy.$DOMAIN`) || Host(`mediaflow-proxy.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.routers.mediaflow-proxy.middlewares: nginx-auth@file
      traefik.http.services.mediaflow-proxy.loadbalancer.server.port: ${MEDIAFLOW_PROXY_PORT:-8888}
      # Deluge
      traefik.http.routers.deluge.service: deluge
      traefik.http.routers.deluge.rule: Host(`deluge.$DOMAIN`) || Host(`deluge.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.deluge.loadbalancer.server.port: 8112
    restart: always
  code-server:
    # 🔹🔹 Code Server 🔹🔹
    image: docker.io/linuxserver/code-server:latest
    container_name: code-server
    hostname: code-server
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${CODE_SERVER_IPV4_ADDRESS:-10.76.128.92}
    expose:
      - 8443
    volumes:
      - ${CONFIG_PATH:-./volumes}/code-server/dev/config:/config
      - ${CONFIG_PATH:-./volumes}/code-server/dev/workspace:/workspace
      - ${ROOT_PATH:-.}:/workspace
    <<: *common-uidgid
    environment:
      <<: *common-env
      CODESERVER_PASSWORD: $SUDO_PASSWORD
      PASSWORD: $SUDO_PASSWORD
      CODESERVER_SUDO_PASSWORD: $SUDO_PASSWORD
      SUDO_PASSWORD: $SUDO_PASSWORD
      CODESERVER_DEFAULT_WORKSPACE: ${CODESERVER_DEFAULT_WORKSPACE:-/workspace}
      DEFAULT_WORKSPACE: ${CODESERVER_DEFAULT_WORKSPACE:-/workspace}
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.code-server.rule: Host(`code-server.$DOMAIN`) || Host(`code-server.$TS_HOSTNAME.$DOMAIN`)
#      traefik.http.routers.code-server.middlewares: "allow-iframe-from-domain@file"
      traefik.http.services.code-server.loadbalancer.server.port: 8443
      homepage.group: Infrastructure
      homepage.name: Code Dev
      homepage.icon: code-server.png
      homepage.href: https://code-server.$DOMAIN/
      homepage.description: Code Dev is a tool that allows you to develop code in a container.
    cpu_shares: 10
    mem_reservation: 200M
    deploy:
      resources:
        reservations:
          cpus: 0.15
          memory: 200M
        limits:
          cpus: 2
          memory: 4G
    restart: always

  nginx-auth:
    # 🔹🔹 Nginx Authentication Middleware 🔹🔹
    # Acts as forwardAuth service for traefik middleware
    image: nginx:alpine
    container_name: nginx-auth
    hostname: nginx-auth
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${NGINX_AUTH_IPV4_ADDRESS:-10.76.128.94}
    expose:
      - 80
    volumes:
      - ${CONFIG_PATH:-./volumes}/nginx-middlewares/nginx.conf:/etc/nginx/nginx.conf:ro
      - ${CONFIG_PATH:-./volumes}/nginx-middlewares/auth:/etc/nginx/auth:ro
#      - ${CONFIG_PATH:-./volumes}/nginx-middlewares/cache:/var/cache/nginx
#      - ${CONFIG_PATH:-./volumes}/nginx-middlewares/logs:/var/log/nginx
    environment:
      <<: *common-env
      NGINX_ACCESS_LOG: /dev/stdout
      NGINX_ERROR_LOG: /dev/stderr
      NGINX_LOG_LEVEL: warn
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.nginx-auth.service: nginx-auth
      traefik.http.routers.nginx-auth.rule: Host(`nginx-auth.$DOMAIN`) || Host(`nginx-auth.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.nginx-auth.loadbalancer.server.port: 80
      traefik.http.middlewares.nginx-auth.forwardauth.address: "http://nginx-auth:80/auth"
      traefik.http.middlewares.nginx-auth.forwardauth.trustForwardHeader: "true"
      traefik.http.middlewares.nginx-auth.forwardauth.authResponseHeaders: "X-Auth-Method,X-Auth-Passed,X-Middleware-Name"
    deploy:
      resources:
        reservations:
          cpus: 0.01
          memory: 6M
        limits:
          cpus: 1
          memory: 1G
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:80/health > /dev/null 2>&1 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: always

  # Multiple DNS challenge provider are not supported with Traefik,
  # but you can use CNAME to handle that. For example, if you have example.org (account foo) and
  # example.com (account bar) you can create a CNAME on example.org called _acme-challenge.example.org
  # pointing to challenge.example.com. This way, you can obtain certificates for example.org with the bar account.
  traefik:
    # 🔹🔹 Traefik 🔹🔹
    profiles:
      - reverse_proxy
    depends_on:
      crowdsec:
        condition: service_started
      nginx-auth:
        condition: service_healthy
      bolabaden-nextjs:
        condition: service_healthy
    image: traefik:latest
    container_name: coolify-proxy
    hostname: traefik
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${TRAEFIK_IPV4_ADDRESS:-10.76.128.85}
    expose:
      - 80
      - 443
      - 443/udp
      - 8080
    ports:
      - 80:80
      - 443:443
      - 443:443/udp
    cap_add:
      - NET_ADMIN
    sysctls:
      net.ipv6.conf.all.disable_ipv6: 1
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:z
      - ${CERTS_PATH:-./certs}:/certs
      - ${CONFIG_PATH:-./volumes}/traefik/config:/config
      - ${CONFIG_PATH:-./volumes}/traefik/plugins-local:/plugins-local
      - ${CONFIG_PATH:-./volumes}/traefik:/etc/traefik/  # Put the provided fileConfig.yml files at this location
      - ${CONFIG_PATH:-./volumes}/traefik/logs:/logs
    #      - /etc/localtime:/etc/localtime:ro
    configs:
      - source: traefik-dynamic.yaml
        target: /traefik/dynamic/traefik.yaml
        mode: 0644
      - source: traefik-logrotate
        target: /etc/logrotate.d/traefik
        mode: 0644
    environment:
      <<: *common-env
      CLOUDFLARE_EMAIL: $CLOUDFLARE_EMAIL
      CLOUDFLARE_DNS_API_TOKEN: $CLOUDFLARE_DNS_API_TOKEN
      CLOUDFLARE_ZONE_API_TOKEN: $CLOUDFLARE_ZONE_API_TOKEN
      LETS_ENCRYPT_EMAIL: $LETS_ENCRYPT_EMAIL?
      DUCKDNS_TOKEN: $DUCKDNS_TOKEN
#    command: --configFile=/config/traefik.yaml
    command:
      - '--accessLog.bufferingSize=0'  # Collect logs as in-memory buffer before writing into log file
      #- '--accessLog.fields.headers.defaultMode=drop'  # Drop all headers per default
      #- '--accessLog.fields.headers.names.User-Agent=keep'  # Log user agent strings
      #- '--accessLog.filePath=/logs/traefik.log'
      #- '--accessLog.filters.statusCodes=400-599' # Log successful/failed http requests
      #- '--accessLog.format=json'
      - '--accessLog=true'
      - '--api.dashboard=true'
      - '--api.debug=true'
      - '--api.disableDashboardAd=true'
      - '--api.insecure=false'
      - '--metrics.prometheus.buckets=0.1,0.3,1.2,5.0'
      - '--certificatesResolvers.letsencrypt.acme.caServer=https://acme-v02.api.letsencrypt.org/directory'
      - '--certificatesResolvers.letsencrypt.acme.dnsChallenge.provider=cloudflare'
      - '--certificatesResolvers.letsencrypt.acme.dnsChallenge.resolvers=1.1.1.1,8.8.8.8,1.0.0.1'
#      - '--certificatesResolvers.letsencrypt.acme.dnsChallenge.propagation.disableANSChecks=true'  # tells Traefik to skip checking against DuckDNS's authoritative nameservers, which are timing out.
#      - '--certificatesResolvers.letsencrypt.acme.dnsChallenge.propagation.requireAllRNS=true'  # ensures that the challenge TXT record is propagated to all recursive nameservers before validation, which is recommended when authoritative nameserver checks are disabled.
      - '--certificatesResolvers.letsencrypt.acme.dnsChallenge=true'
      - '--certificatesResolvers.letsencrypt.acme.email=$LETS_ENCRYPT_EMAIL?'
      - '--certificatesResolvers.letsencrypt.acme.httpChallenge.entryPoint=http'
      - '--certificatesResolvers.letsencrypt.acme.httpChallenge=true'
      - '--certificatesResolvers.letsencrypt.acme.storage=/certs/acme.json'
      - '--certificatesResolvers.letsencrypt.acme.tlsChallenge=true'
      - '--entryPoints.http.address=:80'
      - '--entryPoints.http.forwardedHeaders.trustedIPs=103.21.244.0/22,103.22.200.0/22,103.31.4.0/22,104.16.0.0/13,104.24.0.0/14,108.162.192.0/18,131.0.72.0/22,141.101.64.0/18,162.158.0.0/15,172.64.0.0/13,173.245.48.0/20,188.114.96.0/20,190.93.240.0/20,197.234.240.0/22,198.41.128.0/17,2400:cb00::/32,2606:4700::/32,2803:f800::/32,2405:b500::/32,2405:8100::/32,2a06:98c0::/29,2c0f:f248::/32'
      - '--entryPoints.http.http.encodeQuerySemiColons=true'
# we're using the middleware instead of this, because this forces https everywhere and exclusions aren't possible.
#      - '--entryPoints.http.http.redirections.entryPoint.scheme=https'
#      - '--entryPoints.http.http.redirections.entryPoint.to=https'
      - '--entryPoints.http.http.tls.certResolver=letsencrypt'
      - '--entryPoints.http.http2.maxConcurrentStreams=250'
      - '--entryPoints.https.address=:443'
      - '--entryPoints.https.forwardedHeaders.trustedIPs=103.21.244.0/22,103.22.200.0/22,103.31.4.0/22,104.16.0.0/13,104.24.0.0/14,108.162.192.0/18,131.0.72.0/22,141.101.64.0/18,162.158.0.0/15,172.64.0.0/13,173.245.48.0/20,188.114.96.0/20,190.93.240.0/20,197.234.240.0/22,198.41.128.0/17,2400:cb00::/32,2606:4700::/32,2803:f800::/32,2405:b500::/32,2405:8100::/32,2a06:98c0::/29,2c0f:f248::/32'
      - '--entryPoints.https.http.encodeQuerySemiColons=true'
      #- '--entryPoints.https.http.middlewares=autodetect@file,http-to-https-redirect-simple@file,gzip@file,www-to-non-www@file'  # crowdsec@docker,security-headers@file,rate-limit@file,crowdsec@file'  # http security headers, rate limiting, and crowdsec bouncer by default
      - '--entryPoints.https.http.tls.certResolver=letsencrypt'
      - '--entryPoints.https.http.tls.domains[0].main=$DOMAIN'
      - '--entryPoints.https.http.tls.domains[0].sans=*.$DOMAIN,*.$TS_HOSTNAME.$DOMAIN'
#      - '--entryPoints.https.http.tls.domains[1].main=$DUCKDNS_SUBDOMAIN.duckdns.org'
#      - '--entryPoints.https.http.tls.domains[1].sans=*.$DUCKDNS_SUBDOMAIN.duckdns.org,*.$TS_HOSTNAME.$DUCKDNS_SUBDOMAIN.duckdns.org'
#      - '--entryPoints.https.http.tls.domains[2].main=$TS_HOSTNAME.duckdns.org'
#      - '--entryPoints.https.http.tls.domains[2].sans=*.$TS_HOSTNAME.duckdns.org'
      - '--entryPoints.https.http.tls=true'
      - '--entryPoints.https.http2.maxConcurrentStreams=250'
      - '--entryPoints.https.http3'
      - '--global.checkNewVersion=true'
      - '--global.sendAnonymousUsage=false'
      - '--log.level=INFO'
      - '--ping.terminatingStatusCode=503'
      - '--ping=true'
      - '--providers.docker.defaultRule=Host(`{{ normalize .ContainerName }}.$DOMAIN`) || Host(`{{ normalize .ContainerName }}.$TS_HOSTNAME.$DOMAIN`)'
      - '--providers.docker.exposedByDefault=false'
      - '--providers.docker.network=publicnet'
      - '--providers.docker=true'
      - '--providers.file.directory=/traefik/dynamic/'
      - '--providers.file.watch=true'
      - '--serversTransport.insecureSkipVerify=true'
      - '--experimental.plugins.crowdsec-bouncer-traefik-plugin.modulename=github.com/maxlerebourg/crowdsec-bouncer-traefik-plugin'
      - '--experimental.plugins.crowdsec-bouncer-traefik-plugin.version=v1.4.2'
    labels:
      deunhealth.restart.on.unhealthy: true
      traefik.enable: "true"
      traefik.http.routers.traefik.service: api@internal
      traefik.http.routers.traefik.rule: Host(`traefik.$DOMAIN`) || Host(`traefik.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.traefik.loadbalancer.server.port: 8080
      coolify.managed: true
      coolify.proxy: true
      homepage.group: Infrastructure
      homepage.name: Traefik
      homepage.icon: traefik.png
      homepage.href: https://traefik.$DOMAIN/api
      homepage.widget.type: traefik
      homepage.description: Traefik is a modern HTTP reverse proxy and load balancer.
    restart: always

  i2pd:
    build:
      context: https://github.com/purplei2p/i2pd.git#master:/contrib/docker
      dockerfile: Dockerfile
    # use this one with amd64:
    #image: geti2p/i2p
    # use these with arm64:
    #image: ghcr.io/purplei2p/i2pd:latest
    image: purplei2p/i2pd:latest
    container_name: i2pd
    hostname: i2pd
    extra_hosts: *common-hostname-aliases
    networks:
      - i2pnet
    expose:
      - 2827       # I2P HTTP Proxy
      - 4444       # HTTP Proxy
      - 4445       # HTTPS Proxy
      - 4447       # SOCKS5 Proxy
      - 6668       # IRC Proxy (optional)
      - 7650       # I2P HTTP Proxy
      - 7652       # LAN interface TCP
      - 7653/udp   # LAN interface UDP
      - 7654       # I2CP Protocol
      - 7656       # SAM Bridge (optional)
      - 7658       # I2P Site
      - 7659       # SMTP Proxy
      - 7660       # POP3 Proxy
      - 7661       # I2P external port
      - 7070       # WebUI Console (optional)
      - 24531      # I2NP Protocol
      - 24531/udp  # I2NP Protocol UDP
    configs:
      - source: i2pd.conf
        target: ${I2P_DATA_DIR:-/home/i2pd/data}/i2pd.conf
        mode: 0644
#      - source: tunnels.conf
#        target: /home/i2pd/.i2pd/tunnels.conf
#        mode: 0644
#      - source: addresses.csv
#        target: /home/i2pd/.i2pd/addressbook/addresses.csv
#        mode: 0644
    volumes:
      - ${CERTS_PATH:-./certs}/i2pd_certificates/family:/i2pd_certificates/family
      - ${CERTS_PATH:-./certs}/i2pd_certificates/reseed:/i2pd_certificates/reseed
      - ${CONFIG_PATH:-./volumes}/i2pd/i2psnark:/i2psnark
      - /var/run/docker.sock:/var/run/docker.sock  # For watcher
    environment:
      <<: *common-env
      DATA_DIR: ${I2P_DATA_DIR:-${I2P_DATA_DIR:-/home/i2pd/data}}
      EXT_PORT: ${I2P_EXT_PORT:-7661}
    #command: ["/bin/sh", "-c", "COMMAND=/usr/local/bin/i2pd if [ \"$1\" = \"--help\" ]; then set -- $COMMAND --help; else mkdir -p \"$DATA_DIR\"/certificates; if [ ! -L \"$DATA_DIR\"/certificates/i2pd_certificates ]; then ln -s /i2pd_certificates \"$DATA_DIR\"/certificates/i2pd_certificates 2>/dev/null || true; fi; set -- $COMMAND $DEFAULT_ARGS $@; fi; exec \"$@\""]
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.i2pd.rule: Host(`i2p-webui.$DOMAIN`) || Host(`i2p-webui.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.i2pd.loadbalancer.server.port: 7070
      traefik.http.routers.i2pd-http-proxy.rule: Host(`i2p-http-proxy.$DOMAIN`) || Host(`i2p-http-proxy.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.routers.i2pd-http-proxy.service: i2pd
      traefik.http.routers.i2pd-http-proxy.middlewares: error-mw@docker
      traefik.http.services.i2pd-http-proxy.loadbalancer.server.port: 4444
      traefik.http.routers.i2pd-socks5-proxy.rule: Host(`i2p-socks5-proxy.$DOMAIN`) || Host(`i2p-socks5-proxy.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.routers.i2pd-socks5-proxy.service: i2pd
      traefik.http.routers.i2pd-socks5-proxy.middlewares: error-mw@docker
      traefik.http.services.i2pd-socks5-proxy.loadbalancer.server.port: 4447
      traefik.http.routers.i2pd-sam-bridge.rule: Host(`i2p-sam-bridge.$DOMAIN`) || Host(`i2p-sam-bridge.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.routers.i2pd-sam-bridge.service: i2pd
      traefik.http.services.i2pd-sam-bridge.loadbalancer.server.port: 7656
      homepage.group: Infrastructure
      homepage.name: I2PD
      homepage.icon: i2pd.png
      homepage.href: https://i2p-webui.$DOMAIN/
      homepage.description: I2P is a private, decentralized network layer that allows you to access websites and services without revealing your IP address.
    restart: always
  i2p-watcher:
    depends_on:
      - i2pd
    image: python:3.13-alpine
    container_name: i2p-watcher
    hostname: i2p-watcher
    extra_hosts: *common-hostname-aliases
    networks:
      - publicnet
    configs:
      - source: watcher.py
        target: /app/watcher.py
        mode: 0774
    volumes:
      - ${CONFIG_PATH:-./volumes}/i2pd/config:/app/config
      - ${CONFIG_PATH:-./volumes}/i2pd/keys:/app/keys
      - /var/run/docker.sock:/var/run/docker.sock
    working_dir: /app
    command: ["python3", "watcher.py"]
    restart: always

  prometheus:
    image: prom/prometheus:latest
    container_name: prometheus
    hostname: prometheus
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${PROMETHEUS_IPV4_ADDRESS:-10.76.127.80}
    volumes:
      - ${CONFIG_PATH:-./volumes}/prometheus/data:/prometheus
    configs:
      - source: prometheus.yml
        target: /etc/prometheus/prometheus.yml
        mode: 0774
      - source: alert.rules
        target: /etc/prometheus/alert.rules
        mode: 0774
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.prometheus.rule: Host(`prometheus.$DOMAIN`) || Host(`prometheus.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.prometheus.loadbalancer.server.port: 9090
    restart: always

  bolabaden-nextjs:
    # 🔹🔹 bolabaden nextJS main website 🔹🔹
    depends_on:
      - tinyauth
    build:
      context: ${SRC_PATH:-./projects}/bolabaden-site
      dockerfile: Dockerfile
    image: th3w1zard1/bolabaden-nextjs:latest
    container_name: bolabaden-nextjs
    hostname: bolabaden-nextjs
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${BOLABADEN_NEXTJS_IPV4_ADDRESS:-10.76.128.45}
    expose:
      - ${BOLABADEN_NEXTJS_PORT:-3000}
    ports:
      - ${BOLABADEN_NEXTJS_PORT:-3000}:${BOLABADEN_NEXTJS_PORT:-3000}
    environment:
      <<: *common-env
      NODE_ENV: production
      PORT: ${BOLABADEN_NEXTJS_PORT:-3000}
      HOSTNAME: ${BOLABADEN_NEXTJS_HOSTNAME:-0.0.0.0}
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      # Error middleware configuration
      traefik.http.middlewares.error-mw.errors.query: /api/error/{status}.html
      traefik.http.middlewares.error-mw.errors.service: error-service
      traefik.http.middlewares.error-mw.errors.status: 400-599
      # Error service configuration
      traefik.http.services.error-service.loadbalancer.server.port: ${BOLABADEN_NEXTJS_PORT:-3000}
      # Errors router
      traefik.http.routers.error-router.rule: Host(`errors.$DOMAIN`) || Host(`errors.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.routers.error-router.service: error-service
      traefik.http.routers.error-router.middlewares: error-mw@docker

      # Main website router
      traefik.http.routers.bolabaden-nextjs.rule: Host(`$DOMAIN`) || Host(`$TS_HOSTNAME.$DOMAIN`)
      traefik.http.routers.bolabaden-nextjs.service: bolabaden-nextjs
      traefik.http.routers.bolabaden-nextjs.middlewares: error-mw@docker
      # Bolabaden NextJS service configuration
      traefik.http.services.bolabaden-nextjs.loadbalancer.server.port: ${BOLABADEN_NEXTJS_PORT:-3000}
      # Iframe embed service for other subdomains
      traefik.http.routers.bolabaden-embed.rule: Host(`embed.$DOMAIN`) || Host(`embed.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.routers.bolabaden-embed.service: bolabaden-nextjs
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:3000 > /dev/null 2>&1 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: always

  tinyauth:
    # 🔹🔹 TinyAuth 🔹🔹
    image: ghcr.io/steveiliop56/tinyauth:v3
    container_name: tinyauth
    hostname: auth
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${TINYAUTH_IPV4_ADDRESS:-10.76.128.82}
    volumes:
      - ${CONFIG_PATH:-./volumes}/tinyauth:/data
    environment:
      - SECRET=$TINYAUTH_SECRET?
      - APP_URL=https://auth.$DOMAIN
      - USERS=$TINYAUTH_USERS
      # Google OAuth Configuration
      - GOOGLE_CLIENT_ID=$TINYAUTH_GOOGLE_CLIENT_ID
      - GOOGLE_CLIENT_SECRET=$TINYAUTH_GOOGLE_CLIENT_SECRET
      # GitHub OAuth Configuration
      - GITHUB_CLIENT_ID=$TINYAUTH_GITHUB_CLIENT_ID
      - GITHUB_CLIENT_SECRET=$TINYAUTH_GITHUB_CLIENT_SECRET
      # Additional recommended settings
      - SESSION_EXPIRY=${TINYAUTH_SESSION_EXPIRY:-604800} # 2 weeks session expiry
      - COOKIE_SECURE=${TINYAUTH_COOKIE_SECURE:-true} # Send cookie only with HTTPS
      - APP_TITLE=${TINYAUTH_APP_TITLE:-Bolabaden} # Customize login page title
      - LOGIN_MAX_RETRIES=${TINYAUTH_LOGIN_MAX_RETRIES:-15} # Maximum login attempts before account lockout
      - LOGIN_TIMEOUT=${TINYAUTH_LOGIN_TIMEOUT:-300} # Lock account for 5 minutes after too many failed attempts
      - OAUTH_AUTO_REDIRECT=${TINYAUTH_OAUTH_AUTO_REDIRECT:-none} # Options: none, github, google, or generic
      # Uncomment below if you want to restrict OAuth login to specific users
      - OAUTH_WHITELIST=${TINYAUTH_OAUTH_WHITELIST:-boden.crouch@gmail.com,halomastar@gmail.com,athenajaguiar@gmail.com,bolabaden.duckdns@gmail.com,dgorsch2@gmail.com,dgorsch4@gmail.com}  # e.g. user1,user2,/^admin.*/
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.tinyauth.rule: Host(`auth.$DOMAIN`) || Host(`auth.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.middlewares.tinyauth.forwardauth.address: http://auth:3000/api/auth/traefik
      homepage.group: Security
      homepage.name: TinyAuth
      homepage.icon: shield-lock.png
      homepage.href: https://auth.$DOMAIN/
      homepage.description: Authentication service for securing your applications
    restart: always

  whoami:
    # 🔹🔹 Whoami 🔹🔹
    image: traefik/whoami:latest
    container_name: whoami
    hostname: whoami
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${WHOAMI_IPV4_ADDRESS:-10.76.128.81}
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.whoami.rule: Host(`whoami.$DOMAIN`) || Host(`whoami.$TS_HOSTNAME.$DOMAIN`)
      homepage.group: Web Services
      homepage.name: Whoami
      homepage.icon: whoami.png
      homepage.href: https://whoami-nginx.$DOMAIN
      homepage.description: Whoami service with multiple auth examples - nginx auth (API key OR IP OR TinyAuth), direct access, and TinyAuth only.
    restart: always
  crowdsec:
    # 🔹🔹 CrowdSec 🔹🔹
    # Highly recommend this guide: https://blog.lrvt.de/configuring-crowdsec-with-traefik/
    image: docker.io/crowdsecurity/crowdsec:latest
    container_name: crowdsec
    hostname: crowdsec
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${CROWDSEC_IPV4_ADDRESS:-10.76.128.104}
    ports:
      - 127.0.0.1:9876:8080  # Port mapping for local firewall bouncers
    expose:
      - 8080  # HTTP API for bouncers
      - 6060  # Metrics endpoint for prometheus
      - 7422  # AppSec WAF endpoint
    environment:
      - GID=1000
      ## Please note the spaces between the collections names (hence why the quotes are needed)
      - 'COLLECTIONS=crowdsecurity/traefik crowdsecurity/http-cve crowdsecurity/base-http-scenarios crowdsecurity/sshd crowdsecurity/linux crowdsecurity/appsec-generic-rules crowdsecurity/appsec-virtual-patching crowdsecurity/appsec-crs'
    configs:
      - source: crowdsec-appsec.yaml
        target: /etc/crowdsec/acquis.d/appsec.yaml
        mode: 0644
      - source: acquis.yaml
        target: /etc/acquis.yaml
        mode: 0644
    volumes:
      # CrowdSec container data
      - ${CONFIG_PATH:-./volumes}/crowdsec/data:/var/lib/crowdsec/data
      - ${CONFIG_PATH:-./volumes}/crowdsec/etc/crowdsec:/etc/crowdsec
      - ${CONFIG_PATH:-./volumes}/crowdsec/plugins:/usr/local/lib/crowdsec/plugins/
      # Log bind mounts into crowdsec
      - ${CONFIG_PATH:-./volumes}/crowdsec/var/log/auth.log:/var/log/auth.log:ro
      - ${CONFIG_PATH:-./volumes}/crowdsec/var/log/syslog:/var/log/syslog:ro
      - ${CONFIG_PATH:-./volumes}/traefik/logs:/var/log/traefik:ro
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.middlewares.crowdsec.plugin.crowdsec-bouncer-traefik-plugin.enabled: "true"
      traefik.http.middlewares.crowdsec.plugin.crowdsec-bouncer-traefik-plugin.crowdsecAppsecEnabled: "true"
      traefik.http.middlewares.crowdsec.plugin.crowdsec-bouncer-traefik-plugin.crowdsecAppsecHost: "${CROWDSEC_INTERNAL_HOST:-crowdsec:7422}"
      traefik.http.middlewares.crowdsec.plugin.crowdsec-bouncer-traefik-plugin.crowdsecLapiKey: "$CROWDSEC_CTI_API_KEY"
      traefik.http.routers.crowdsec.middlewares: nginx-auth@file
      traefik.http.routers.crowdsec.rule: Host(`crowdsec.$DOMAIN`) || Host(`crowdsec.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.crowdsec.loadbalancer.server.port: 8080
      homepage.group: Security
      homepage.name: CrowdSec
      homepage.icon: crowdsec.png
      homepage.href: https://crowdsec.$DOMAIN/
      homepage.description: CrowdSec is an open-source, behavioral, intrusion detection system.
      homepage.widget.type: crowdsec
      homepage.widget.url: http://crowdsec:8080
      homepage.widget.username: $CROWDSEC_AGENT_USERNAME  # machine_id in crowdsec
      homepage.widget.password: $CROWDSEC_AGENT_PASSWORD
    restart: always

  deunhealth:
    # 🔹🔹 DeUnhealth 🔹🔹
    # Restart unhealthy containers marked with `deunhealth.restart.on.unhealthy=true` label
    # Receive Docker events as stream instead of polling periodically. Doesn't need network!
    image: qmcgaw/deunhealth
    container_name: deunhealth
    network_mode: none
    security_opt:
      - no-new-privileges:true
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:z
    environment:
      <<: *common-env # for TZ env var.
      LOG_LEVEL: ${DEUNHEALTH_LOG_LEVEL:-debug} # debug, info, warning, error
      HEALTH_SERVER_ADDRESS: ${DEUNHEALTH_HEALTH_SERVER_ADDRESS:-127.0.0.1:9999} # Internal health http server listening address. Nothing to do with restarting unhealthy containers.
    labels:
      deunhealth.restart.on.unhealthy: "true"
      homepage.group: Infrastructure
      homepage.name: DeUnhealth
      homepage.icon: deunhealth.png
      homepage.href: https://deunhealth.$DOMAIN/
      homepage.description: DeUnhealth is a tool that automatically restarts unhealthy containers.
    restart: always

  dozzle:
    # 🔹🔹 Dozzle 🔹🔹
    image: amir20/dozzle
    container_name: dozzle
    hostname: dozzle
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${DOZZLE_IPV4_ADDRESS:-10.76.128.89}
    expose:
      - 8080
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:z
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.dozzle.middlewares: nginx-auth@file
      traefik.http.routers.dozzle.rule: Host(`dozzle.$DOMAIN`) || Host(`dozzle.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.dozzle.loadbalancer.server.port: 8080
      homepage.group: System Monitoring
      homepage.name: Dozzle
      homepage.icon: dozzle.png
      homepage.href: https://dozzle.$DOMAIN/
      homepage.description: Real-time log viewer for Docker containers, providing an easy way to monitor application logs and troubleshoot issues.
    restart: always
  dashboard:
    # 🔹🔹 Dashboard 🔹🔹
    profiles:
      - unfinished
    image: nginx:alpine
    container_name: dashboard
    hostname: dashboard
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${DASHBOARD_IPV4_ADDRESS:-10.76.128.150}
    volumes:
      - ./dashboard.html:/usr/share/nginx/html/index.html
    environment:
      <<: *common-env
      NGINX_ACCESS_LOG: /dev/stdout
      NGINX_ERROR_LOG: /dev/stderr
      NGINX_LOG_LEVEL: debug
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.dashboard.rule: Host(`dashboard.$DOMAIN`) || Host(`dashboard.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.dashboard.loadbalancer.server.port: 80
      homepage.group: "Dashboards"
      homepage.name: "Main Dashboard"
      homepage.icon: "dashboard.png"
      homepage.href: "https://dashboard.$DOMAIN/"
      homepage.description: "Main dashboard with embedded services"
    restart: always
  homepage:
    # 🔹🔹 Homepage 🔹🔹  # https://github.com/gethomepage/homepage
    image: ghcr.io/gethomepage/homepage
    container_name: homepage
    hostname: homepage
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${HOMEPAGE_IPV4_ADDRESS:-10.76.128.88}
    volumes:
      # DO NOT create a bind mount to the entire /app/public/ directory.
      - /var/run/docker.sock:/var/run/docker.sock:z
      - ${CONFIG_PATH:-./volumes}/homepage:/app/config
    environment:
      <<: *common-env
      HOMEPAGE_ALLOWED_HOSTS: ${HOMEPAGE_ALLOWED_HOSTS:-*}
      HOMEPAGE_VAR_TITLE: ${HOMEPAGE_VAR_TITLE:-Bolabaden}
      HOMEPAGE_VAR_SEARCH_PROVIDER: ${HOMEPAGE_VAR_SEARCH_PROVIDER:-google}
      HOMEPAGE_VAR_HEADER_STYLE: ${HOMEPAGE_VAR_HEADER_STYLE:-}
      HOMEPAGE_VAR_WEATHER_CITY: ${HOMEPAGE_VAR_WEATHER_CITY:-Chicago}
      HOMEPAGE_VAR_WEATHER_LAT: ${HOMEPAGE_VAR_WEATHER_LAT:-41.8781}
      HOMEPAGE_VAR_WEATHER_LONG: ${HOMEPAGE_VAR_WEATHER_LONG:--87.6298}
      #      HOMEPAGE_VAR_WEATHER_TIME: ${HOMEPAGE_VAR_WEATHER_TIME:-}
      HOMEPAGE_VAR_WEATHER_UNIT: ${HOMEPAGE_VAR_WEATHER_UNIT:-fahrenheit}
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.homepage.middlewares: nginx-auth@file
      traefik.http.routers.homepage.rule: Host(`homepage.$DOMAIN`) || Host(`homepage.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.homepage.loadbalancer.server.port: 3000
      homepage.group: Dashboards
      homepage.name: Homepage
      homepage.icon: homepage.png
      homepage.href: https://homepage.$DOMAIN/
      homepage.description: Homepage is a dashboard that displays all of your services.
    healthcheck:  # docker exec homepage ls -la /bin /usr/bin | grep -E 'curl|wget|nc|telnet|http|python|ncat|nmap'
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:3000 > /dev/null 2>&1 || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 3
    cpu_shares: 10
    mem_reservation: 128M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 128M
        limits:
          cpus: 0.25  # deliberately hobble the CPU in favor of GPU transcoding
          memory: 1G
    restart: always

  speedtest:
    # 🔹🔹 Speedtest 🔹🔹
    image: docker.io/linuxserver/speedtest-tracker
    container_name: speedtest
    hostname: speedtest
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${SPEEDTEST_IPV4_ADDRESS:-10.76.128.86}
    security_opt:
      - no-new-privileges:true
    volumes:
      - ${CONFIG_PATH:-./volumes}/speedtest-tracker/config:/config
      - ${CERTS_PATH:-./certs}/speedtest-tracker/keys:/config/keys
    environment:
      <<: *common-env
      ADMIN_EMAIL: $SPEEDTEST_TRACKER_ADMIN_EMAIL # Email of the initial admin user. Note: Only effective during initial setup.
      ADMIN_NAME: $SPEEDTEST_TRACKER_ADMIN_NAME # Name of the initial admin user. Note: Only effective during initial setup.
      ADMIN_PASSWORD: ${SPEEDTEST_TRACKER_ADMIN_PASSWORD:-b00tstr4p} # Password of the initial admin user. Note: Only effective during initial setup.
      API_RATE_LIMIT: ${SPEEDTEST_TRACKER_API_RATE_LIMIT:-60} # Number of requests per minute to the API. Default: 60
      APP_KEY: $SPEEDTEST_TRACKER_APP_KEY # Key used to encrypt and decrypt data. You can generate a key at https://speedtest-tracker.dev.
      APP_NAME: ${SPEEDTEST_TRACKER_APP_NAME:-Speedtest Tracker} # Used to define the application's name in the dashboard and in notifications.
      APP_TIMEZONE: ${TZ:-America/Chicago} # Application timezone should be set if your database does not use UTC as its  timezone.
      APP_URL: ${SPEEDTEST_TRACKER_APP_URL:-} # URL used for links in emails and notifications.
      ASSET_URL: ${SPEEDTEST_TRACKER_ASSET_URL:-} # URL used for assets, needed when using a reverse proxy.
      CHART_BEGIN_AT_ZERO: ${SPEEDTEST_TRACKER_CHART_BEGIN_AT_ZERO:-true} # Begin the dashboard axis charts at zero. Default: true
      CHART_DATETIME_FORMAT: ${SPEEDTEST_TRACKER_CHART_DATETIME_FORMAT:-j/m G:i} # Set the formatting of timestamps in charts.
      CONTENT_WIDTH: ${SPEEDTEST_TRACKER_CONTENT_WIDTH:-7xl} # Width of the content section of each page. Can be set to any value found in the Filament docs. Default: 7xl
      DATETIME_FORMAT: ${SPEEDTEST_TRACKER_DATETIME_FORMAT:-j M Y, G:i:s} # Set the formatting of timestamps in tables and notifications.
      DB_CONNECTION: ${SPEEDTEST_TRACKER_DB_CONNECTION:-sqlite}
      DISPLAY_TIMEZONE: ${TZ:-America/Chicago} # Display timestamps in your local time.
      PRUNE_RESULTS_OLDER_THAN: ${SPEEDTEST_TRACKER_PRUNE_RESULTS_OLDER_THAN:-0} # Set the value to greater than zero to prune stored results. This value should be represented in days.
      PUBLIC_DASHBOARD: ${SPEEDTEST_TRACKER_PUBLIC_DASHBOARD:-true} # Enables the public dashboard for guest (unauthenticated) users. Default: false
      SPEEDTEST_BLOCKED_SERVERS: ${SPEEDTEST_TRACKER_BLOCKED_SERVERS:-} # Comma separated list of server IDs that should not be used when running speedtests.
      SPEEDTEST_INTERFACE: ${SPEEDTEST_TRACKER_INTERFACE:-} # Set the network interface to use for the test. This need to be the network interface available inside the container
      SPEEDTEST_SCHEDULE: ${SPEEDTEST_TRACKER_SCHEDULE:-0 * * * *} # Cron expression used to run speedtests on a scheduled basis.
      SPEEDTEST_SERVERS: ${SPEEDTEST_TRACKER_SERVERS:-} # Comma separated list of server IDs to randomly use for speedtest.
      SPEEDTEST_SKIP_IPS: ${SPEEDTEST_TRACKER_SKIP_IPS:-} # A comma separated list of public IP addresses where tests will be skipped when present.
      THRESHOLD_DOWNLOAD: ${SPEEDTEST_TRACKER_THRESHOLD_DOWNLOAD:-900} # Set the Download Threshold. Note: Only effective during initial setup.
      THRESHOLD_ENABLED: ${SPEEDTEST_TRACKER_THRESHOLD_ENABLED:-true} # Enable the thresholds. Note: Only effective during initial setup.
      THRESHOLD_PING: ${SPEEDTEST_TRACKER_THRESHOLD_PING:-25} # Set the Ping Threshold. Note: Only effective during initial setup.
      THRESHOLD_UPLOAD: ${SPEEDTEST_TRACKER_THRESHOLD_UPLOAD:-900} # Set the Upload Threshold. Note: Only effective during initial setup.
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.speedtest.rule: Host(`speedtest.$DOMAIN`) || Host(`speedtest.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.speedtest.loadbalancer.server.port: 80
      homepage.group: Network Monitoring
      homepage.name: Speedtest
      homepage.icon: speedtest-tracker.png
      homepage.href: https://speedtest.$DOMAIN/
      homepage.description: Regularly tests your internet speed and tracks performance over time, helping you monitor your connection quality.
      homepage.widget.type: speedtest
      homepage.widget.url: http://speedtest:80
      homepage.widget.fields: '["download", "upload", "ping", "speedtest"]'
    healthcheck:  # docker exec speedtest ls -la /bin /usr/bin | grep -E 'curl|wget|nc|telnet|http|python|ncat|nmap'
      test: ["CMD-SHELL", "curl -fs http://127.0.0.1:80 > /dev/null || exit 1"]
      start_period: 20s
      timeout: 15s
      interval: 30s
    restart: always

  watchtower:
    # 🔹🔹 Watchtower 🔹🔹
    image: containrrr/watchtower:latest
    container_name: watchtower
    hostname: watchtower
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${WATCHTOWER_IPV4_ADDRESS:-10.76.128.83}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
#      - ${CONFIG_PATH:-./volumes}/watchtower/data:/data
    environment:
      <<: *common-env
      WATCHTOWER_CLEANUP: ${WATCHTOWER_CLEANUP:-true}
      WATCHTOWER_SCHEDULE: ${WATCHTOWER_SCHEDULE:-0 0 6 * * *} # Run at 6am daily
      WATCHTOWER_NOTIFICATION_URL: ${WATCHTOWER_NOTIFICATION_URL:-}
      WATCHTOWER_NOTIFICATION_REPORT: ${WATCHTOWER_NOTIFICATION_REPORT:-true}
      WATCHTOWER_DEBUG: ${WATCHTOWER_DEBUG:-true}
      WATCHTOWER_LOG_LEVEL: ${WATCHTOWER_LOG_LEVEL:-debug}
      WATCHTOWER_NOTIFICATION_TEMPLATE: |
        {{- if .Report -}}
          {{- with .Report -}}
            {{- if ( or .Updated .Failed ) -}}
        {{len .Scanned}} Scanned, {{len .Updated}} Updated, {{len .Failed}} Failed
              {{- range .Updated}}
        - {{.Name}} ({{.ImageName}}): {{.CurrentImageID.ShortID}} updated to {{.LatestImageID.ShortID}}
              {{- end -}}
              {{- range .Skipped}}
        - {{.Name}} ({{.ImageName}}): {{.State}}: {{.Error}}
              {{- end -}}
              {{- range .Failed}}
        - {{.Name}} ({{.ImageName}}): {{.State}}: {{.Error}}
              {{- end -}}
            {{- end -}}
          {{- end -}}
        {{- else -}}
          {{range .Entries -}}{{.Message}}{{"\n"}}{{- end -}}
        {{- end -}}
    labels:
      deunhealth.restart.on.unhealthy: "true"
    restart: always

  flaresolverr:
    # 🔹🔹 Flaresolverr 🔹🔹  # https://github.com/flaresolverr/flaresolverr
    image: ghcr.io/flaresolverr/flaresolverr:latest
    #image: ghcr.io/thephaseless/byparr:latest
    container_name: flaresolverr
    hostname: flaresolverr
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${FLARESOLVERR_IPV4_ADDRESS:-10.76.128.93}
    expose:
      - ${FLARESOLVERR_PORT:-8191}
    environment:
      <<: *common-env
      # FlareSolverr Environment Variables
      # Verbosity of the logging. Use LOG_LEVEL=debug for more information.
      LOG_LEVEL: info  # Only for debugging. If true all HTML that passes through the proxy will be logged to the console in debug level.
      LOG_HTML: false  # Captcha solving method. It is used when a captcha is encountered.
      CAPTCHA_SOLVER: none  # Note: none of the captcha solvers work with the current version of Flaresolverr.
      PORT: ${FLARESOLVERR_PORT:-8191}  # Listening port.
      HOST: 0.0.0.0  # Listening interface.
      HEADLESS: true  # Only for debugging. To run the web browser in headless mode or visible.
      BROWSER_TIMEOUT: 120000  # If you are experiencing errors/timeouts because your system is slow, you can try to increase this value.
      TEST_URL: https://www.google.com  # FlareSolverr makes a request on start to make sure the web browser is working.
      PROMETHEUS_ENABLED: ${FLARESOLVERR_PROMETHEUS_ENABLED:-true}  # Enable Prometheus exporter
      PROMETHEUS_PORT: ${PROMETHEUS_PORT:-9090}  # Listening port for Prometheus exporter
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.flaresolverr.middlewares: nginx-auth@file
      traefik.http.routers.flaresolverr.rule: Host(`flaresolverr.$DOMAIN`) || Host(`flaresolverr.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.flaresolverr.loadbalancer.server.port: ${FLARESOLVERR_PORT:-8191}
      homepage.group: Infrastructure
      homepage.name: Flaresolverr
      homepage.icon: flaresolverr.png
      homepage.href: https://flaresolverr.$DOMAIN/
      homepage.description: Flaresolverr is a proxy server that can bypass captchas and access cloudflare-captcha-protected websites.
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:${FLARESOLVERR_PORT:-8191}/health > /dev/null 2>&1 || exit 1"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
    cpus: 0.6
    mem_reservation: 64M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 64M
        limits:
          cpus: 0.6
          memory: 1G
    restart: always

  stremio:
    # 🔹🔹 Stremio 🔹🔹
    # Stream Movies/TV over debrid instantly.
#    build:
#      context: ${SRC_PATH:-./projects}/stremio/stremio-docker
#      context: https://github.com/tsaridas/stremio-docker.git#main
#      dockerfile: Dockerfile
#    image: tsaridas/stremio-docker:latest
    image: th3w1zard1/stremio-docker:latest
    container_name: stremio
    network_mode: service:warp
    volumes:
      - ${CONFIG_PATH:-./volumes}/stremio/root/.stremio-server:/root/.stremio-server
#      - ${CONFIG_PATH:-./volumes}/stremio/srv/stremio-server:/srv/stremio-server
    environment:
#      Don't use IPADDRESS auto-discovery when routing through a VPN/tunnel.
#      IPADDRESS: 0.0.0.0
      IPADDRESS: ${EXTERNAL_IP:?}
#     SERVER_URL: "https://0-0-0-0.519b6502d940.stremio.rocks:12470/"
#      SERVER_URL: "https://170-9-225-137.519b6502d940.stremio.rocks:12470/"
      SERVER_URL: "https://stremio.$DOMAIN/"
#      WEBUI_LOCATION: "https://app.strem.io/shell-v4.4/"  # Where the streaming server will redirect to. Defaults to "https://app.strem.io/shell-v4.4/"
#      WEBUI_LOCATION: https://stremio-web.$DOMAIN/
      WEBUI_LOCATION: https://stremio-web.$DOMAIN/shell/
      NO_CORS: 0
      CASTING_DISABLED: 1
    labels:
      deunhealth.restart.on.unhealthy: "true"
      homepage.group: Media Streaming Platforms
      homepage.name: Stremio
      homepage.icon: stremio.png
      homepage.href: https://stremio.$DOMAIN/
      homepage.description: A one-stop hub for video content aggregation, allowing you to stream movies, series, and more from various sources.
      homepage.widget.type: stremio
      homepage.widget.url: http://stremio:11470
    healthcheck:
      test: ["CMD-SHELL", "(curl -fs http://127.0.0.1:11470 > /dev/null 2>&1 || curl -fs https://127.0.0.1:12470 > /dev/null 2>&1) || exit 1"]
      interval: 60s
      timeout: 5s
      retries: 3
      start_period: 1m30s
    cpu_shares: 10
    mem_reservation: 256M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 256M
        limits:
          cpus: 2
          memory: 2G
    restart: always
  aiostreams:
    # 🔹🔹 AIOStreams 🔹🔹
    build:
      context: https://github.com/viren070/aiostreams.git#main
      dockerfile: Dockerfile
      tags:
        - th3w1zard1/aiostreams:latest
    image: ghcr.io/viren070/aiostreams
    container_name: aiostreams
    hostname: ${AIOSTREAMS_HOSTNAME:-aiostreams}
    networks:
      - publicnet
    volumes:
      - ${CONFIG_PATH:-./volumes}/stremio/addons/aiostreams/data:/app/data
    environment:
      - PUID=${PUID:-1001}
      - PGID=${PGID:-988}
      - UMASK=${UMASK:-022}
      - TZ=${TZ:-America/Chicago}
      # ==============================================================================
      #                         ESSENTIAL ADDON SETUP
      # ==============================================================================
      # These are the most important settings you'll need to configure.

      # --- Addon Identification ---
      # Descriptive name for your addon instance.
      - ADDON_NAME=${AIOSTREAMS_ADDON_NAME:-BadenAIO}
      # Unique identifier for your addon.
      - ADDON_ID=aiostreams.$DOMAIN

      # --- Network Configuration ---
      # The port on which the addon will listen.
      # Default: 3000
      - PORT=${AIOSTREAMS_PORT:-3005}

      # The base URL of your addon. Highly recommended for proper functioning.
      # Used for generating installation URLs and identifying self-scraping.
      # Example: https://aiostreams.yourdomain.com
      - BASE_URL=https://aiostreams.$DOMAIN

      # --- Security ---
      # CRITICAL: Secret key for encrypting addon configuration.
      # MUST be a 64-character hex string.
      # Generate one using:
      #   Linux/macOS: openssl rand -hex 32
      #   Windows (PowerShell): -join ((0..31) | ForEach-Object { '{0:x2}' -f (Get-Random -Minimum 0 -Maximum 255) })
      #   Or: [System.Guid]::NewGuid().ToString("N") + [System.Guid]::NewGuid().ToString("N") (ensure it's 64 chars)
      - SECRET_KEY=${AIOSTREAMS_SECRET_KEY:-207d3e5d2dd7edca578b121814f32ed4d73568b5059fd710637a40d310928cde}

      # API key to protect your addon installation and usage.
      # Leave empty to disable password protection.
      # Can be any string.
      - ADDON_PASSWORD=${AIOSTREAMS_ADDON_PASSWORD:-sk_4dc059c0399c43fd94c09baaf0b94da119fc526775914bf2b3a3fb6e073e26d9}

      # --- Database ---
      # REQUIRED: The database URI for storing addon configuration.
      # Supports SQLite (simplest) or PostgreSQL.
      #
      # SQLite example (stores data in a file):
      #   DATABASE_URI=sqlite://./data/db.sqlite
      #   (You can change './data/db.sqlite' to your preferred path)
      #
      # PostgreSQL example:
      #   DATABASE_URI=postgres://username:password@host:port/database_name
      #   (e.g., postgresql://postgres:password@localhost:5432/aiostreams)
      - DATABASE_URI=${AIOSTREAMS_DATABASE_URI:-sqlite://./data/db.sqlite}


      # ==============================================================================
      #                     DEBRID & OTHER SERVICE API KEYS
      # ==============================================================================

      # Provide a default TMDB access token to be used for the Title Matching filter if a user does not provide any.
      - TMDB_ACCESS_TOKEN=$TMDB_ACCESS_TOKEN

      # Configure API keys for debrid services and others you plan to use.
      # 'DEFAULT_' values are pre-filled in the user's config page.
      # 'FORCED_' values override user settings and hide the option.

      # --- Real-Debrid ---
      - DEFAULT_REALDEBRID_API_KEY=$REALDEBRID_API_KEY
      #- FORCED_REALDEBRID_API_KEY=$REALDEBRID_API_KEY

      # --- AllDebrid ---
      - DEFAULT_ALLDEBRID_API_KEY=$ALLDEBRID_API_KEY
      #- FORCED_ALLDEBRID_API_KEY=$ALLDEBRID_API_KEY

      # --- Premiumize ---
      - DEFAULT_PREMIUMIZE_API_KEY=$PREMIUMIZE_API_KEY
      #- FORCED_PREMIUMIZE_API_KEY=$PREMIUMIZE_API_KEY

      # --- Debrid-Link ---
      - DEFAULT_DEBRIDLINK_API_KEY=$DEBRIDLINK_API_KEY
      #- FORCED_DEBRIDLINK_API_KEY=$DEBRIDLINK_API_KEY

      # --- Torbox ---
      - DEFAULT_TORBOX_API_KEY=$TORBOX_API_KEY
      #- FORCED_TORBOX_API_KEY=$TORBOX_API_KEY

      # --- OffCloud ---
      - DEFAULT_OFFCLOUD_API_KEY=$OFFCLOUD_API_KEY
      #- FORCED_OFFCLOUD_API_KEY=$OFFCLOUD_API_KEY
      - DEFAULT_OFFCLOUD_EMAIL=$OFFCLOUD_EMAIL
      #- FORCED_OFFCLOUD_EMAIL=$OFFCLOUD_EMAIL
      - DEFAULT_OFFCLOUD_PASSWORD=$OFFCLOUD_PASSWORD
      #- FORCED_OFFCLOUD_PASSWORD=$OFFCLOUD_PASSWORD

      # --- Put.io ---
      - DEFAULT_PUTIO_CLIENT_ID=$PUTIO_CLIENT_ID
      #- FORCED_PUTIO_CLIENT_ID=$PUTIO_CLIENT_ID
      - DEFAULT_PUTIO_CLIENT_SECRET=$PUTIO_CLIENT_SECRET
      #- FORCED_PUTIO_CLIENT_SECRET=$PUTIO_CLIENT_SECRET

      # --- EasyNews ---
      - DEFAULT_EASYNEWS_USERNAME=$EASYNEWS_USERNAME
      #- FORCED_EASYNEWS_USERNAME=$EASYNEWS_USERNAME
      - DEFAULT_EASYNEWS_PASSWORD=$EASYNEWS_PASSWORD
      #- FORCED_EASYNEWS_PASSWORD=$EASYNEWS_PASSWORD

      # --- EasyDebrid ---
      - DEFAULT_EASYDEBRID_API_KEY=$EASYDEBRID_API_KEY
      #- FORCED_EASYDEBRID_API_KEY=$EASYDEBRID_API_KEY

      # --- PikPak ---
      - DEFAULT_PIKPAK_EMAIL=$PIKPAK_EMAIL
      #- FORCED_PIKPAK_EMAIL=$PIKPAK_EMAIL
      - DEFAULT_PIKPAK_PASSWORD=$PIKPAK_PASSWORD
      #- FORCED_PIKPAK_PASSWORD=$PIKPAK_PASSWORD

      # --- Seedr ---
      - DEFAULT_SEEDR_ENCODED_TOKEN=$SEEDR_ENCODED_TOKEN
      #- FORCED_SEEDR_ENCODED_TOKEN=$SEEDR_ENCODED_TOKEN


      # ==============================================================================
      #                      CUSTOMIZATION & ACCESS CONTROL
      # ==============================================================================

      # --- Custom HTML ---
      # Display custom HTML at the top of the addon's configuration page.
      # Example: CUSTOM_HTML="<div>Welcome to my AIOStreams!</div>"
      - CUSTOM_HTML=$AIOSTREAMS_CUSTOM_HTML

      # --- Trusted Users ---
      # Comma-separated list of trusted UUIDs.
      # Trusted users can access features like regex filters if REGEX_FILTER_ACCESS is 'trusted'.
      # Example: TRUSTED_UUIDS=ae32f456-1234-5678-9012-345678901234,another-uuid-here
      - TRUSTED_UUIDS=$AIOSTREAMS_TRUSTED_UUIDS

      # --- Regex Filter Access ---
      # Controls who can use regex filters.
      # 'none': No one can use regex filters.
      # 'trusted': Only users listed in TRUSTED_UUIDS.
      # 'all': All users (only recommended if ADDON_PASSWORD is set).
      # Default: trusted
      - REGEX_FILTER_ACCESS=${AIOSTREAMS_REGEX_FILTER_ACCESS:-trusted}

      # --- Aliased Configurations (Vanity URLs) ---
      # Create shorter, memorable installation URLs.
      # Format: aliasName1:uuid1:encryptedPassword1,aliasName2:uuid2:encryptedPassword2
      # Users can then access the addon via /stremio/u/aliasName/manifest.json
      #- ALIASED_CONFIGURATIONS=$AIOSTREAMS_ALIASED_CONFIGURATIONS

      # ==============================================================================
      #                           CACHE CONFIGURATION
      # ==============================================================================

      # --- Default maximum cache size ----
      # The maximum number of items that can be held in a given cache instance, if not overriden by a specific cache instance
      - DEFAULT_MAX_CACHE_SIZE=${AIOSTREAMS_DEFAULT_MAX_CACHE_SIZE:-100000}

      # --- Proxy IP TTL (StremThru/MediaFlow Proxy)
      # The Time-To-Live (in seconds) of items in the Public IP cache.
      # Set to -1 to disable caching
      - PROXY_IP_CACHE_TTL=${AIOSTREAMS_PROXY_IP_CACHE_TTL:-900}  # Default is 900

      # --- Addon Resource Caching ---
      # Control the Caching of resources fetched from other addons
      # Set to -1 to disable caching.
      - MANIFEST_CACHE_TTL=${AIOSTREAMS_MANIFEST_CACHE_TTL:-300}  # Default is 300
      - SUBTITLE_CACHE_TTL=${AIOSTREAMS_SUBTITLE_CACHE_TTL:-300}  # Default is 300
      - STREAM_CACHE_TTL=${AIOSTREAMS_STREAM_CACHE_TTL:-1}  # Default is 1
      - CATALOG_CACHE_TTL=${AIOSTREAMS_CATALOG_CACHE_TTL:-300}  # Default is 300
      - META_CACHE_TTL=${AIOSTREAMS_META_CACHE_TTL:-300}  # Default is 300
      - ADDON_CATALOG_CACHE_TTL=${AIOSTREAMS_ADDON_CATALOG_CACHE_TTL:-300}  # Default is 300


      # --- RPDB API Key Validation Caching ---
      # Control how long a valid API key check is cached for
      # Default: 7 days
      - RPDB_API_KEY_VALIDITY_CACHE_TTL=${AIOSTREAMS_RPDB_API_KEY_VALIDITY_CACHE_TTL:-604800}

      # ==============================================================================
      #                             FEATURE CONTROL
      # ==============================================================================
      # Enable or disable specific addon features.

      # --- Self-Scraping ---
      # Prevent this AIOStreams instance from being added as an addon to itself.
      # Default: true
      - DISABLE_SELF_SCRAPING=${AIOSTREAMS_DISABLE_SELF_SCRAPING:-true}

      # --- Disabled Hosts ---
      # Prevent certain hostnames from being added as addons.
      # Format: host1:reason1,host2:reason2
      # Example: DISABLED_HOSTS=torrentio.strem.fun:Blocked by Torrentio
      #- DISABLED_HOSTS=

      # --- Disabled Addons (Marketplace) ---
      # Disable specific addons from appearing in the marketplace.
      # See https://github.com/Viren070/AIOStreams/blob/main/packages/core/src/utils/marketplace.ts for IDs.
      # Format: addonID1:reason1,addonID2:reason2
      # Example: DISABLED_ADDONS=torrentio:Blocked by Torrentio
      #- DISABLED_ADDONS=

      # --- Disabled Services (Configuration Page) ---
      # Hide certain services (e.g., debrid services) from the configuration page.
      # Format: service1:reason1,service2:reason2
      # Example: DISABLED_SERVICES=realdebrid:Not available on this instance
      #- DISABLED_SERVICES=


      # ==============================================================================
      #                                 LOGGING
      # ==============================================================================

      # --- Log Level ---
      # Set the verbosity of logs. Options: "error", "warn", "info", "http", "verbose","debug", "silly"
      # Default: info
      - LOG_LEVEL=${AIOSTREAMS_LOG_LEVEL:-info}

      # --- Log Format ---
      # Output logs in "json" or "text" format.
      # Default: text
      - LOG_FORMAT=${AIOSTREAMS_LOG_FORMAT:-text}

      # --- Log Sensitive Information ---
      # Whether to include potentially sensitive info (like API keys) in logs.
      # Useful for debugging, but disable for production if concerned.
      # Default: true
      - LOG_SENSITIVE_INFO=${AIOSTREAMS_LOG_SENSITIVE_INFO:-true}

      # --- Log Timezone ---
      # Adjust the timezone used for logging
      # e.g. Europe/Paris, America/New_York
      - LOG_TIMEZONE=${TZ:-America/Chicago}


      # ==============================================================================
      #                  PROXY FOR OUTGOING ADDON REQUESTS (Torrentio, etc.)
      # ==============================================================================
      # Configure a proxy for requests made *by* this AIOStreams instance *to* other addons (e.g., Torrentio).
      # Useful if your server's IP is blocked by an upstream service.

      # --- Addon Proxy URL ---
      # The proxy URL to use for all requests to upstream addons.
      # Example: ADDON_PROXY=http://warp:1080 (using https://github.com/cmj2002/warp-docker)
      - ADDON_PROXY=${AIOSTREAMS_ADDON_PROXY:-http://warp:1080}

      # --- Addon Proxy Configuration ---
      # Optionally, specify which domains to proxy.
      # Comma-separated list of rules: domain_pattern:boolean. Later rules have higher priority.
      # Wildcards (*) can be used.
      # Example: ADDON_PROXY_CONFIG="*:false,*.strem.fun:true" (only proxy *.strem.fun domains)
      - ADDON_PROXY_CONFIG=${AIOSTREAMS_ADDON_PROXY_CONFIG:-"*:false,*.strem.fun:true"}


      # ==============================================================================
      #                  DEFAULT/FORCED STREAM PROXY (MediaFlow, StremThru)
      # ==============================================================================
      # Configure how AIOStreams handles stream proxies like MediaFlow or StremThru for playback.
      # 'DEFAULT_' values are pre-filled. 'FORCE_' values override user settings.

      # --- Stream Proxy Enabled ---
      # - DEFAULT_PROXY_ENABLED=${AIOSTREAMS_DEFAULT_PROXY_ENABLED:-true}  # Default state for enabling a stream proxy.
      - FORCE_PROXY_ENABLED=${AIOSTREAMS_FORCE_PROXY_ENABLED:-true}   # Force stream proxy on/off for all users.

      # --- Stream Proxy ID ---
      # 'mediaflow' or 'stremthru'
      - DEFAULT_PROXY_ID=${AIOSTREAMS_DEFAULT_PROXY_ID:-stremthru}
      #- FORCE_PROXY_ID=stremthru

      # --- Stream Proxy URL ---
      # URL of your MediaFlow or StremThru instance.
      - DEFAULT_PROXY_URL=http://stremthru:8080
      #- FORCE_PROXY_URL=${STREMTHRU_INTERNAL_URL:-http://stremthru:8080}

      # --- Stream Proxy Credentials ---
      # Format: username:password
      - DEFAULT_PROXY_CREDENTIALS=${STREMTHRU_CREDENTIALS:-bolabaden:duckdns}
      #- FORCE_PROXY_CREDENTIALS=${STREMTHRU_CREDENTIALS:-bolabaden:duckdns}

      # --- Stream Proxy Public IP ---
      # Public IP for the proxy, if needed.
      #- DEFAULT_PROXY_PUBLIC_IP={AIOSTREAMS_DEFAULT_PROXY_PUBLIC_IP:-170.9.225.137}
      #- FORCE_PROXY_PUBLIC_IP={AIOSTREAMS_DEFAULT_PROXY_PUBLIC_IP:-170.9.225.137}

      # --- Proxied Services ---
      # Comma-separated list of services whose streams should be proxied (e.g., realdebrid,alldebrid).
      # DEFAULT_PROXY_PROXIED_SERVICES=
      # FORCE_PROXY_PROXIED_SERVICES=

      # --- Disable Proxied Addons Feature ---
      # If true, it disables the 'Proxied Addons' option.
      #- FORCE_PROXY_DISABLE_PROXIED_ADDONS=${AIOSTREAMS_FORCE_PROXY_DISABLE_PROXIED_ADDONS:-false}

      # --- Encrypt Streaming URLs ---
      # Encrypt MediaFlow/StremThru URLs for better compatibility with external players.
      - ENCRYPT_MEDIAFLOW_URLS=${AIOSTREAMS_ENCRYPT_MEDIAFLOW_URLS:-true}
      - ENCRYPT_STREMTHRU_URLS=${AIOSTREAMS_ENCRYPT_STREMTHRU_URLS:-true}


      # --- Forced Public proxy URL adjustments ----
      # If you'd like to force some adjustments to be made to the streaming urls generated by either proxy, you can do that here.
      # This is useful when you want to use a local url for requests but have AIOStreams force the urls to use a specific host, port, and protocol.
      - FORCE_PUBLIC_PROXY_HOST=stremthru.$DOMAIN  # Default is stremthru.$DOMAIN
      - FORCE_PUBLIC_PROXY_PORT=443
      - FORCE_PUBLIC_PROXY_PROTOCOL=https


      # ==============================================================================
      #                       ADVANCED CONFIGURATION & LIMITS
      # ==============================================================================

      # --- General Default Timeout ---
      # Default timeout in milliseconds for all requests if not overridden by a specific timeout.
      # Default: 15000 (15 seconds)
      - DEFAULT_TIMEOUT=${AIOSTREAMS_DEFAULT_TIMEOUT:-15000}

      # --- Configuration Limits ---
      # Maximum number of addons allowed per AIOStreams configuration.
      - MAX_ADDONS=${AIOSTREAMS_MAX_ADDONS:-100}
      # Maximum number of groups allowed per AIOStreams configuration
      - MAX_GROUPS=${AIOSTREAMS_MAX_GROUPS:-50}
      # Maximum number of keyword filters per AIOStreams configuration.
      - MAX_KEYWORD_FILTERS=${AIOSTREAMS_MAX_KEYWORD_FILTERS:-50}
      # Maximum number of condition filters per AIOStreams configuration
      - MAX_CONDITION_FILTERS=${AIOSTREAMS_MAX_CONDITION_FILTERS:-200}
      # Maximum timeout (ms) an addon can be set to via override.
      - MAX_TIMEOUT=${AIOSTREAMS_MAX_TIMEOUT:-50000}
      # Minimum timeout (ms) an addon can be set to via override.
      - MIN_TIMEOUT=${AIOSTREAMS_MIN_TIMEOUT:-1000}


      # ==============================================================================
      #                           RATE LIMIT CONFIGURATION
      # ==============================================================================
      # Configure rate limits to prevent abuse. Typically, defaults are fine.

      # --- Disable Rate Limits ---
      # Set to true to disable all rate limits (NOT RECOMMENDED).
      # Default: false
      - DISABLE_RATE_LIMITS=${AIOSTREAMS_DISABLE_RATE_LIMITS:-false}

      # Window and Max requests refer to the maximum number of requests a user can make within a specific timeframe

      # --- Static File Serving ---
      - STATIC_RATE_LIMIT_WINDOW=${AIOSTREAMS_STATIC_RATE_LIMIT_WINDOW:-5}
      - STATIC_RATE_LIMIT_MAX_REQUESTS=${AIOSTREAMS_STATIC_RATE_LIMIT_MAX_REQUESTS:-75}

      # --- User API ---
      - USER_API_RATE_LIMIT_WINDOW=${AIOSTREAMS_USER_API_RATE_LIMIT_WINDOW:-5}
      - USER_API_RATE_LIMIT_MAX_REQUESTS=${AIOSTREAMS_USER_API_RATE_LIMIT_MAX_REQUESTS:-5}

      # --- Stream API ---
      - STREAM_API_RATE_LIMIT_WINDOW=${AIOSTREAMS_STREAM_API_RATE_LIMIT_WINDOW:-5}
      - STREAM_API_RATE_LIMIT_MAX_REQUESTS=${AIOSTREAMS_STREAM_API_RATE_LIMIT_MAX_REQUESTS:-10}

      # --- Format API ---
      - FORMAT_API_RATE_LIMIT_WINDOW=${AIOSTREAMS_FORMAT_API_RATE_LIMIT_WINDOW:-5}
      - FORMAT_API_RATE_LIMIT_MAX_REQUESTS=${AIOSTREAMS_FORMAT_API_RATE_LIMIT_MAX_REQUESTS:-30}

      # --- Catalog API ---
      - CATALOG_API_RATE_LIMIT_WINDOW=${AIOSTREAMS_CATALOG_API_RATE_LIMIT_WINDOW:-5}
      - CATALOG_API_RATE_LIMIT_MAX_REQUESTS=${AIOSTREAMS_CATALOG_API_RATE_LIMIT_MAX_REQUESTS:-5}

      # --- Stremio Stream ---
      - STREMIO_STREAM_RATE_LIMIT_WINDOW=${AIOSTREAMS_STREMIO_STREAM_RATE_LIMIT_WINDOW:-15}
      - STREMIO_STREAM_RATE_LIMIT_MAX_REQUESTS=${AIOSTREAMS_STREMIO_STREAM_RATE_LIMIT_MAX_REQUESTS:-10}

      # --- Stremio Catalog ---
      - STREMIO_CATALOG_RATE_LIMIT_WINDOW=${AIOSTREAMS_STREMIO_CATALOG_RATE_LIMIT_WINDOW:-5}
      - STREMIO_CATALOG_RATE_LIMIT_MAX_REQUESTS=${AIOSTREAMS_STREMIO_CATALOG_RATE_LIMIT_MAX_REQUESTS:-30}

      # --- Stremio Manifest ---
      - STREMIO_MANIFEST_RATE_LIMIT_WINDOW=${AIOSTREAMS_STREMIO_MANIFEST_RATE_LIMIT_WINDOW:-5}
      - STREMIO_MANIFEST_RATE_LIMIT_MAX_REQUESTS=${AIOSTREAMS_STREMIO_MANIFEST_RATE_LIMIT_MAX_REQUESTS:-5}

      # --- Stremio Subtitle ---
      - STREMIO_SUBTITLE_RATE_LIMIT_WINDOW=${AIOSTREAMS_STREMIO_SUBTITLE_RATE_LIMIT_WINDOW:-5}
      - STREMIO_SUBTITLE_RATE_LIMIT_MAX_REQUESTS=${AIOSTREAMS_STREMIO_SUBTITLE_RATE_LIMIT_MAX_REQUESTS:-10}

      # --- Stremio Meta ---
      - STREMIO_META_RATE_LIMIT_WINDOW=${AIOSTREAMS_STREMIO_META_RATE_LIMIT_WINDOW:-5}
      - STREMIO_META_RATE_LIMIT_MAX_REQUESTS=${AIOSTREAMS_STREMIO_META_RATE_LIMIT_MAX_REQUESTS:-15}


      # ==============================================================================
      #                         INACTIVE USER PRUNING
      # ==============================================================================
      # Automatically prune (delete) inactive user configurations.

      # --- Prune Interval ---
      # How often to check for inactive users, in seconds.
      # Default: 86400 (1 day)
      - PRUNE_INTERVAL=${AIOSTREAMS_PRUNE_INTERVAL:-86400}

      # --- Prune Max Inactivity Days ---
      # Maximum days of inactivity before a user's configuration is pruned.
      # Set to -1 to disable
      # Default: -1
      - PRUNE_MAX_DAYS=${AIOSTREAMS_PRUNE_MAX_DAYS:-1}

      # ==============================================================================
      #                      EXTERNAL ADDON SERVICE URLs & TIMEOUTS
      # ==============================================================================
      # URLs and default timeouts for various external Stremio addons that AIOStreams can integrate with.
      # Change these if you use self-hosted versions or if defaults become outdated.

      # ----------- COMET ------------
      - COMET_URL=${COMET_INTERNAL_URL:-http://comet:3000}
      # DEFAULT_COMET_TIMEOUT=
      # Advanced: Override Comet hostname/port/protocol if COMET_URL is internal but needs to be public-facing.
      # Only uncomment and set if needed. Usually, leave these commented.
      #- FORCE_COMET_HOSTNAME=comet.$DOMAIN
      #- FORCE_COMET_PORT=443
      #- FORCE_COMET_PROTOCOL=https

      # ----------- MEDIAFUSION ------------
      - MEDIAFUSION_URL=${MEDIAFUSION_INTERNAL_URL:-http://mediafusion:8000}
      #- MEDIAFUSION_URL=${MEDIAFUSION_INTERNAL_URL:-http://mediafusion.elfhosted.com}
      # DEFAULT_MEDIAFUSION_TIMEOUT=
      - MEDIAFUSION_CONFIG_TIMEOUT=${MEDIAFUSION_CONFIG_TIMEOUT:-5000} # Timeout (ms) for /encrypt-user-data endpoint.
      # API Password for self-hosted MediaFusion (for auto-configuration).
      - MEDIAFUSION_API_PASSWORD=$SUDO_PASSWORD

      # ----------- JACKETTIO -------------
      - JACKETTIO_URL=${JACKETTIO_URL:-https://jackettio.elfhosted.com/}
      # DEFAULT_JACKETTIO_TIMEOUT=
      # Default indexers for auto-configuration with Jackettio.
      - DEFAULT_JACKETTIO_INDEXERS=["bitsearch", "eztv", "thepiratebay", "therarbg", "yts"]
      # Default StremThru URL used by Jackettio.
      - DEFAULT_JACKETTIO_STREMTHRU_URL=${JACKETTIO_STREMTHRU_URL:-https://stremthru.13377001.xyz}
      # Self-hosted StremThru for Jackettio:
      #- DEFAULT_JACKETTIO_STREMTHRU_URL=${STREMTHRU_INTERNAL_URL:-http://stremthru:8080}
      # Advanced: Override Jackettio hostname/port/protocol (similar to Comet).
      #- FORCE_JACKETTIO_HOSTNAME=jackettio.$DOMAIN
      #- FORCE_JACKETTIO_PORT=443
      #- FORCE_JACKETTIO_PROTOCOL=https

      # --------- STREMTHRU-STORE ---------
      #- STREMTHRU_STORE_URL=${STREMTHRU_STORE_URL:-https://stremthru.elfhosted.com/stremio/store}
      - STREMTHRU_STORE_URL=${STREMTHRU_STORE_INTERNAL_URL:-http://stremthru:8080/stremio/store}
      # DEFAULT_STREMTHRU_STORE_TIMEOUT=
      # Advanced: Override StremThru Store  hostname/port/protocol (similar to Comet).
      #- FORCE_STREMTHRU_STORE_HOST=stremthru.$DOMAIN
      #- FORCE_STREMTHRU_STORE_PORT=443
      #- FORCE_STREMTHRU_STORE_PROTOCOL=https
      # --------- STREMTHRU-TORZ -----
      - STREMTHRU_TORZ_URL=${STREMTHRU_TORZ_URL:-https://stremthru.elfhosted.com/stremio/torz}
      #- STREMTHRU_TORZ_URL=${STREMTHRU_TORZ_INTERNAL_URL:-http://stremthru:8080/stremio/torz}
      # DEFAULT_STREMTHRU_TORZ_TIMEOUT=
      # Advanced: Override StremThru Torz hostname/port/protocol (similar to Comet).
      #- FORCE_STREMTHRU_TORZ_HOST=stremthru.$DOMAIN
      #- FORCE_STREMTHRU_TORZ_PORT=443
      #- FORCE_STREMTHRU_TORZ_PROTOCOL=https

      # --------- EASYNEWS+ ADDON ---------
      - EASYNEWS_PLUS_URL=${EASYNEWS_PLUS_URL:-https://b89262c192b0-stremio-easynews-addon.baby-beamup.club/}
      # DEFAULT_EASYNEWS_PLUS_TIMEOUT=

      # -------- EASYNEWS++ ADDON ---------
      - EASYNEWS_PLUS_PLUS_URL=${EASYNEWS_PLUS_PLUS_URL:-https://easynews-cloudflare-worker.jqrw92fchz.workers.dev/}
      # DEFAULT_EASYNEWS_PLUS_PLUS_TIMEOUT=

      # --------- STREAMFUSION ---------
      - STREAMFUSION_URL=${STREAMFUSION_URL:-https://stream-fusion.stremiofr.com/}
      # DEFAULT_STREAMFUSION_TIMEOUT=

      # --------- MARVEL UNIVERSE ---------
      - MARVEL_UNIVERSE_URL=${MARVEL_UNIVERSE_URL:-https://addon-marvel.onrender.com/}
      # DEFAULT_MARVEL_UNIVERSE_TIMEOUT=

      # --------- DC UNIVERSE ---------
      - DC_UNIVERSE_URL=${DC_UNIVERSE_URL:-https://addon-dc-cq85.onrender.com/}
      # DEFAULT_DC_UNIVERSE_TIMEOUT=

      # --------- STAR WARS UNIVERSE ---------
      - STAR_WARS_UNIVERSE_URL=${STAR_WARS_UNIVERSE_URL:-https://addon-star-wars-u9e3.onrender.com/}
      # DEFAULT_STAR_WARS_UNIVERSE_TIMEOUT=

      # --------- ANIME KITSU ---------
      - ANIME_KITSU_URL=${ANIME_KITSU_URL:-https://anime-kitsu.strem.fun/}
      # DEFAULT_ANIME_KITSU_TIMEOUT=

      # --------- NUVIOSTREAMS ---------
      - NUVIOSTREAMS_URL=${NUVIOSTREAMS_URL:-https://nuviostreams.hayd.uk/}
      # DEFAULT_NUVIOSTREAMS_TIMEOUT=

      # --------- TMDB COLLECTIONS ---------
      - TMDB_COLLECTIONS_URL=${TMDB_COLLECTIONS_URL:-https://61ab9c85a149-tmdb-collections.baby-beamup.club/}
      # DEFAULT_TMDB_COLLECTIONS_TIMEOUT=

      # ----------- TORRENTIO -------------
      - TORRENTIO_URL=${TORRENTIO_URL:-https://torrentio.strem.fun/}
      # DEFAULT_TORRENTIO_TIMEOUT=

      # -------- ORION STREMIO ADDON --------
      - ORION_STREMIO_ADDON_URL=${ORION_STREMIO_ADDON_URL:-https://5a0d1888fa64-orion.baby-beamup.club/}
      # DEFAULT_ORION_STREMIO_ADDON_TIMEOUT=

      # ------------ PEERFLIX --------------
      - PEERFLIX_URL=${PEERFLIX_URL:-https://peerflix-addon.onrender.com/}
      # DEFAULT_PEERFLIX_TIMEOUT=

      # -------- TORBOX STREMIO ADDON --------
      - TORBOX_STREMIO_URL=${TORBOX_STREMIO_URL:-https://stremio.torbox.app/}
      # DEFAULT_TORBOX_STREMIO_TIMEOUT=

      # -------- EASYNEWS ADDON (Standalone) --------
      - EASYNEWS_URL=${EASYNEWS_URL:-https://ea627ddf0ee7-easynews.baby-beamup.club/}
      # DEFAULT_EASYNEWS_TIMEOUT=

      # ------------ DEBRIDIO -----------
      - DEBRIDIO_URL=${DEBRIDIO_URL:-https://addon.debridio.com/}
      # DEFAULT_DEBRIDIO_TIMEOUT=

      # ------------ DEBRIDIO TVDB ------------
      - DEBRIDIO_TVDB_URL=${DEBRIDIO_TVDB_URL:-https://tvdb-addon.debridio.com/}
      # DEFAULT_DEBRIDIO_TVDB_TIMEOUT=

      # ------------ DEBRIDIO TMDB ------------
      - DEBRIDIO_TMDB_URL=${DEBRIDIO_TMDB_URL:-https://tmdb-addon.debridio.com/}
      # DEFAULT_DEBRIDIO_TMDB_TIMEOUT=

      # ------------ DEBRIDIO TV ------------
      - DEBRIDIO_TV_URL=${DEBRIDIO_TV_URL:-https://tv-addon.debridio.com/}
      # DEFAULT_DEBRIDIO_TV_TIMEOUT=

      # ------------ DEBRIDIO WATCHTOWER ------------
      - DEBRIDIO_WATCHTOWER_URL=${DEBRIDIO_WATCHTOWER_URL:-https://wt-addon.debridio.com/}
      # DEFAULT_DEBRIDIO_WATCHTOWER_TIMEOUT=

      # ------------ OPENSUBTITLES V3 ------------
      - OPENSUBTITLES_URL=${OPENSUBTITLES_URL:-https://opensubtitles-v3.strem.io/}
      # DEFAULT_OPENSUBTITLES_TIMEOUT=

      # ------------ TORRENT CATALOGS ------------
      - TORRENT_CATALOGS_URL=${TORRENT_CATALOGS_URL:-https://torrent-catalogs.strem.fun/}
      # DEFAULT_TORRENT_CATALOGS_TIMEOUT=

      # ------------ RPDB CATALOGS ------------
      - RPDB_CATALOGS_URL=${RPDB_CATALOGS_URL:-https://1fe84bc728af-rpdb.baby-beamup.club/}
      # DEFAULT_RPDB_CATALOGS_TIMEOUT=

      # ------------- DMM Cast ----------------
      # DEFAULT_DMM_CAST_TIMEOUT=

      # ----------- STREAMING CATALOGS ---------
      - STREAMING_CATALOGS_URL=${STREAMING_CATALOGS_URL:-https://7a82163c306e-stremio-netflix-catalog-addon.baby-beamup.club/}
      # DEFAULT_STREAMING_CATALOGS_TIMEOUT=

      # ----------- ANIME CATALOGS -----------
      - ANIME_CATALOGS_URL=${ANIME_CATALOGS_URL:-https://1fe84bc728af-stremio-anime-catalogs.baby-beamup.club/}
      # DEFAULT_ANIME_CATALOGS_TIMEOUT=

      # ----------- DOCTOR WHO UNIVERSE -----------
      - DOCTOR_WHO_UNIVERSE_URL=${DOCTOR_WHO_UNIVERSE_URL:-https://new-who.onrender.com}
      # DEFAULT_DOCTOR_WHO_UNIVERSE_TIMEOUT=

      # ----------- WEBSTREAMR -----------
      - WEBSTREAMR_URL=${WEBSTREAMR_URL:-https://webstreamr.hayd.uk}
      # DEFAULT_WEBSTREAMR_TIMEOUT=
      # ==============================================================================
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.aiostreams.service: aiostreams
      traefik.http.routers.aiostreams.rule: Host(`aiostreams.$DOMAIN`) || Host(`aiostreams.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.aiostreams.loadbalancer.server.port: ${AIOSTREAMS_PORT:-3005}
      homepage.group: Stremio Addons
      homepage.name: AIOStreams
      homepage.icon: aiostreams.png
      homepage.href: https://aiostreams.$DOMAIN/
      homepage.description: AIOStreams is a stremio addon that combines multiple stremio addons into one, providing additional functionality that can be used for all these addons that may not natively otherwise support them.
    cpu_shares: 10
    mem_reservation: 50M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 50M
        limits:
          cpus: 1
          memory: 1024M
    restart: always
  jackett:
    # 🔹🔹 Jackett 🔹🔹
    depends_on:
      flaresolverr:
        condition: service_healthy
    image: linuxserver/jackett
    container_name: jackett
    network_mode: service:warp
    volumes:
      - ${CONFIG_PATH:-./volumes}/jackett/config:/config
      - ${ZURG_MOUNT_PATH:-/mnt/remote/realdebrid}/blackhole:/blackhole
      - /mnt:/mnt
    environment:
      <<: *common-env
      AUTO_UPDATE: "true"
      RUN_OPTS: ""
      JACKETT_API_KEY: $JACKETT_API_KEY
    labels:
      deunhealth.restart.on.unhealthy: "true"
      homepage.group: Source Aggregator
      homepage.name: Jackett Indexer
      homepage.icon: jackett.png
      homepage.href: https://jackett.$DOMAIN/
      homepage.description: Connects your download applications with various source providers and indexers, making it easier to find and download content through your download clients.
      homepage.weight: 1
      homepage.widget.type: jackett
      homepage.widget.url: ${JACKETT_INTERNAL_URL:-http://jackett:9117}
      homepage.widget.key: $JACKETT_API_KEY
    healthcheck:
      test: ["CMD-SHELL", "curl -fs http://127.0.0.1:9117/api/v2.0/indexers/all/results/torznab?t=indexers&apikey=$JACKETT_API_KEY || exit 1"]
      interval: 30s
      timeout: 10s
      start_period: 60s
    cpu_shares: 10
    mem_reservation: 128M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 128M
        limits:
          cpus: 1
          memory: 1024M
    restart: always
  prowlarr:
    # 🔹🔹 Prowlarr 🔹🔹
    image: linuxserver/prowlarr
    container_name: prowlarr
    network_mode: service:warp
    configs:
      - source: prowlarr_indexer_proxy.json
        target: /config/prowlarr_indexer_proxy.json
        mode: 0777
      - source: prowlarr_indexers.json
        target: /etc/config/prowlarr_indexers.json
        mode: 0777
    volumes:
      - ${CONFIG_PATH:-./volumes}/prowlarr/config:/config:rw
      - ${CONFIG_PATH:-./volumes}/prowlarr/config/etc/config:/etc/config
      - /mnt:/mnt
    environment:
      <<: *common-env
      PROWLARR_API_KEY: $PROWLARR_API_KEY
    labels:
      deunhealth.restart.on.unhealthy: "true"
      homepage.group: Indexers
      homepage.name: Prowlarr
      homepage.icon: prowlarr.png
      homepage.href: https://prowlarr.$DOMAIN/
      homepage.description: An indexer manager/proxy for your Usenet and Torrent downloaders, integrating with PVR apps like Sonarr and Radarr.
      homepage.weight: 1
      homepage.widget.type: prowlarr
      homepage.widget.url: ${PROWLARR_INTERNAL_URL:-http://prowlarr:9696}
      homepage.widget.key: $PROWLARR_API_KEY
    cpu_shares: 10
    mem_reservation: 128M
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 6G
        reservations:
          cpus: '0.0'
          memory: 128M
    healthcheck:  # docker exec prowlarr ls -la /bin /usr/bin | grep -E 'curl|wget|nc|telnet|http|python|ncat|nmap'
      test: ["CMD-SHELL", "curl -fs -H \"Authorization: Bearer $PROWLARR_API_KEY\" http://127.0.0.1:9696/api/v1/system/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    restart: always
  comet:
    image: g0ldyy/comet:latest
    container_name: comet
    network_mode: service:warp
    volumes:
      - ${CONFIG_PATH:-./volumes}/stremio/addons/comet/data:/data
    environment:
      <<: *common-env
      ADDON_ID: comet.$DOMAIN
      ADDON_NAME: Comet | $DOMAIN
      FASTAPI_HOST: 0.0.0.0
      FASTAPI_PORT: ${COMET_PORT:-2020}
      FASTAPI_WORKERS: ${CPU_COUNT:-4}
      USE_GUNICORN: True
      DASHBOARD_ADMIN_PASSWORD: $SUDO_PASSWORD
      DATABASE_TYPE: sqlite
      DATABASE_PATH: /data/comet.db
      METADATA_CACHE_TTL: 2592000
      TORRENT_CACHE_TTL: 1296000
      DEBRID_CACHE_TTL: 86400
      INDEXER_MANAGER_TYPE: prowlarr  # jackett, prowlarr, none.
      INDEXER_MANAGER_URL: ${PROWLARR_INTERNAL_URL:-http://prowlarr:9696}  # http://jackett:9117, http://prowlarr:9696
      INDEXER_MANAGER_API_KEY: $PROWLARR_API_KEY  # $JACKETT_API_KEY, $PROWLARR_API_KEY
      INDEXER_MANAGER_TIMEOUT: 60
      INDEXER_MANAGER_INDEXERS: ${COMET_INDEXERS:-["animetosho", "anirena", "bitsearch", "eztv", "nyaasi", "thepiratebay", "therarbg", "yts"]}
      GET_TORRENT_TIMEOUT: 2
      DOWNLOAD_TORRENT_FILES: True
      SCRAPE_COMET: true
      COMET_URL: https://comet.elfhosted.com
      SCRAPE_ZILEAN: true
      ZILEAN_URL: https://zilean.elfhosted.com
      SCRAPE_TORRENTIO: true
      TORRENTIO_URL: https://torrentio.strem.fun
      SCRAPE_MEDIAFUSION: true
      MEDIAFUSION_URL: https://mediafusion.elfhosted.com
      STREMTHRU_URL: https://stremthru.$DOMAIN
      PROXY_DEBRID_STREAM: True
      PROXY_DEBRID_STREAM_PASSWORD: $STREMTHRU_PROXY_AUTH
      PROXY_DEBRID_STREAM_MAX_CONNECTIONS: -1
      PROXY_DEBRID_STREAM_DEBRID_DEFAULT_SERVICE: premiumize
      PROXY_DEBRID_STREAM_DEBRID_DEFAULT_APIKEY: $PREMIUMIZE_API_KEY
      REMOVE_ADULT_CONTENT: True
    labels:
      deunhealth.restart.on.unhealthy: "true"
      homepage.group: Stremio Addons
      homepage.name: Comet
      homepage.icon: comet.png
      homepage.href: https://comet.$DOMAIN/
      homepage.description: Comet is a Stremio Addon for finding media sources from various sources.
    healthcheck:  # docker exec comet ls -la /bin /usr/bin | grep -E 'curl|wget|nc|telnet|http|python|ncat|nmap'
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:${COMET_PORT:-2020}/health > /dev/null 2>&1 || exit 1"]
      interval: 1m
      timeout: 5s
      retries: 3
      start_period: 15s
    restart: always
  mediafusion:
    depends_on:
      mongodb:
        condition: service_healthy
      redis:
        condition: service_healthy
    image: mhdzumair/mediafusion:latest
    container_name: mediafusion
    network_mode: service:warp
    environment:
      <<: *common-env
      # CORE APPLICATION SETTINGS
      ADDON_NAME: "${MEDIAFUSION_ADDON_NAME:-MediaFusion}"  # The name of the MediaFusion addon
      VERSION: "1.0.0"  # The version of the MediaFusion addon
      DESCRIPTION: "MediaFusion | $DOMAIN"  # A brief description of the MediaFusion addon to show on Stremio Addon page
      BRANDING_DESCRIPTION: "${MEDIAFUSION_BRANDING_DESCRIPTION:-}"  # Additional branding description text
      CONTACT_EMAIL: "webmaster@$DOMAIN"  # The contact email for the MediaFusion addon
      HOST_URL: "https://mediafusion.$DOMAIN"  # The URL where MediaFusion is hosted (required)
      POSTER_HOST_URL: "https://mediafusion.$DOMAIN"  # The URL where poster images are served from (default: Same as host_url)
      SECRET_KEY: "${MEDIAFUSION_SECRET_KEY:-c75ac92df70c4383c8d1bb37f65454f9}"  # A 32-character secret key for securely signing the session (required)
      API_PASSWORD: "$SUDO_PASSWORD"  # The password for accessing the API endpoints (required)
      LOGGING_LEVEL: "INFO"  # The logging level of the application (DEBUG, INFO, WARNING, ERROR, CRITICAL)
      LOGO_URL: "${MEDIAFUSION_LOGO_URL:-}"  # The URL of the MediaFusion logo
      IS_PUBLIC_INSTANCE: "${MEDIAFUSION_IS_PUBLIC_INSTANCE:-False}"  # Set to True for community instances that don't require authentication except for /scraper endpoint
      MIN_SCRAPING_VIDEO_SIZE: "${MEDIAFUSION_MIN_SCRAPING_VIDEO_SIZE:-26214400}"  # Minimum video size in bytes (25 MB) for scraping
      METADATA_PRIMARY_SOURCE: "${MEDIAFUSION_METADATA_PRIMARY_SOURCE:-tmdb}"  # Primary source for metadata. Options: "imdb" or "tmdb"
      # STREAMING PROVIDER SETTINGS
      DISABLED_PROVIDERS: "${MEDIAFUSION_DISABLED_PROVIDERS:-[]}"  # List of disabled streaming providers (p2p, realdebrid, seedr, debridlink, alldebrid, offcloud, pikpak, torbox, premiumize, qbittorrent, stremthru, easydebrid)
      # DATABASE AND CACHE SETTINGS
      MONGO_URI: "${MEDIAFUSION_MONGO_URI:-mongodb://${MONGODB_HOSTNAME:-mongodb}:27017/mediafusion}"  # MongoDB connection URI (required)
      DB_MAX_CONNECTIONS: "${MEDIAFUSION_DB_MAX_CONNECTIONS:-50}"  # Maximum database connections
      REDIS_URL: "${MEDIAFUSION_REDIS_URL:-redis://redis:6379}"  # Redis service URL for caching and tasks
      REDIS_MAX_CONNECTIONS: "${MEDIAFUSION_REDIS_MAX_CONNECTIONS:-100}"  # Maximum Redis connections
      # EXTERNAL SERVICE SETTINGS
#      REQUESTS_PROXY_URL: "${MEDIAFUSION_REQUESTS_PROXY_URL:-}"  # Optional proxy URL for requests
      PLAYWRIGHT_CDP_URL: "${PLAYWRIGHT_CDP_URL:-ws://browserless:3000?blockAds=true&stealth=true}"  # Playwright CDP service URL
      FLARESOLVERR_URL: "${FLARESOLVERR_URL:-http://flaresolverr:8191/v1}"  # FlareSolverr service URL
      TMDB_API_KEY: "$TMDB_API_KEY"  # TMDB API key for metadata fetching
      # PROWLARR SETTINGS
      IS_SCRAP_FROM_PROWLARR: "False"  # Enable/disable Prowlarr scraping
      PROWLARR_API_KEY: "$PROWLARR_API_KEY"  # Prowlarr API key
      PROWLARR_URL: "${PROWLARR_INTERNAL_URL:-http://prowlarr:9696}"  # Prowlarr service URL
      PROWLARR_LIVE_TITLE_SEARCH: "${MEDIAFUSION_PROWLARR_LIVE_TITLE_SEARCH:-True}"  # Enable live title search in Prowlarr
      PROWLARR_BACKGROUND_TITLE_SEARCH: "${MEDIAFUSION_PROWLARR_BACKGROUND_TITLE_SEARCH:-True}"  # Enable background title search
      PROWLARR_SEARCH_QUERY_TIMEOUT: "${MEDIAFUSION_PROWLARR_SEARCH_QUERY_TIMEOUT:-30}"  # Search query timeout in seconds
      PROWLARR_IMMEDIATE_MAX_PROCESS: "${MEDIAFUSION_PROWLARR_IMMEDIATE_MAX_PROCESS:-10}"  # Max immediate processes
      PROWLARR_IMMEDIATE_MAX_PROCESS_TIME: "${MEDIAFUSION_PROWLARR_IMMEDIATE_MAX_PROCESS_TIME:-15}"  # Max process time in seconds
      PROWLARR_SEARCH_INTERVAL_HOUR: "${MEDIAFUSION_PROWLARR_SEARCH_INTERVAL_HOUR:-72}"  # Search interval in hours
      PROWLARR_FEED_SCRAPE_INTERVAL_HOUR: "${MEDIAFUSION_PROWLARR_FEED_SCRAPE_INTERVAL_HOUR:-3}"  # Feed scraping interval in hours
      # JACKETT SETTINGS
      IS_SCRAP_FROM_JACKETT: "True"  # Enable/disable Jackett scraping
      JACKETT_URL: "${JACKETT_INTERNAL_URL:-http://jackett:9117}"  # Jackett service URL
      JACKETT_API_KEY: "$JACKETT_API_KEY"  # Jackett API key
      JACKETT_SEARCH_INTERVAL_HOUR: "72"  # Search interval in hours
      JACKETT_SEARCH_QUERY_TIMEOUT: "30"  # Search query timeout in seconds
      JACKETT_IMMEDIATE_MAX_PROCESS: "10"  # Max immediate processes
      JACKETT_IMMEDIATE_MAX_PROCESS_TIME: "15"  # Max process time in seconds
      JACKETT_LIVE_TITLE_SEARCH: "True"  # Enable live title search
      JACKETT_BACKGROUND_TITLE_SEARCH: "True"  # Enable background title search
      JACKETT_FEED_SCRAPE_INTERVAL_HOUR: "3"  # Feed scraping interval in hours
      # ZILEAN SETTINGS
      IS_SCRAP_FROM_ZILEAN: "True"  # Enable/disable Zilean scraping
      ZILEAN_SEARCH_INTERVAL_HOUR: "24"  # Search interval in hours
      ZILEAN_URL: "${ZILEAN_URL:-https://zilean.elfhosted.com}"  # Zilean service URL (https://zilean.elfhosted.com has zero rate limits)
      # TORRENTIO SETTINGS
      IS_SCRAP_FROM_TORRENTIO: "${MEDIAFUSION_IS_SCRAP_FROM_TORRENTIO:-True}"  # Enable/disable Torrentio scraping
      TORRENTIO_SEARCH_INTERVAL_DAYS: "${MEDIAFUSION_TORRENTIO_SEARCH_INTERVAL_DAYS:-3}"  # Search interval in days
      TORRENTIO_URL: "${TORRENTIO_URL:-https://torrentio.strem.fun}"  # Torrentio service URL
      # MEDIAFUSION SETTINGS
      IS_SCRAP_FROM_MEDIAFUSION: "${MEDIAFUSION_IS_SCRAP_FROM_MEDIAFUSION:-True}"  # Enable/disable MediaFusion scraping
      MEDIAFUSION_SEARCH_INTERVAL_DAYS: "${MEDIAFUSION_MEDIAFUSION_SEARCH_INTERVAL_DAYS:-3}"  # Search interval in days
      MEDIAFUSION_URL: "${MEDIAFUSION_URL:-https://mediafusion.elfhosted.com}"  # MediaFusion service URL
      SYNC_DEBRID_CACHE_STREAMS: "${MEDIAFUSION_SYNC_DEBRID_CACHE_STREAMS:-False}"  # Enable syncing debrid cache streams
      # BT4G SETTINGS
      IS_SCRAP_FROM_BT4G: "${MEDIAFUSION_IS_SCRAP_FROM_BT4G:-True}"  # Enable/disable BT4G scraping
      BT4G_URL: "${BT4G_URL:-https://bt4gprx.com}"  # BT4G service URL
      BT4G_SEARCH_INTERVAL_HOUR: "${MEDIAFUSION_BT4G_SEARCH_INTERVAL_HOUR:-72}"  # Search interval in hours
      BT4G_SEARCH_TIMEOUT: "${MEDIAFUSION_BT4G_SEARCH_TIMEOUT:-10}"  # Search timeout in seconds
      BT4G_IMMEDIATE_MAX_PROCESS: "${MEDIAFUSION_BT4G_IMMEDIATE_MAX_PROCESS:-15}"  # Max immediate processes
      BT4G_IMMEDIATE_MAX_PROCESS_TIME: "${MEDIAFUSION_BT4G_IMMEDIATE_MAX_PROCESS_TIME:-15}"  # Max process time in seconds
      # PREMIUMIZE SETTINGS
      #PREMIUMIZE_OAUTH_CLIENT_ID: "$PREMIUMIZE_OAUTH_CLIENT_ID"  # Premiumize OAuth client ID
      #PREMIUMIZE_OAUTH_CLIENT_SECRET: "$PREMIUMIZE_OAUTH_CLIENT_SECRET"  # Premiumize OAuth client secret
      # TELEGRAM SETTINGS
      TELEGRAM_BOT_TOKEN: "$TELEGRAM_BOT_TOKEN"  # Telegram bot token for notifications about contribution streams
      TELEGRAM_CHAT_ID: "$TELEGRAM_CHAT_ID"  # Telegram chat ID for notifications
      # CONTENT FILTERING SETTINGS
      ADULT_CONTENT_REGEX_KEYWORDS: "\\b(porn|xxx|sex|nude|naked|erotic|fetish|bdsm|milf|anal|oral|gangbang|threesome|masturbat|orgasm|cumshot|blowjob|handjob|footjob|creampie|facial|bukkake|hentai|yaoi|yuri|ecchi|doujin|camgirl|webcam|stripper|escort|prostitut|brothel|redlight|peepshow|voyeur|exhibitionist|swinger|orgy|hardcore|softcore|playboy|penthouse|hustler|brazzers|bangbros|realitykings|naughtyamerica|digitalplayground|wickedpictures|evilangel|kink|publicagent|fakehub|teamskeet|mofos|twistys|metart|sexart|x-art|joymii|nubiles|18eighteen|lolita|incest|taboo|kinky|slutty|horny|sexy|sensual|seductive|raw|perverted|depraved)\\b"  # Regex pattern for filtering adult content
      ADULT_CONTENT_FILTER_IN_TORRENT_TITLE: "True"  # Enable adult content filtering in torrent titles
      # FEATURE TOGGLES
      IS_SCRAP_FROM_YTS: "True"  # Enable/disable YTS scraping
      ENABLE_RATE_LIMIT: "True"  # Enable/disable rate limiting
      VALIDATE_M3U8_URLS_LIVENESS: "True"  # Validate M3U8 URLs for liveness
      STORE_STREMTHRU_MAGNET_CACHE: "False"  # Store StremThru magnet cache
      SCRAPE_WITH_AKA_TITLES: "True"  # Include alternative titles in scraping
      ENABLE_FETCHING_TORRENT_METADATA_FROM_P2P: "False"  # Enable fetching torrent metadata from P2P (Caution: may raise DMCA issues)
      # TIME-RELATED SETTINGS
      META_CACHE_TTL: "1800"  # Metadata cache TTL in seconds (Default: 1800 seconds = 30 minutes)
      WORKER_MAX_TASKS_PER_CHILD: "20"  # Max tasks per worker child process
      # SCHEDULER SETTINGS
      DISABLE_ALL_SCHEDULER: "False"  # Disable all schedulers
      BACKGROUND_SEARCH_INTERVAL_HOURS: "72"  # Background search interval in hours
      BACKGROUND_SEARCH_CRONTAB: "*/5 * * * *"  # Background search schedule
      TAMILMV_SCHEDULER_CRONTAB: "0 */3 * * *"  # TamilMV scheduler crontab expression
      TAMIL_BLASTERS_SCHEDULER_CRONTAB: "0 */6 * * *"  # Tamil Blasters scheduler crontab expression
      FORMULA_TGX_SCHEDULER_CRONTAB: "*/30 * * * *"  # Formula TGX scheduler crontab expression
      NOWMETV_SCHEDULER_CRONTAB: "0 0 * * *"  # NowMeTV scheduler crontab expression
      NOWSPORTS_SCHEDULER_CRONTAB: "0 10 * * *"  # NowSports scheduler crontab expression
      TAMILULTRA_SCHEDULER_CRONTAB: "0 8 * * *"  # TamilUltra scheduler crontab expression
      VALIDATE_TV_STREAMS_IN_DB_CRONTAB: "0 */6 * * *"  # Validate TV streams in DB crontab expression
      SPORT_VIDEO_SCHEDULER_CRONTAB: "*/20 * * * *"  # Sport video scheduler crontab expression
      DLHD_SCHEDULER_CRONTAB: "25 * * * *"  # DLHD scheduler crontab expression
      MOTOGP_TGX_SCHEDULER_CRONTAB: "0 5 * * *"  # MotoGP TGX scheduler crontab expression
      UPDATE_SEEDERS_CRONTAB: "0 0 * * *"  # Update seeders crontab expression
      ARAB_TORRENTS_SCHEDULER_CRONTAB: "0 0 * * *"  # Arab Torrents scheduler crontab expression
      WWE_TGX_SCHEDULER_CRONTAB: "10 */3 * * *"  # WWE TGX scheduler crontab expression
      UFC_TGX_SCHEDULER_CRONTAB: "30 */3 * * *"  # UFC TGX scheduler crontab expression
      MOVIES_TV_TGX_SCHEDULER_CRONTAB: "0 * * * *"  # Movies TV TGX scheduler crontab expression
      PROWLARR_FEED_SCRAPER_CRONTAB: "0 */3 * * *"  # Prowlarr feed scraper crontab expression
      JACKETT_FEED_SCRAPER_CRONTAB: "0 */3 * * *"  # Jackett feed scraper crontab expression
      CLEANUP_EXPIRED_SCRAPER_TASK_CRONTAB: "0 * * * *"  # Cleanup expired scraper task crontab expression
      CLEANUP_EXPIRED_CACHE_TASK_CRONTAB: "0 0 * * *"  # Cleanup expired cache task crontab expression
    labels:
      deunhealth.restart.on.unhealthy: "true"
      homepage.group: Stremio Addons
      homepage.name: MediaFusion
      homepage.icon: mediafusion.png
      homepage.href: https://mediafusion.$DOMAIN/
      homepage.description: MediaFusion is a AIO media stream aggregator addon for Stremio/Kodi.
    cpu_shares: 10
    mem_reservation: 512M
    deploy:
      resources:
        reservations:
          cpus: 0.01
          memory: 512M
        limits:
          cpus: 1
          memory: 1G
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8000/health > /dev/null 2>&1 || exit 1"]
      interval: 1m
      timeout: 10s
      retries: 5
      start_period: "1m"
    restart: always
  mediaflow-proxy:
    image: mhdzumair/mediaflow-proxy
    container_name: mediaflow-proxy
    network_mode: service:warp
    environment:
      <<: *common-env
      PORT: ${MEDIAFLOW_PROXY_PORT:-8888}
      API_PASSWORD: $MEDIAFLOW_PROXY_API_PASSWORD
      ENABLE_STREAMING_PROGRESS: ${MEDIAFLOW_PROXY_ENABLE_STREAMING_PROGRESS:-false}
    labels:
      deunhealth.restart.on.unhealthy: "true"
      homepage.name: MediaFlow Proxy
      homepage.icon: mediaflow-proxy.png
      homepage.description: MediaFlow Proxy is a M3U/HSTS proxy for Stremio/Kodi.
      homepage.group: Stremio Addons
      homepage.href: https://mediaflow-proxy.$DOMAIN/
    healthcheck:
      test: ["CMD-SHELL", "python3 -c \"import socket; s=socket.socket(); s.connect(('127.0.0.1', ${MEDIAFLOW_PROXY_PORT:-8888})); print('Healthcheck passed')\""]
      interval: 1m
      timeout: 10s
      retries: 5
      start_period: 10s
    restart: always

  stremthru:
    depends_on:
      redis:
        condition: service_healthy
    image: muniftanjim/stremthru:latest
    container_name: stremthru
    hostname: stremthru
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${STREMTHRU_IPV4_ADDRESS:-10.76.128.99}
    volumes:
      - ${CONFIG_PATH:-./volumes}/stremio/addons/stremthru/app/data:/app/data
    environment:
      <<: *common-env
      STREMTHRU_LOG_LEVEL: ${STREMTHRU_LOG_LEVEL:-INFO}  # DEBUG, INFO, WARN, ERROR
      STREMTHRU_LOG_FORMAT: ${STREMTHRU_LOG_FORMAT:-text}  # json, text
      STREMTHRU_PROXY_AUTH: ${STREMTHRU_PROXY_AUTH:-admin:admin}
      STREMTHRU_AUTH_ADMIN: ${STREMTHRU_AUTH_ADMIN:-admin}

      # A list of store credentials per user. In a comma separated list of username:store_name:store_token.
      # You can also use * as the username to apply to all users.
      # This lets you use different API keys for different users.
      # e.g. something like *:realdebrid:abc...xyz would mean for all users, this API key would be used for Real Debrid.
      # (Not needed if using StremThru as a external proxy within other apps/addons like AIOStreams)
      STREMTHRU_STORE_AUTH: $STREMTHRU_STORE_AUTH

      # A list of proxy configurations per store. This lets you control what is proxied based on what store its from.
      # The format is a comma separated list of <store>:<true/false>
      # You can use * to apply to all stores.
      # e.g. *:true,premiumize:false would mean all stores are proxied, except for premiumize.
      # (Does not affect proxying of stream links generated by other apps/addons like AIOStreams)
      STREMTHRU_STORE_CONTENT_PROXY: ${STREMTHRU_STORE_CONTENT_PROXY:-*:true,premiumize:false}

      # This is the maximum number of connections to the proxy per user.
      # In the format of a comma separated list of <username>:<number of connections> pairs.
      # e.g. user1:10,user2:5 would mean user1 can have 10 connections, and user2 can have 5 connections.
      # If you want to set a limit for all users, use * as the username.
      # A connection limit of 0 means no limit.
      # (This DOES affect the proxying of stream links generated by other apps/addons like AIOStreams)
      STREMTHRU_CONTENT_PROXY_CONNECTION_LIMIT: ${STREMTHRU_CONTENT_PROXY_CONNECTION_LIMIT:-0}
      # ================================
      # This is the configuration for the tunnel.
      # The tunnel is separate to the actual proxying of your addons. It lets you configure a
      # HTTP proxy to use for requests. Configuring a hostname to not be tunneled doesn't affect the proxying of the addon itself,
      # only that it won't also be tunneled through the HTTP proxy.
      #
      # If you do not need a tunnel, you can comment out the STREMTHRU_HTTP_PROXY variable.
      #
      # This is the URL of the tunnel.
      # warp: http://warp:1080 or gluetun: http://gluetun:8080
      # (Must have the warp or gluetun profile running to use it as a tunnel)
      STREMTHRU_HTTP_PROXY: ${STREMTHRU_HTTP_PROXY:-http://warp:1080}
      #STREMTHRU_HTTP_PROXY: ${STREMTHRU_HTTP_PROXY:-}

      # This allows you to configure what to use the tunnel for.
      # The format is <domain>:<true/false>,<domain>:<true/false>,<domain>:<true/false>.
      # * represents all domains.
      # The following configuration only routes requests to torrentio.strem.fun through the tunnel.
      # This is necessary when Torrentio blocks your VPS and we want to access the original RD link.
      # It wouldn't use the tunnel for the actual playback link, only to get access to the streaming link through Torrentio.
      #STREMTHRU_TUNNEL: '*:false,torrentio.strem.fun:true'
      # all disabled
      #STREMTHRU_TUNNEL: ${STREMTHRU_TUNNEL:-*:false}
      # all enabled:
      STREMTHRU_TUNNEL: ${STREMTHRU_TUNNEL:-*:true}
      # This setting is the same as above, but has a higher priority, and used specifically to configure specific stores.
      # The format is <store>:<true/api>.
      # Using api as the value means only the api access will be routed through the tunnel, not the playback links.
      # Using true would route everything for that store through the tunnel.
      #STREMTHRU_STORE_TUNNEL: 'realdebrid:api'
      # all disabled
      #STREMTHRU_STORE_TUNNEL: ${STREMTHRU_STORE_TUNNEL:-*:false}
      # all enabled:
      STREMTHRU_STORE_TUNNEL: ${STREMTHRU_STORE_TUNNEL:-*:true}
      # In most cases, the defaults above are fine. If you want to route everything through the tunnel (e.g. route everything through a VPN),
      # simply leave the STREMTHRU_TUNNEL and STREMTHRU_STORE_TUNNEL variables blank.
      # ================================
      # This lets you enable or disable features in StremThru.
      # The format is a comma separated list of features, optionally prefixed with a - to disable them.
      # List of features:
      # - anime
      # - dmm_hashlist - Whether to scrape DMM hashlists for torrents.
      # - imdb_title
      # - stremio_list
      # - stremio_p2p
      # - stremio_sidekick
      # - stremio_store
      # - stremio_torz
      # - stremio_wrap
      # e.g. -dmm_hashlist,-imdb_title would disable the DMM hashlists and IMDB title features.
      # ================================
      STREMTHRU_FEATURE: ${STREMTHRU_FEATURE:-"stremio_store,stremio_wrap"}
      STREMTHRU_DATABASE_URI: ${STREMTHRU_DATABASE_URI:-sqlite://./data/stremthru.db}
      STREMTHRU_REDIS_URI: redis://${REDIS_IPV4_ADDRESS:-10.76.128.87}:6379
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.stremthru.service: stremthru
      traefik.http.routers.stremthru.rule: Host(`stremthru.$DOMAIN`) || Host(`stremthru.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.stremthru.loadbalancer.server.port: 8080
      homepage.group: Stremio Addons
      homepage.name: StremThru
      homepage.icon: stremthru.png
      homepage.href: https://stremthru.$DOMAIN/
      homepage.description: StremThru is tunnel/proxy for debrid services.
    restart: always

  gptr:
    build:
      #context: https://github.com/th3w1zard1/gpt-researcher.git#master  # :/frontend/nextjs
      context: ./src/gpt-researcher
      dockerfile: Dockerfile.fullstack
    image: th3w1zard1/ai-researchwizard:latest
    container_name: gptr
    hostname: gptr
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${GPT_RESEARCHER_IPV4_ADDRESS:-10.76.128.43}
    expose:
      - 3000  # Nginx-proxied access to GPTR.
      - 3001  # Internal NextJS GPTR site
      - 8000  # Static GPTR
      - 8080  # MCP Server Port
    volumes:
      - ${CONFIG_PATH:-./volumes}/gptr/logs:/usr/src/app/logs
      - ${CONFIG_PATH:-./volumes}/gptr/outputs:/usr/src/app/outputs
      - ${CONFIG_PATH:-./volumes}/gptr/reports:/usr/src/app/reports
    stdin_open: true
    environment:
      - ANTHROPIC_API_KEY=$ANTHROPIC_API_KEY
      - BRAVE_API_KEY=$BRAVE_API_KEY
      - DEEPSEEK_API_KEY=$DEEPSEEK_API_KEY
      - EXA_API_KEY=$EXA_API_KEY
      - FIRECRAWL_API_KEY=$FIRECRAWL_API_KEY
      - FIRE_CRAWL_API_KEY=$FIRE_CRAWL_API_KEY
      - GEMINI_API_KEY=$GEMINI_API_KEY
      - GLAMA_API_KEY=$GLAMA_API_KEY
      - GROQ_API_KEY=$GROQ_API_KEY
      - HF_TOKEN=$HF_TOKEN
      - HUGGINGFACE_ACCESS_TOKEN=$HUGGINGFACE_ACCESS_TOKEN
      - HUGGINGFACE_API_TOKEN=$HUGGINGFACE_API_TOKEN
      - LANGCHAIN_API_KEY=$LANGCHAIN_API_KEY
      - MISTRAL_API_KEY=$MISTRAL_API_KEY
      - MISTRALAI_API_KEY=$MISTRALAI_API_KEY
      - OPENAI_API_KEY=$OPENAI_API_KEY
      - OPENROUTER_API_KEY=$OPENROUTER_API_KEY
      - PERPLEXITY_API_KEY=$PERPLEXITY_API_KEY
      - PERPLEXITYAI_API_KEY=$PERPLEXITYAI_API_KEY
      - REPLICATE_API_KEY=$REPLICATE_API_KEY
      - REVID_API_KEY=$REVID_API_KEY
      - SAMBANOVA_API_KEY=$SAMBANOVA_API_KEY
      - SEARCH1API_KEY=$SEARCH1API_KEY
      - SERPAPI_API_KEY=$SERPAPI_API_KEY
      - TAVILY_API_KEY=$TAVILY_API_KEY
      - TOGETHERAI_API_KEY=$TOGETHERAI_API_KEY
      - UNIFY_API_KEY=$UNIFY_API_KEY
      - UPSTAGE_API_KEY=$UPSTAGE_API_KEY
      - UPSTAGEAI_API_KEY=$UPSTAGEAI_API_KEY
      - YOU_API_KEY=$YOU_API_KEY
      - CHOKIDAR_USEPOLLING=true
      - LOGGING_LEVEL=DEBUG
      - NEXT_PUBLIC_GA_MEASUREMENT_ID=$NEXT_PUBLIC_GA_MEASUREMENT_ID
      - NEXT_PUBLIC_GPTR_API_URL=https://gptr.$DOMAIN
      - LANGSMITH_TRACING=true
      - LANGSMITH_ENDPOINT=https://api.smith.langchain.com
      - LANGSMITH_API_KEY=$LANGSMITH_API_KEY
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.gptr.service: gptr
      traefik.http.routers.gptr.rule: Host(`gptr-nextjs.$DOMAIN`) || Host(`gptr-nextjs.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.gptr.loadbalancer.server.port: 3000
      traefik.http.routers.gptr-legacy.service: gptr-legacy
      traefik.http.routers.gptr-legacy.rule: Host(`gptr.$DOMAIN`) || Host(`gptr.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.gptr-legacy.loadbalancer.server.port: 8000
      traefik.http.routers.gptr-mcp.service: gptr-mcp
      traefik.http.routers.gptr-mcp.rule: Host(`gptr-mcp.$DOMAIN`) || Host(`gptr-mcp.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.gptr-mcp.loadbalancer.server.port: 8080
      homepage.group: AI
      homepage.name: AI Research Wizard
      homepage.description: AI Research Wizard is a web scraper and researcher that uses AI to help you find information quickly.
      homepage.icon: gptr.png
      homepage.href: https://gptr.$DOMAIN/
    healthcheck:  # docker exec gptr ls -la /bin /usr/bin | grep -E 'curl|wget|nc|telnet|http|python|ncat|nmap'
      test: ["CMD-SHELL", "(wget -qO- http://127.0.0.1:3000 > /dev/null 2>&1 || exit 1) && (wget -qO- http://127.0.0.1:8000 > /dev/null 2>&1 || exit 1) || exit 1"]  # && (wget -qO- http://127.0.0.1:8080 > /dev/null 2>&1) || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 2m
    restart: always
  qdrant:
    # 🔹🔹 Qdrant 🔹🔹
    profiles:
      - extras
    image: qdrant/qdrant:latest
    container_name: qdrant
    hostname: qdrant
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${QDRANT_IPV4_ADDRESS:-10.76.128.44}
    expose:
      - 6333
    volumes:
      - ${CONFIG_PATH:-./volumes}/qdrant/storage:/qdrant/storage
    environment:
      <<: *common-env
      QDRANT_STORAGE_PATH: /qdrant/storage
      QDRANT_STORAGE_TYPE: disk
      QDRANT_STORAGE_DISK_PATH: /qdrant/storage
      QDRANT_STORAGE_DISK_TYPE: disk
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.qdrant.rule: Host(`qdrant.$DOMAIN`) || Host(`qdrant.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.qdrant.loadbalancer.server.port: 6333
      homepage.group: AI
      homepage.name: Qdrant
      homepage.icon: qdrant.png
      homepage.href: https://qdrant.$DOMAIN/
      homepage.description: Qdrant is a vector database for storing and querying vectors.
    restart: always
  lobechat:
    # 🔹🔹 LobeChat 🔹🔹
    profiles:
      - extras
    image: lobehub/lobe-chat:latest
    container_name: lobechat
    hostname: ${LOBECHAT_HOSTNAME:-lobechat}
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:  # https://docs.docker.com/engine/network/drivers/bridge/#differences-between-user-defined-bridges-and-the-default-bridge
        ipv4_address: ${LOBECHAT_IPV4_ADDRESS:-10.76.128.46}
    expose:
      - 3210
    volumes:
      - ${CONFIG_PATH:-./volumes}/lobechat/host_config:/configs/lobechat/host_config
    env_file:
      - ${ROOT_PATH:-.}/.env
    environment:
      <<: *common-env
      # Basic configuration
      NEXT_PUBLIC_BASE_URL: https://lobechat.$DOMAIN
      ACCESS_CODE: $SUDO_PASSWORD

      # AI Provider API Keys
      OPENAI_API_KEY: $OPENAI_API_KEY
      ANTHROPIC_API_KEY: $ANTHROPIC_API_KEY
      GOOGLE_API_KEY: $GOOGLE_API_KEY
      PERPLEXITY_API_KEY: $PERPLEXITY_API_KEY
      MISTRAL_API_KEY: $MISTRAL_API_KEY
      GROQ_API_KEY: $GROQ_API_KEY
      OPENROUTER_API_KEY: $OPENROUTER_API_KEY

      # Optional configurations
      NEXT_PUBLIC_ANALYTICS_VERCEL: ${LOBECHAT_ANALYTICS_VERCEL:-}
      NEXT_PUBLIC_ANALYTICS_POSTHOG: ${LOBECHAT_ANALYTICS_POSTHOG:-}
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.lobechat.rule: Host(`lobechat.$DOMAIN`) || Host(`lobechat.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.lobechat.loadbalancer.server.port: 3210
      homepage.group: AI
      homepage.name: LobeChat
      homepage.icon: lobechat.png
      homepage.href: https://lobechat.$DOMAIN/
      homepage.description: Modern AI chat interface supporting multiple LLM providers
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:3210 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    restart: always

  meilisearch:
    # 🔹🔹 Meilisearch 🔹🔹
    profiles:
      - extras
    image: getmeili/meilisearch:v1.5
    container_name: meilisearch
    hostname: meilisearch
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${MEILISEARCH_IPV4_ADDRESS:-10.76.128.124}
    environment:
      - MEILI_MASTER_KEY=$MEILI_MASTER_KEY
    volumes:
      - meili_data:/meili_data
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.meilisearch.middlewares: nginx-auth@file
      traefik.http.routers.meilisearch.rule: Host(`meilisearch.$DOMAIN`) || Host(`meilisearch.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.meilisearch.loadbalancer.server.port: 7700
      homepage.group: Search
      homepage.name: Meilisearch
      homepage.icon: meilisearch.png
      homepage.href: https://meilisearch.$DOMAIN/
      homepage.description: Meilisearch is a fast, easy to use, and reliable search engine.
      homepage.weight: 0
      homepage.widget.type: meilisearch
      homepage.widget.url: ${MEILI_INTERNAL_URL:-http://meilisearch:7700}
      homepage.widget.key: $MEILI_MASTER_KEY
    restart: always

  homer:
    # 🔹🔹 Homer Dashboard 🔹🔹
    image: docker.io/b4bz/homer:latest
    container_name: homer
    hostname: homer
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${HOMER_IPV4_ADDRESS:-10.76.128.106}
    expose:
      - 8080
    volumes:
      - ${CONFIG_PATH:-./volumes}/homer/assets:/www/assets
    configs:
      - source: homer-config.yml
        target: /www/assets/config.yml
    environment:
      <<: *common-env
      UID: ${PUID:-1001}
      GID: ${PGID:-988}
      INIT_ASSETS: 1
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.homer.rule: Host(`homer.$DOMAIN`) || Host(`homer.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.homer.loadbalancer.server.port: 8080
      homepage.group: Dashboards
      homepage.name: Homer
      homepage.icon: http://127.0.0.1:8080/assets/icons/pwa-192x192.png
      homepage.href: https://homer.$DOMAIN/
      homepage.description: Static homepage dashboard
    cpu_shares: 10
    mem_reservation: 6M
    mem_limit: 128M
    deploy:
      resources:
        reservations:
          cpus: 0.01
          memory: 6M
        limits:
          cpus: 0.1
          memory: 128M
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:8080 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: always

  wizarr:
    # 🔹🔹 Wizarr User Management 🔹🔹
    image: ghcr.io/wizarrrr/wizarr:latest
    container_name: wizarr
    hostname: wizarr
    extra_hosts: *common-hostname-aliases
    networks:
      publicnet:
        ipv4_address: ${WIZARR_IPV4_ADDRESS:-10.76.128.107}
    expose:
      - 5690
    volumes:
      - ${CONFIG_PATH:-./volumes}/wizarr/database:/data/database
      - ${CONFIG_PATH:-./volumes}/wizarr/config:/config
    environment:
      <<: *common-env
      APP_URL: http://wizarr:5690
      DISABLE_BUILTIN_AUTH: ${WIZARR_DISABLE_BUILTIN_AUTH:-true}
      PLEX_URL: http://plex:32400
      PLEX_TOKEN: $PLEX_TOKEN
    labels:
      deunhealth.restart.on.unhealthy: "true"
      traefik.enable: "true"
      traefik.http.routers.wizarr.rule: Host(`wizarr.$DOMAIN`) || Host(`wizarr.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.wizarr.loadbalancer.server.port: 5690
      homepage.group: Media Management
      homepage.name: Wizarr
      homepage.icon: http://127.0.0.1:5690/static/wizarr-logo.png
      homepage.href: https://wizarr.$DOMAIN/
      homepage.description: User invitation and management system for Plex
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:5690/static/wizarr-logo.png || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    restart: always

  deluge:
    # 🔹🔹 Deluge 🔹🔹
    profiles:
      - torrenting
    image: docker.io/linuxserver/deluge
#    image: ghcr.io/geek-cookbook/deluge  # gluetun + deluge AIO image.
    container_name: deluge
    network_mode: service:warp
    volumes:
      - ${DATA_PATH:-/mnt/data}/downloads:/downloads:rw
      - ${CONFIG_PATH:-./volumes}/deluge:/config:rw
    environment:
      <<: *common-env
      DELUGE_LOGLEVEL: ${DELUGE_LOGLEVEL:-info}
    labels:
      deunhealth.restart.on.unhealthy: "true"
      homepage.group: BitTorrent Clients
      homepage.name: Deluge
      homepage.icon: deluge.png
      homepage.href: https://deluge.$DOMAIN/
      homepage.description: A lightweight, cross-platform BitTorrent client designed for downloading files efficiently and securely from the internet.
      homepage.weight: 2
      homepage.widget.type: deluge
      homepage.widget.url: ${DELUGE_INTERNAL_URL:-http://deluge:8112}
      homepage.widget.username: ${DELUGE_USERNAME:-admin}
      homepage.widget.password: ${DELUGE_PASSWORD:-admin}
    restart: always
  transmission:
    # 🔹🔹 Transmission 🔹🔹
    profiles:
      - torrenting
    image: linuxserver/transmission
    container_name: transmission
    network_mode: service:warp
    volumes:
      - ${CONFIG_PATH:-./volumes}/transmission/data:/config
      - ${DATA_PATH:-/mnt/data}/watch:/watch
      - ${DATA_PATH:-/mnt/data}/downloads/complete:/downloads/complete
      - ${DATA_PATH:-/mnt/data}/downloads/incomplete:/downloads/incomplete
    environment:
      <<: *common-env
      #      HOST_WHITELIST: ${TRANSMISSION_HOST_WHITELIST:-}
      USER: ${TRANSMISSION_USER:-admin}
      PASS: ${TRANSMISSION_PASS:-admin}
      PEERPORT: ${TRANSMISSION_PEERPORT:-51413}
      TRANSMISSION_WEB_HOME: ${TRANSMISSION_WEB_HOME:-}
    labels:
      deunhealth.restart.on.unhealthy: "true"
      homepage.group: BitTorrent Clients
      homepage.name: Transmission BitTorrent Client
      homepage.icon: transmission.png
      homepage.href: https://transmission.$DOMAIN/
      homepage.description: A lightweight and efficient BitTorrent client for downloading and managing torrent files, known for its simplicity and speed.
      homepage.weight: 2
      homepage.widget.type: transmission
      homepage.widget.url: ${TRANSMISSION_INTERNAL_URL:-http://transmission:9091}
      homepage.widget.username: ${TRANSMISSION_USER:-admin}
      homepage.widget.password: ${TRANSMISSION_PASS:-admin}
    restart: always
  zurg:
    depends_on:
      rclone-zurg:
        condition: service_healthy
    image: ghcr.io/debridmediamanager/zurg-testing:latest
    container_name: zurg
    network_mode: service:warp
    volumes:
      - ${CONFIG_PATH:-./volumes}/zurg/app/data:/app/data
    configs:
      - source: zurg-config.yml
        target: /app/config.yml
        mode: 0775
      - source: zurg-cli_debrid_update.sh
        target: /app/plex_update.sh
        mode: 0775
    environment:
      <<: *common-env
      DOMAIN: $DOMAIN
      TS_HOSTNAME: $TS_HOSTNAME
      K8S_APP_NAME: zurg
      ELF_APP_NAME: zurg
      ZURG_LOG_LEVEL: ${ZURG_LOG_LEVEL:-debug}
      REALDEBRID_API_KEY: $REALDEBRID_API_KEY
      REAL_DEBRID_API_KEY: $REAL_DEBRID_API_KEY
      REALDEBRID_TOKEN: $REALDEBRID_TOKEN
    labels:
      deunhealth.restart.on.unhealthy: "true"
      homepage.group: Download Clients
      homepage.name: Zurg
      homepage.icon: zurg.png
      homepage.href: https://zurg.$DOMAIN/
      homepage.description: Real-Debrid WebDAV proxy for seamless streaming
    cpu_shares: 10
    mem_reservation: 32M
    deploy:
      resources:
        reservations:
          cpus: 0
          memory: 32M
        limits:
          cpus: 1
          memory: 2G
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:9999/http/version.txt || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    restart: always

networks:
  publicnet:  # https://docs.docker.com/engine/network/drivers/bridge/#differences-between-user-defined-bridges-and-the-default-bridge
    external: true  # docker network create --driver=bridge --attachable publicnet --subnet=${PUBLICNET_SUBNET:-10.76.0.0/16} --gateway=${PUBLICNET_GATEWAY:-10.76.0.1} --ip-range=${PUBLICNET_IP_RANGE:-10.76.0.0/16} -o com.docker.network.bridge.name=br_publicnet
    attachable: true
    name: publicnet
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: "br_publicnet"
    ipam:
      config:
        - subnet: ${PUBLICNET_SUBNET:-10.76.0.0/16}
          gateway: ${PUBLICNET_GATEWAY:-10.76.0.1}
          ip_range: ${PUBLICNET_IP_RANGE:-10.76.0.0/16}
  warp-network:
    external: true
    name: warp-network
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: "br_warp-network"
    ipam:
      config:
        - subnet: ${WARP_NETWORK_SUBNET:-10.45.0.0/16}
          gateway: ${WARP_NETWORK_GATEWAY:-10.45.0.1}
          ip_range: ${WARP_NETWORK_IP_RANGE:-10.45.0.0/16}

volumes:
  meili_data:
  qdrant_storage:
  container_configs:
  container_backups:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: ${BACKUP_PATH:-./backup}
  tmp:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size=100g
  shared:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size=100m
  gluetun_scripts:
    driver: local
    driver_opts:
      type: tmpfs
      device: tmpfs
      o: size=10m
