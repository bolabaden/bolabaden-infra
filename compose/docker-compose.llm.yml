# yaml-language-server: $schema=https://raw.githubusercontent.com/compose-spec/compose-spec/master/schema/compose-spec.json



secrets:
  # AI/LLM API Keys
  anthropic-api-key:
    file: ${SECRETS_PATH:?}/anthropic-api-key.txt
  openai-api-key:
    file: ${SECRETS_PATH:?}/openai-api-key.txt
  openrouter-api-key:
    file: ${SECRETS_PATH:?}/openrouter-api-key.txt
  groq-api-key:
    file: ${SECRETS_PATH:?}/groq-api-key.txt
  deepseek-api-key:
    file: ${SECRETS_PATH:?}/deepseek-api-key.txt
  gemini-api-key:
    file: ${SECRETS_PATH:?}/gemini-api-key.txt
  mistral-api-key:
    file: ${SECRETS_PATH:?}/mistral-api-key.txt
  perplexity-api-key:
    file: ${SECRETS_PATH:?}/perplexity-api-key.txt
  replicate-api-key:
    file: ${SECRETS_PATH:?}/replicate-api-key.txt
  sambanova-api-key:
    file: ${SECRETS_PATH:?}/sambanova-api-key.txt
  togetherai-api-key:
    file: ${SECRETS_PATH:?}/togetherai-api-key.txt
  hf-token:
    file: ${SECRETS_PATH:?}/hf-token.txt
  tavily-api-key:
    file: ${SECRETS_PATH:?}/tavily-api-key.txt
  firecrawl-api-key:
    file: ${SECRETS_PATH:?}/firecrawl-api-key.txt
  github-token:
    file: ${SECRETS_PATH:?}/github-token.txt
  coolify-token:
    file: ${SECRETS_PATH:?}/coolify-token.txt
  goal-story-api-key:
    file: ${SECRETS_PATH:?}/goal-story-api-key.txt
  smithery-api-key:
    file: ${SECRETS_PATH:?}/smithery-api-key.txt
  # Application Secrets
  # NOTE: mcpo-api-key not used - MCPO CLI doesn't support --api-key-file, requires env var
  open-webui-secret-key:
    file: ${SECRETS_PATH:?}/open-webui-secret-key.txt
  litellm-master-key:
    file: ${SECRETS_PATH:?}/litellm-master-key.txt
  # Other optional API keys
  brave-api-key:
    file: ${SECRETS_PATH:?}/brave-api-key.txt
  exa-api-key:
    file: ${SECRETS_PATH:?}/exa-api-key.txt
  glama-api-key:
    file: ${SECRETS_PATH:?}/glama-api-key.txt
  jina-api-key:
    file: ${SECRETS_PATH:?}/jina-api-key.txt
  kagi-api-key:
    file: ${SECRETS_PATH:?}/kagi-api-key.txt
  langchain-api-key:
    file: ${SECRETS_PATH:?}/langchain-api-key.txt
  serpapi-api-key:
    file: ${SECRETS_PATH:?}/serpapi-api-key.txt
  search1api-key:
    file: ${SECRETS_PATH:?}/search1api-key.txt
  upstage-api-key:
    file: ${SECRETS_PATH:?}/upstage-api-key.txt
  revid-api-key:
    file: ${SECRETS_PATH:?}/revid-api-key.txt


configs:
  mcpProxy:
    content: |
      {
        "mcpProxy": {
          "baseURL": "https://mcp.$TS_HOSTNAME.$DOMAIN",
          "addr": ":${MCP_PROXY_PORT:-9090}",
          "name": "MCP Proxy",
          "version": "1.0.0",
          "type": "streamable-http",
          "options": {
            "panicIfInvalid": false,
            "logEnabled": true,
            "authTokens": [
              "${MCP_AUTH_TOKEN:?}"
            ]
          }
        },
        "mcpServers": {
          "github": {
            "command": "npx",
            "args": [
              "-y",
              "@modelcontextprotocol/server-github"
            ],
            "env": {
              "GITHUB_PERSONAL_ACCESS_TOKEN": "$GITHUB_PERSONAL_ACCESS_TOKEN"
            },
            "options": {
              "toolFilter": {
                "mode": "block",
                "list": [
                  "create_or_update_file"
                ]
              }
            }
          },
          "fetch": {
            "command": "uvx",
            "args": [
              "mcp-server-fetch"
            ],
            "options": {
              "panicIfInvalid": true,
              "logEnabled": false,
              "authTokens": [
                "SpecificTokens"
              ]
            }
          },
          "coolify": {
            "command": "npx",
            "args": [
              "-y",
              "@masonator/coolify-mcp"
            ],
            "env": {
              "COOLIFY_ACCESS_TOKEN": "$COOLIFY_TOKEN",
              "COOLIFY_BASE_URL": "https://coolify.$DOMAIN"
            }
          },
          "coolify2": {
            "command": "npx",
            "args": [
              "-y",
              "coolify-mcp-server"
            ],
            "env": {
              "COOLIFY_BASE_URL": "https://coolify.$DOMAIN",
              "COOLIFY_TOKEN": "$COOLIFY_TOKEN"
            }
          },
          "curl": {
            "command": "npx",
            "args": [
              "-y",
              "@mcp-get-community/server-curl"
            ]
          },
          "firecrawl": {
            "command": "npx",
            "args": [
              "-y",
              "firecrawl-mcp"
            ],
            "env": {
              "FIRECRAWL_API_KEY": "$FIRECRAWL_API_KEY",
              "FIRE_CRAWL_API_KEY": "$FIRECRAWL_API_KEY",
              "FIRECRAWL_API_URL": "https://firecrawl-api.$DOMAIN",
              "FIRE_CRAWL_API_URL": "https://firecrawl-api.$DOMAIN"
            }
          },
          "firecrawl2": {
            "command": "npx",
            "args": [
              "-y",
              "mcp-server-firecrawl"
            ],
            "env": {
              "FIRECRAWL_API_KEY": "$FIRECRAWL_API_KEY",
              "FIRE_CRAWL_API_KEY": "$FIRECRAWL_API_KEY",
              "FIRECRAWL_API_URL": "https://firecrawl-api.$DOMAIN",
              "FIRE_CRAWL_API_URL": "https://firecrawl-api.$DOMAIN"
            }
          },
          "goalStory": {
            "command": "npx",
            "args": [
              "-y",
              "goalstory-mcp",
              "https://prod-goalstory-rqc2.encr.app",
              "$GOAL_STORY_API_KEY"
            ],
            "transportType": "stdio"
          },
          "tavily": {
            "command": "npx",
            "args": [
              "-y",
              "tavily-mcp"
            ],
            "env": {
              "TAVILY_API_KEY": "$TAVILY_API_KEY"
            }
          },
          "unified-diff-mcp": {
            "command": "npx",
            "args": [
              "-y",
              "@smithery/cli",
              "run",
              "@gorosun/unified-diff-mcp",
              "--key",
              "$SMITHERY_API_KEY"
            ]
          }
        }
      }
  mcp_servers.json:
    content: |
      {
        "mcpServers": {
          "curl": {
            "command": "npx",
            "args": [
              "-y",
              "@mcp-get-community/server-curl"
            ]
          },
          "firecrawl": {
            "command": "npx",
            "args": [
              "-y",
              "firecrawl-mcp"
            ],
            "env": {
              "FIRECRAWL_API_KEY": "fc-e0e316bead3d497fb24d42ea21d5daf6",
              "FIRE_CRAWL_API_KEY": "fc-e0e316bead3d497fb24d42ea21d5daf6",
              "FIRECRAWL_API_URL": "https://firecrawl-api.bolabaden.org",
              "FIRE_CRAWL_API_URL": "https://firecrawl-api.bolabaden.org"
            }
          },
          "goalStory": {
            "command": "npx",
            "args": [
              "-y",
              "goalstory-mcp",
              "https://prod-goalstory-rqc2.encr.app",
              "c5e7d1ea44eab701bae258695253914c2579d19c637694e03136e3867e081195"
            ],
            "transportType": "stdio"
          },
          "tavily": {
            "command": "npx",
            "args": [
              "-y",
              "tavily-mcp"
            ],
            "env": {
              "TAVILY_API_KEY": "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"
            }
          }
        }
      }
  litellm_config.yaml:
    file: ${CONFIG_PATH:-./volumes}/litellm/litellm_config.yaml



services:
  open-webui:
    # ðŸ”¹ðŸ”¹ Open WebUI ðŸ”¹ðŸ”¹
    depends_on:
      mcpo:
        condition: service_healthy
      litellm:
        condition: service_healthy
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui
    hostname: open-webui
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - backend
      - publicnet
    secrets:
      - open-webui-secret-key
    expose:
      - ${OPEN_WEBUI_PORT:-8080}
    volumes:
      - ${CONFIG_PATH:-./volumes}/open-webui/uploads:/app/backend/data/uploads
      - ${CONFIG_PATH:-./volumes}/open-webui/vector_db:/app/backend/data/vector_db:rw
      - ${CONFIG_PATH:-./volumes}/open-webui/webui.db:/app/backend/data/webui.db:rw
    environment:
      # ===== GENERAL CONFIGURATION =====
      # Controls whether admin users can export data
      ENABLE_ADMIN_EXPORT: True

      # Enables admin users to access all chats
      ENABLE_ADMIN_CHAT_ACCESS: True

      # Bypasses model access control. (default: False)
      BYPASS_MODEL_ACCESS_CONTROL: True

      # Environment setting (dev/prod) - Docker default is prod
      ENV: prod

      # If set to False, all PersistentConfig variables are treated as regular variables
      ENABLE_PERSISTENT_CONFIG: True

      # Sets the port to run Open WebUI from
      PORT: ${OPEN_WEBUI_PORT:-8080}

      # When enabled, saves each chunk of streamed chat data to database in real time
      ENABLE_REALTIME_CHAT_SAVE: True

      # Used for identifying the Git SHA of the build for releases
      WEBUI_BUILD_HASH: dev-build

      # ===== AIOHTTP CLIENT CONFIGURATION =====
      # Specifies the timeout duration in seconds for the AIOHTTP client
      AIOHTTP_CLIENT_TIMEOUT: 300

      # Sets the timeout in seconds for fetching the model list
      AIOHTTP_CLIENT_TIMEOUT_MODEL_LIST: 10

      # Sets the timeout in seconds for fetching the OpenAI model list
      AIOHTTP_CLIENT_TIMEOUT_OPENAI_MODEL_LIST: 10

      # ===== DIRECTORY CONFIGURATION =====
      # Specifies the base directory for data storage
      DATA_DIR: ./data

      # Specifies the location of the built frontend files
      FRONTEND_BUILD_DIR: ../build

      # Specifies the directory for static files
      STATIC_DIR: ./static

      # ===== OLLAMA CONFIGURATION =====
      # Configures the Ollama backend URL
      OLLAMA_BASE_URL: /ollama

      # Builds the Docker image with a bundled Ollama instance
      USE_OLLAMA_DOCKER: false

      # If set, assumes Helm chart deployment
      K8S_FLAG: False

      # ===== SECURITY CONFIGURATION =====
      # Forwards user information as X-headers to OpenAI API and Ollama API
      ENABLE_FORWARD_USER_INFO_HEADERS: False

      # Sets the SameSite attribute for session cookies (lax/strict/none)
      WEBUI_SESSION_COOKIE_SAME_SITE: lax

      # Sets the Secure attribute for session cookies
      WEBUI_SESSION_COOKIE_SECURE: False

      # Sets the SameSite attribute for auth cookies
      WEBUI_AUTH_COOKIE_SAME_SITE: lax

      # Sets the Secure attribute for auth cookies
      WEBUI_AUTH_COOKIE_SECURE: False

      # Enables or disables authentication
      WEBUI_AUTH: ${WEBUI_AUTH:-True}

      # Overrides the randomly generated string used for JSON Web Token
      WEBUI_SECRET_KEY_FILE: /run/secrets/open-webui-secret-key

      # When enabled, makes automatic update checks and notifies about version updates
      ENABLE_VERSION_UPDATE_CHECK: True

      # Disables Open WebUI's network connections for update checks and automatic model downloads
      OFFLINE_MODE: False

      # Resets the config.json file on startup
      RESET_CONFIG_ON_START: False

      # Enables safe mode, which disables potentially unsafe features
      SAFE_MODE: False

      # Sets the allowed origins for Cross-Origin Resource Sharing (CORS)
      CORS_ALLOW_ORIGIN: ${OPEN_WEBUI_CORS_ALLOWED_ORIGIN:-*}

      # Determines whether to allow custom models defined on the Hub
      RAG_EMBEDDING_MODEL_TRUST_REMOTE_CODE: True

      # Determines whether to allow custom models for reranking
      RAG_RERANKING_MODEL_TRUST_REMOTE_CODE: True

      # Toggles automatic update of the Sentence-Transformer model
      RAG_EMBEDDING_MODEL_AUTO_UPDATE: True

      # Toggles automatic update of the reranking model
      RAG_RERANKING_MODEL_AUTO_UPDATE: True

      # ===== VECTOR DATABASE CONFIGURATION =====
      # Specifies which vector database system to use
      VECTOR_DB: chroma

      # ===== RAG CONFIGURATION =====
      # Sets the directory for TikToken cache
      TIKTOKEN_CACHE_DIR: /app/backend/data/cache/tiktoken

      # Sets the batch size for OpenAI embeddings
      RAG_EMBEDDING_OPENAI_BATCH_SIZE: 1

      # ===== DOCKER-SPECIFIC CONFIGURATION =====
      # Indicates running in Docker environment
      DOCKER: true

      # Home directory for the container
      HOME: /root

      # Hugging Face home directory for embedding models
      HF_HOME: /app/backend/data/cache/embedding/models

      # Sentence Transformers home directory
      SENTENCE_TRANSFORMERS_HOME: /app/backend/data/cache/embedding/models

      # CUDA Docker version to use
      USE_CUDA_DOCKER_VER: cu128

      # Embedding model to use in Docker
      USE_EMBEDDING_MODEL_DOCKER: sentence-transformers/all-MiniLM-L6-v2

      # ===== TELEMETRY DISABLING =====
      # Disables anonymized telemetry
      ANONYMIZED_TELEMETRY: false

      # Disables tracking
      DO_NOT_TRACK: true

      # Disables Scarf analytics
      SCARF_NO_ANALYTICS: true
    labels:
      traefik.enable: true
      traefik.http.routers.open-webui.rule: Host(`open-webui.$DOMAIN`) || Host(`open-webui.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.open-webui.loadbalancer.server.port: ${OPEN_WEBUI_PORT:-8080}
      homepage.group: AI
      homepage.name: Open WebUI
      homepage.icon: open-webui.png
      homepage.href: https://open-webui.$DOMAIN/
      homepage.description: Web interface for chatting with local/remote LLMs, tools, and knowledge bases
      kuma.open-webui.http.name: open-webui.$TS_HOSTNAME.$DOMAIN
      kuma.open-webui.http.url: https://open-webui.$DOMAIN
      kuma.open-webui.http.interval: 20
    command: ["bash", "start.sh"]
    working_dir: /app/backend
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:8080"]
      interval: 5s
      timeout: 30s
      retries: 10
    restart: always
  mcpo:
    image: ghcr.io/open-webui/mcpo:main
    container_name: mcpo
    hostname: mcpo
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - backend
      - publicnet
    expose:
      - ${MCPO_PORT:-8000}
    configs:
      - source: mcp_servers.json
        target: /app/config/mcp_servers.json
        mode: 0444
#    volumes:
#      - ${CONFIG_PATH:-./volumes}/mcp/mcpo/logs:/app/logs
#      - ${CONFIG_PATH:-./volumes}/mcp/mcpo/data:/app/data
    environment:
      MCPO_API_KEY: ${MCPO_API_KEY:?}
    labels:
      traefik.enable: true
      traefik.http.routers.mcpo.middlewares: nginx-auth@file
      traefik.http.routers.mcpo.rule: Host(`mcpo.$DOMAIN`) || Host(`mcpo.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.mcpo.loadbalancer.server.port: ${MCPO_PORT:-8000}
      homepage.group: MCPO
      homepage.name: MCPO
      homepage.icon: mcpo.png
      homepage.href: https://mcpo.$DOMAIN/
      homepage.description: MCP Orchestrator exposing model/context tools over the MCP protocol
      kuma.mcpo.http.name: mcpo.$TS_HOSTNAME.$DOMAIN
      kuma.mcpo.http.url: https://mcpo.$DOMAIN
      kuma.mcpo.http.interval: 30
    command: --api-key "${MCPO_API_KEY:?}" --host 0.0.0.0 --port ${MCPO_PORT:-8000} --cors-allow-origins "*" --config /app/config/mcp_servers.json
    working_dir: /app
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://127.0.0.1:${MCPO_PORT:-8000}/openapi.json"]
      timeout: 10s
      start_period: 40s
    restart: always

  # Model Updater Service - Runs on-demand to update free_chat_models.json<-->litellm_config.yaml
  model-updater:
    profiles:
      - unfinished
    build:
      context: https://github.com/th3w1zard1/llm_fallbacks.git
      dockerfile_inline: |
        FROM python:3.13-slim
        WORKDIR /app
        RUN apt-get update && apt-get install -y curl build-essential && rm -rf /var/lib/apt/lists/*
        COPY . .
        RUN pip install --no-cache-dir -r requirements.txt
        RUN mkdir -p /app/cache
        ENV PYTHONPATH=/app
        ENV PYTHONUNBUFFERED=1
        RUN useradd --create-home --shell /bin/bash app && chown -R app:app /app
        USER app
        CMD ["python", "src/llm_fallbacks/generate_configs.py"]
    container_name: model-updater
    hostname: model-updater
    networks:
      - backend
      - publicnet
    volumes:
      - ${CONFIG_PATH:-./volumes}/litellm:/app/config:rw
      - model-updater-cache:/app/cache
    environment:
      OPENROUTER_API_KEY: $OPENROUTER_API_KEY
      POSTGRES_PASSWORD: ${LITELLM_POSTGRES_PASSWORD:-postgres}
      TZ: ${TZ:-America/Chicago}
    restart: no

  litellm:
    image: ghcr.io/berriai/litellm-database:main-stable
#    image: ghcr.io/berriai/litellm:main-stable
    container_name: litellm
    hostname: litellm
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - backend
      - publicnet
    secrets:
      - litellm-master-key
      - anthropic-api-key
      - openai-api-key
      - openrouter-api-key
      - groq-api-key
      - deepseek-api-key
      - gemini-api-key
      - mistral-api-key
      - perplexity-api-key
      - replicate-api-key
      - sambanova-api-key
      - togetherai-api-key
      - hf-token
      - langchain-api-key
      - serpapi-api-key
      - search1api-key
      - upstage-api-key
      - jina-api-key
      - kagi-api-key
      - glama-api-key
    expose:
      - ${LITELLM_PORT:-4000}
    depends_on:
      litellm-postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    volumes:
      - ${CONFIG_PATH:-./volumes}/litellm:/app/config:ro
    configs:
      - source: litellm_config.yaml
        target: /app/config/litellm_config.yaml
        mode: 0440
    environment:
      LITELLM_LOG: ${LITELLM_LOG:-INFO}
      LITELLM_MASTER_KEY_FILE: /run/secrets/litellm-master-key
      LITELLM_MODE: ${LITELLM_MODE:-PRODUCTION}
      UI_USERNAME: ${LITELLM_UI_USERNAME:-admin}
      UI_PASSWORD_FILE: /run/secrets/litellm-master-key
      DATABASE_URL: postgresql://${LITELLM_POSTGRES_USER:-litellm}:${LITELLM_POSTGRES_PASSWORD:-litellm}@${LITELLM_POSTGRES_HOSTNAME:-litellm-postgres}:5432/${LITELLM_POSTGRES_DB:-litellm}
      REDIS_HOST: ${REDIS_HOSTNAME:-redis}
      REDIS_PORT: ${REDIS_PORT:-6379}
      POSTGRES_USER: ${LITELLM_POSTGRES_USER:-litellm}
      POSTGRES_PASSWORD: ${LITELLM_POSTGRES_PASSWORD:-litellm}
      POSTGRES_DB: ${LITELLM_POSTGRES_DB:-litellm}
      # API keys loaded from secrets
      ANTHROPIC_API_KEY_FILE: /run/secrets/anthropic-api-key
      OPENAI_API_KEY_FILE: /run/secrets/openai-api-key
      OPENROUTER_API_KEY_FILE: /run/secrets/openrouter-api-key
      GROQ_API_KEY_FILE: /run/secrets/groq-api-key
      DEEPSEEK_API_KEY_FILE: /run/secrets/deepseek-api-key
      GEMINI_API_KEY_FILE: /run/secrets/gemini-api-key
      MISTRAL_API_KEY_FILE: /run/secrets/mistral-api-key
      PERPLEXITY_API_KEY_FILE: /run/secrets/perplexity-api-key
      REPLICATE_API_KEY_FILE: /run/secrets/replicate-api-key
      SAMBANOVA_API_KEY_FILE: /run/secrets/sambanova-api-key
      TOGETHERAI_API_KEY_FILE: /run/secrets/togetherai-api-key
      HF_TOKEN_FILE: /run/secrets/hf-token
      LANGCHAIN_API_KEY_FILE: /run/secrets/langchain-api-key
      SERPAPI_API_KEY_FILE: /run/secrets/serpapi-api-key
      SEARCH1API_KEY_FILE: /run/secrets/search1api-key
      UPSTAGE_API_KEY_FILE: /run/secrets/upstage-api-key
      JINA_API_KEY_FILE: /run/secrets/jina-api-key
      KAGI_API_KEY_FILE: /run/secrets/kagi-api-key
      GLAMA_API_KEY_FILE: /run/secrets/glama-api-key
    labels:
      traefik.enable: true
      homepage.group: AI
      homepage.name: Litellm
      homepage.icon: litellm.png
      homepage.href: https://litellm.$DOMAIN/
      homepage.description: LLM gateway/router with provider failover, caching, rate limits, and analytics
      kuma.litellm.http.name: litellm.$TS_HOSTNAME.$DOMAIN
      kuma.litellm.http.url: https://litellm.$DOMAIN
      kuma.litellm.http.interval: 60
    command: --config /app/config/litellm_config.yaml --port ${LITELLM_PORT:-4000} --host 0.0.0.0
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:${LITELLM_PORT:-4000}/health/liveliness"]
      timeout: 15s
      retries: 5
      start_period: 30s
    restart: always

  litellm-postgres:
    image: docker.io/postgres:16.3-alpine3.20
    container_name: litellm-postgres
    hostname: ${LITELLM_POSTGRES_HOSTNAME:-litellm-postgres}
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - backend
      - publicnet
    environment:
      POSTGRES_DB: ${LITELLM_POSTGRES_DB:-litellm}
      POSTGRES_PASSWORD: ${LITELLM_POSTGRES_PASSWORD:-litellm}
      POSTGRES_USER: ${LITELLM_POSTGRES_USER:-litellm}
    volumes:
      - ${CONFIG_PATH:-./volumes}/litellm/pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -U $$POSTGRES_USER -d $$POSTGRES_DB"]
      interval: 5s
      timeout: 5s
      retries: 3
    restart: always

  gptr:
  #  build:
  #    context: https://github.com/bolabaden/ai-researchwizard.git#master  # :/frontend/nextjs
  #    dockerfile: Dockerfile.fullstack
    image: docker.io/bolabaden/ai-researchwizard-aio-fullstack:master
    container_name: gptr
    hostname: gptr
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - backend
      - publicnet
    secrets:
      - anthropic-api-key
      - brave-api-key
      - deepseek-api-key
      - exa-api-key
      - firecrawl-api-key
      - gemini-api-key
      - glama-api-key
      - groq-api-key
      - hf-token
      - langchain-api-key
      - mistral-api-key
      - openai-api-key
      - openrouter-api-key
      - perplexity-api-key
      - replicate-api-key
      - revid-api-key
      - sambanova-api-key
      - search1api-key
      - serpapi-api-key
      - tavily-api-key
      - togetherai-api-key
      - upstage-api-key
    expose:
      - 3000  # Nginx-proxied access to GPTR.
      - 3001  # Internal NextJS GPTR site
      - 8000  # Static GPTR
      - 8080  # MCP Server Port
    volumes:
      - ${CONFIG_PATH:-./volumes}/gptr/logs:/usr/src/app/logs
      - ${CONFIG_PATH:-./volumes}/gptr/outputs:/usr/src/app/outputs
      - ${CONFIG_PATH:-./volumes}/gptr/reports:/usr/src/app/reports
    stdin_open: true
    environment:
      ANTHROPIC_API_KEY_FILE: /run/secrets/anthropic-api-key
      BRAVE_API_KEY_FILE: /run/secrets/brave-api-key
      DEEPSEEK_API_KEY_FILE: /run/secrets/deepseek-api-key
      EXA_API_KEY_FILE: /run/secrets/exa-api-key
      FIRECRAWL_API_KEY_FILE: /run/secrets/firecrawl-api-key
      FIRE_CRAWL_API_KEY_FILE: /run/secrets/firecrawl-api-key
      GEMINI_API_KEY_FILE: /run/secrets/gemini-api-key
      GLAMA_API_KEY_FILE: /run/secrets/glama-api-key
      GROQ_API_KEY_FILE: /run/secrets/groq-api-key
      HF_TOKEN_FILE: /run/secrets/hf-token
      HUGGINGFACE_ACCESS_TOKEN_FILE: /run/secrets/hf-token
      HUGGINGFACE_API_TOKEN_FILE: /run/secrets/hf-token
      LANGCHAIN_API_KEY_FILE: /run/secrets/langchain-api-key
      MISTRAL_API_KEY_FILE: /run/secrets/mistral-api-key
      MISTRALAI_API_KEY_FILE: /run/secrets/mistral-api-key
      OPENAI_API_KEY_FILE: /run/secrets/openai-api-key
      OPENROUTER_API_KEY_FILE: /run/secrets/openrouter-api-key
      PERPLEXITY_API_KEY_FILE: /run/secrets/perplexity-api-key
      PERPLEXITYAI_API_KEY_FILE: /run/secrets/perplexity-api-key
      REPLICATE_API_KEY_FILE: /run/secrets/replicate-api-key
      REVID_API_KEY_FILE: /run/secrets/revid-api-key
      SAMBANOVA_API_KEY_FILE: /run/secrets/sambanova-api-key
      SEARCH1API_KEY_FILE: /run/secrets/search1api-key
      SERPAPI_API_KEY_FILE: /run/secrets/serpapi-api-key
      TAVILY_API_KEY_FILE: /run/secrets/tavily-api-key
      TOGETHERAI_API_KEY_FILE: /run/secrets/togetherai-api-key
      UPSTAGE_API_KEY_FILE: /run/secrets/upstage-api-key
      UPSTAGEAI_API_KEY_FILE: /run/secrets/upstage-api-key
      CHOKIDAR_USEPOLLING: ${CHOKIDAR_USEPOLLING:-true}
      LOGGING_LEVEL: ${GPTR_LOGGING_LEVEL:-DEBUG}
      NEXT_PUBLIC_GA_MEASUREMENT_ID: $NEXT_PUBLIC_GA_MEASUREMENT_ID
      NEXT_PUBLIC_GPTR_API_URL: https://gptr.$DOMAIN
      LANGSMITH_TRACING: ${LANGSMITH_TRACING:-true}
      LANGSMITH_ENDPOINT: ${LANGSMITH_ENDPOINT:-https://api.smith.langchain.com}
      LANGSMITH_API_KEY_FILE: /run/secrets/langchain-api-key
    labels:
      traefik.enable: true
      traefik.http.routers.gptr-nextjs.service: gptr-nextjs@docker
      traefik.http.routers.gptr-nextjs.rule: Host(`gptr-nextjs.$DOMAIN`) || Host(`gptr-nextjs.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.gptr-nextjs.loadbalancer.server.port: 3000
      traefik.http.routers.gptr-legacy.service: gptr-legacy@docker
      traefik.http.routers.gptr-legacy.rule: Host(`gptr.$DOMAIN`) || Host(`gptr.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.gptr-legacy.loadbalancer.server.port: 8000
      traefik.http.routers.gptr-mcp.service: gptr-mcp@docker
      traefik.http.routers.gptr-mcp.rule: Host(`gptr-mcp.$DOMAIN`) || Host(`gptr-mcp.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.gptr-mcp.loadbalancer.server.port: 8080
      homepage.group: AI
      homepage.name: AI Research Wizard
      homepage.description: Full-stack AI research and scraping toolkit with Next.js UI is a web scraper and researcher that uses AI to help you find information quickly.
      homepage.icon: gptr.png
      homepage.href: https://gptr.$DOMAIN/
      kuma.gptr.http.name: gptr.$TS_HOSTNAME.$DOMAIN
      kuma.gptr.http.url: https://gptr.$DOMAIN
      kuma.gptr.http.interval: 30
    healthcheck:  # docker exec gptr ls -la /bin /usr/bin | grep -E 'curl|wget|nc|telnet|http|python|ncat|nmap'
      test: ["CMD-SHELL", "(wget -qO- http://127.0.0.1:3000 > /dev/null 2>&1 && wget -qO- http://127.0.0.1:8000 > /dev/null 2>&1) || exit 1"]  # && (wget -qO- http://127.0.0.1:8080 > /dev/null 2>&1) || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 2m
    restart: always


  qdrant:
    # ðŸ”¹ðŸ”¹ Qdrant ðŸ”¹ðŸ”¹
    profiles:
      - extras
    image: qdrant/qdrant
    container_name: qdrant
    hostname: qdrant
    networks:
      - publicnet
    expose:
      - 6333
    volumes:
      - ${CONFIG_PATH:-./volumes}/qdrant/storage:/qdrant/storage
    environment:
      QDRANT_STORAGE_PATH: /qdrant/storage
      QDRANT_STORAGE_TYPE: disk
      QDRANT_STORAGE_DISK_PATH: /qdrant/storage
      QDRANT_STORAGE_DISK_TYPE: disk
    labels:
      traefik.enable: true
      traefik.http.services.qdrant.loadbalancer.server.port: 6333
      homepage.group: AI
      homepage.name: Qdrant
      homepage.icon: qdrant.png
      homepage.href: https://qdrant.$DOMAIN
      homepage.description: Qdrant is a vector database for storing and querying vectors.
    restart: always
  mcp-proxy:
    profiles:
      - extras  # can be resource intensive, also is duplicating what mcpo is already providing.
    pull_policy: always
    image: ghcr.io/tbxark/mcp-proxy
    container_name: mcp-proxy
    hostname: mcp
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - publicnet
    expose:
      - ${MCP_PROXY_PORT:-9090}
    configs:
      - source: mcpProxy
        target: /config.json
        mode: 0777
    labels:
      traefik.enable: true
      traefik.http.routers.mcp-proxy.rule: Host(`mcp.$DOMAIN`) || Host(`mcp.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.mcp-proxy.loadbalancer.server.port: ${MCP_PROXY_PORT:-9090}
      homepage.group: MCP
      homepage.name: MCP Proxy
      homepage.icon: mcp-proxy.png
      homepage.href: https://mcp.$DOMAIN
      homepage.description: MCP Proxy is a tool for proxying MCP servers.
    command: [  # https://github.com/TBXark/mcp-proxy/blob/master/docs/USAGE.md
      "--config", "config.json",          # Path to config file or a http(s) url (default "config.json")
      "-expand-env",                      # Expand environment variables in config file (default true)
    #  "-http-headers" "string_here"      # Custom HTTP headers to be sent to the MCP server (default none)
    #  "-http-timeout", "10",              # Timeout (seconds) for remote config fetch (default 10)
    #  "-insecure",                        # Skip TLS verification for remote config fetch (default false)
    ]
    restart: always


volumes:
  model-updater-cache:
    driver: local
  scheduler-logs:
    driver: local
