kind: Deploy
apiVersion: garden.io/v0
name: open-webui
description: Open WebUI for LLM chat interface
type: container
profiles:
  - fixme
dependencies:
  - deploy.mcpo
  - deploy.litellm

spec:
  replicas: 3
  image: ghcr.io/open-webui/open-webui:main
  hostname: open-webui
  extraHosts:
    - host.docker.internal:host-gateway
  env:
    ENABLE_ADMIN_EXPORT: True
    ENABLE_ADMIN_CHAT_ACCESS: True
    BYPASS_MODEL_ACCESS_CONTROL: True
    ENV: prod
    ENABLE_PERSISTENT_CONFIG: True
    PORT: ${var.OPEN_WEBUI_PORT || "8080"}
    ENABLE_REALTIME_CHAT_SAVE: True
    WEBUI_BUILD_HASH: dev-build
    AIOHTTP_CLIENT_TIMEOUT: 300
    AIOHTTP_CLIENT_TIMEOUT_MODEL_LIST: 10
    AIOHTTP_CLIENT_TIMEOUT_OPENAI_MODEL_LIST: 10
    DATA_DIR: ./data
    FRONTEND_BUILD_DIR: ../build
    STATIC_DIR: ./static
    OLLAMA_BASE_URL: /ollama
    USE_OLLAMA_DOCKER: false
    K8S_FLAG: False
    ENABLE_FORWARD_USER_INFO_HEADERS: False
    WEBUI_SESSION_COOKIE_SAME_SITE: lax
    WEBUI_SESSION_COOKIE_SECURE: False
    WEBUI_AUTH_COOKIE_SAME_SITE: lax
    WEBUI_AUTH_COOKIE_SECURE: False
    WEBUI_AUTH: ${var.WEBUI_AUTH || "True"}
    WEBUI_SECRET_KEY_FILE: /run/secrets/open-webui-secret-key
    ENABLE_VERSION_UPDATE_CHECK: True
    OFFLINE_MODE: False
    RESET_CONFIG_ON_START: False
    SAFE_MODE: False
    CORS_ALLOW_ORIGIN: ${var.OPEN_WEBUI_CORS_ALLOWED_ORIGIN || "*"}
    RAG_EMBEDDING_MODEL_TRUST_REMOTE_CODE: True
    RAG_RERANKING_MODEL_TRUST_REMOTE_CODE: True
    RAG_EMBEDDING_MODEL_AUTO_UPDATE: True
    RAG_RERANKING_MODEL_AUTO_UPDATE: True
    VECTOR_DB: chroma
    TIKTOKEN_CACHE_DIR: /app/backend/data/cache/tiktoken
    RAG_EMBEDDING_OPENAI_BATCH_SIZE: 1
    DOCKER: true
    HOME: /root
    HF_HOME: /app/backend/data/cache/embedding/models
    SENTENCE_TRANSFORMERS_HOME: /app/backend/data/cache/embedding/models
    USE_CUDA_DOCKER_VER: cu128
    USE_EMBEDDING_MODEL_DOCKER: sentence-transformers/all-MiniLM-L6-v2
    ANONYMIZED_TELEMETRY: false
    DO_NOT_TRACK: true
    SCARF_NO_ANALYTICS: true
  ports:
    - name: http
      containerPort: ${var.OPEN_WEBUI_PORT || "8080"}
  volumes:
    - name: uploads
      containerPath: /app/backend/data/uploads
      hostPath: ${var.config-path}/open-webui/uploads
    - name: vector-db
      containerPath: /app/backend/data/vector_db
      hostPath: ${var.config-path}/open-webui/vector_db
    - name: webui-db
      containerPath: /app/backend/data/webui.db
      hostPath: ${var.config-path}/open-webui/webui.db
    - name: open-webui-secret-key
      containerPath: /run/secrets/open-webui-secret-key
      hostPath: ${var.config-path}/secrets/open-webui-secret-key.txt
      readOnly: true
  command: [bash, start.sh]
  workingDir: /app/backend
  ingresses:
    - path: /
      port: http
      hostname: open-webui.${var.domain}
    - path: /
      port: http
      hostname: open-webui.${var.ts-hostname}.${var.domain}
  annotations:
    traefik.enable: "true"
    traefik.http.routers.open-webui.rule: Host(`open-webui.${var.domain}`) || Host(`open-webui.${var.ts-hostname}.${var.domain}`)
    traefik.http.services.open-webui.loadbalancer.server.port: ${var.OPEN_WEBUI_PORT || "8080"}
    homepage.group: AI
    homepage.name: Open WebUI
    homepage.icon: open-webui.png
    homepage.href: https://open-webui.${var.domain}/
    homepage.description: Web interface for chatting with local/remote LLMs, tools, and knowledge bases
    kuma.open-webui.http.name: open-webui.${var.ts-hostname}.${var.domain}
    kuma.open-webui.http.url: https://open-webui.${var.domain}
    kuma.open-webui.http.interval: "20"
  healthCheck:
    exec:
      command: [sh, -c, "curl -f http://127.0.0.1:${var.OPEN_WEBUI_PORT || \"8080\"}"]
    initialDelaySeconds: 5
    periodSeconds: 5
    timeoutSeconds: 30
    failureThreshold: 10
  restartPolicy: always
