# yaml-language-server: $schema=https://raw.githubusercontent.com/compose-spec/compose-spec/master/schema/compose-spec.json
# TODO: com.docker.network.bridge.inhibit_ipv4=true

secrets:
  warp-license-key:
    file: ${SECRETS_PATH:?}/warp-license-key.txt

volumes:
  warp-config-data:

#  Defined in main `docker-compose.yml` and used by `include:` feature of compose-spec
# networks:
#  warp-nat-net:
#    name: ${DOCKER_NETWORK_NAME:-warp-nat-net}
#    driver: bridge
#    driver_opts:
#      com.docker.network.bridge.name: br_${DOCKER_NETWORK_NAME:-warp-nat-net}
#      com.docker.network.bridge.enable_ip_masquerade: "false"
#    ipam:
#      config:
#        - subnet: ${WARP_NAT_NET_SUBNET:-10.0.2.0/24}
#          gateway: ${WARP_NAT_NET_GATEWAY:-10.0.2.1}

# reminder: everything in docker-compose.yml's `configs:` section must have dollar signs ($) escaped with another dollar sign ($$).
# Single dollar sign specifies that we're expecting docker to resolve the variable itself at deploy time (e.g. when `docker compose up` is ran).
# Double dollar sign specifies a literal dollar sign in the configuration content.
configs:
  warp-nat-setup.sh:
    content: |
      #!/bin/bash
      set -xe

      # Defaults (configurable via env)
      DOCKER_HOST="$${DOCKER_HOST:-unix:///var/run/docker.sock}"
      ROUTER_CONTAINER_NAME="$${ROUTER_CONTAINER_NAME:-warp_router}"
      DOCKER_NETWORK_NAME="$${DOCKER_NETWORK_NAME:-warp-nat-net}"
      WARP_CONTAINER_NAME="$${WARP_CONTAINER_NAME:-warp-nat-gateway}"
      HOST_VETH_IP="$${HOST_VETH_IP:-169.254.100.1}"
      CONT_VETH_IP="$${CONT_VETH_IP:-169.254.100.2}"
      ROUTING_TABLE="$${ROUTING_TABLE:-warp-nat-routing}"
      VETH_HOST="$${VETH_HOST:-veth-warp}" 

      # VETH_CONT is derived from VETH_HOST
      VETH_CONT="$${VETH_HOST#veth-}-nat-cont"
      DOCKER="docker -H $$DOCKER_HOST"
      DEFAULT_DOCKER_NETWORK_NAME="warp-nat-net"

      echo "=========================================="
      echo "Starting WARP NAT setup script"
      echo "=========================================="

      # ==========================================
      # PHASE 1: COMPLETE CLEANUP OF OLD STATE
      # ==========================================
      echo ""
      echo "Phase 1: Cleaning up any existing configuration..."

      # Remove old veth interfaces
      if ip link show "$$VETH_HOST" >/dev/null 2>&1; then
          echo "Removing old veth interface: $$VETH_HOST"
          ip link del "$$VETH_HOST" 2>/dev/null || true
      fi

      # Get the subnet to clean up rules (try to get from network if it exists)
      CLEANUP_SUBNET="$${WARP_NAT_NET_SUBNET:-10.0.2.0/24}"
      if $$DOCKER network inspect "$$DOCKER_NETWORK_NAME" >/dev/null 2>&1; then
          EXISTING_SUBNET=$$($$DOCKER network inspect -f '{{(index .IPAM.Config 0).Subnet}}' "$$DOCKER_NETWORK_NAME" 2>/dev/null | tr -d '[:space:]')
          if [[ -n "$$EXISTING_SUBNET" ]]; then
              CLEANUP_SUBNET="$$EXISTING_SUBNET"
          fi
      fi

      # Remove old routing rules for this subnet
      echo "Removing old routing rules for $$CLEANUP_SUBNET"
      while ip rule del from "$$CLEANUP_SUBNET" table "$$ROUTING_TABLE" 2>/dev/null; do
          echo "  Removed routing rule"
      done

      # Remove old iptables failsafe rules
      echo "Removing old failsafe iptables rules"
      while iptables -D FORWARD -s "$$CLEANUP_SUBNET" -j DROP -m comment --comment "warp-nat-failsafe" 2>/dev/null; do
          echo "  Removed failsafe rule"
      done

      # Remove old NAT rules on host
      echo "Removing old NAT rules on host"
      iptables -t nat -D POSTROUTING -s "$$CLEANUP_SUBNET" ! -d "$$CLEANUP_SUBNET" -j MASQUERADE 2>/dev/null || true

      echo "Phase 1 cleanup complete"

      # Pick a free routing table id dynamically (start at 110)
      pick_table_id() {
          local id=110
          while grep -q "^$$id " /etc/iproute2/rt_tables 2>/dev/null; do
              id=$$((id+1))
          done
          echo $$id
      }

      # ==========================================
      # PHASE 2: SETUP ROUTING TABLE
      # ==========================================
      echo ""
      echo "Phase 2: Setting up routing table..."

      # Get existing routing table ID if name exists, else pick new and add
      if grep -q " $$ROUTING_TABLE$$" /etc/iproute2/rt_tables 2>/dev/null; then
          ROUTING_TABLE_ID=$$(awk "/ $$ROUTING_TABLE\$$/ {print \$$1}" /etc/iproute2/rt_tables)
          echo "Routing table exists: $$ROUTING_TABLE (ID: $$ROUTING_TABLE_ID)"
      else
          ROUTING_TABLE_ID=$$(pick_table_id)
          echo "$$ROUTING_TABLE_ID $$ROUTING_TABLE" >> /etc/iproute2/rt_tables
          echo "Created routing table: $$ROUTING_TABLE (ID: $$ROUTING_TABLE_ID)"
      fi

      # ==========================================
      # PHASE 3: ENSURE NETWORK EXISTS AND DISCOVER WARP CONTAINER
      # ==========================================
      # This phase discovers the actual network name using Docker Compose naming conventions.
      # Unlike the old approach that required the router container to exist and inspected its
      # connected networks, this method:
      # - Works independently of container state (more robust during setup/teardown)
      # - Understands Compose prefixes network names with stack name (e.g., "stack_network")
      # - Supports multiple WARP instances by using stack-specific naming
      # - Is deterministic and idempotent (doesn't depend on container connection state)
      echo ""
      echo "Phase 3: Ensuring network exists and discovering WARP container..."
      
      # Get stack name from compose labels to construct proper network name
      # Tries router container first, falls back to warp container, then empty string
      STACK_NAME="$$(
          $$DOCKER inspect -f '{{ index .Config.Labels "com.docker.compose.project" }}' "$$ROUTER_CONTAINER_NAME" 2>/dev/null \
          || $$DOCKER inspect -f '{{ index .Config.Labels "com.docker.compose.project" }}' "$$WARP_CONTAINER_NAME" 2>/dev/null \
          || echo ""
      )"
      
      # NETWORK DISCOVERY (Non-Destructive Approach)
      # ================================================
      # OLD BEHAVIOR (removed): Would forcefully disconnect ALL containers, delete network, recreate it,
      #                         then reconnect containers - causing service disruption every time script ran.
      # NEW BEHAVIOR: Simply discovers and uses existing network without any modifications.
      # BENEFITS:
      #   - Zero downtime: containers stay connected, no service interruption
      #   - True idempotency: can run multiple times safely without state changes
      #   - Faster: no time wasted on disconnect/reconnect operations
      #   - Simpler: no complex gw_priority preservation logic needed
      #   - Multi-instance safe: different stacks can manage independent networks
      # NETWORK LIFECYCLE: Managed by warp-net-init service, not this routing script.
      
      # Check if network exists using Compose naming pattern (stack_network) first
      # This allows multiple stacks to have their own warp-nat-net without conflicts
      ACTUAL_NETWORK_NAME=""
      if [[ -n "$$STACK_NAME" ]]; then
          if $$DOCKER network inspect "$${STACK_NAME}_$$DOCKER_NETWORK_NAME" >/dev/null 2>&1; then
              ACTUAL_NETWORK_NAME="$${STACK_NAME}_$$DOCKER_NETWORK_NAME"
          fi
      fi

      # Fallback: try plain network name (for external networks or single-stack deployments)
      if [[ -z "$$ACTUAL_NETWORK_NAME" ]]; then
          if $$DOCKER network inspect "$$DOCKER_NETWORK_NAME" >/dev/null 2>&1; then
              ACTUAL_NETWORK_NAME="$$DOCKER_NETWORK_NAME"
          fi
      fi

      # Create network only if it doesn't exist (idempotent approach)
      # NOTE: This does NOT recreate/modify existing networks, avoiding disruption to connected containers.
      # Previous behavior: forcefully recreated network every time, disconnecting/reconnecting all containers.
      # New behavior: uses existing network as-is. To change network config, manually delete network first.
      # Benefits: No container disruption, no "container not connected" errors, faster, more deterministic.
      if [[ -z "$$ACTUAL_NETWORK_NAME" ]]; then
          echo "Network $$DOCKER_NETWORK_NAME does not exist, creating it..."
          BRIDGE_OPT_NAME="br_$$DOCKER_NETWORK_NAME"
          $$DOCKER network create \
              --driver=bridge \
              --attachable \
              -o com.docker.network.bridge.name="$$BRIDGE_OPT_NAME" \
              -o com.docker.network.bridge.enable_ip_masquerade=false \
              --subnet="$${WARP_NAT_NET_SUBNET:-10.0.2.0/24}" \
              --gateway="$${WARP_NAT_NET_GATEWAY:-10.0.2.1}" \
              "$$DOCKER_NETWORK_NAME"
          ACTUAL_NETWORK_NAME="$$DOCKER_NETWORK_NAME"
          echo "Created network: $$ACTUAL_NETWORK_NAME"
      else
          echo "Found existing network: $$ACTUAL_NETWORK_NAME"
      fi

      echo "Found network: $$ACTUAL_NETWORK_NAME"

      # Dynamically get network subnet
      DOCKER_NET="$$($$DOCKER network inspect -f '{{(index .IPAM.Config 0).Subnet}}' "$$ACTUAL_NETWORK_NAME" 2>/dev/null | tr -d '[:space:]')"
      if [[ -z "$$DOCKER_NET" ]]; then
          echo "Error: Could not determine subnet for network $$ACTUAL_NETWORK_NAME"
          exit 1
      fi
      echo "Network subnet: $$DOCKER_NET"

      # Dynamically get the actual bridge name from the network
      BRIDGE_NAME="$$($$DOCKER network inspect -f '{{index .Options "com.docker.network.bridge.name"}}' "$$ACTUAL_NETWORK_NAME" 2>/dev/null)"
      if [[ -z "$$BRIDGE_NAME" || "$$BRIDGE_NAME" == "<no value>" ]]; then
          # Fallback: construct bridge name from network ID (Docker's default pattern)
          NETWORK_ID="$$($$DOCKER network inspect -f '{{.Id}}' "$$ACTUAL_NETWORK_NAME" 2>/dev/null | cut -c1-12)"
          BRIDGE_NAME="br-$$NETWORK_ID"
          echo "Bridge name not explicitly set, using Docker default: $$BRIDGE_NAME"
      else
          echo "Bridge device: $$BRIDGE_NAME"
      fi

      # Verify the bridge exists
      if ! ip link show "$$BRIDGE_NAME" >/dev/null 2>&1; then
          echo "Error: Bridge $$BRIDGE_NAME does not exist"
          echo "Available bridges:"
          ip link show type bridge
          exit 1
      fi

      # Get WARP container PID
      warp_pid="$$($$DOCKER inspect -f '{{.State.Pid}}' $$WARP_CONTAINER_NAME 2>/dev/null || echo \"\")"
      if [[ -z "$$warp_pid" || "$$warp_pid" == "0" ]]; then
          echo "Error: $$WARP_CONTAINER_NAME container not found or not running"
          exit 1
      fi

      if [[ ! -e "/proc/$$warp_pid/ns/net" ]]; then
          echo "Error: $$WARP_CONTAINER_NAME container network namespace not ready"
          exit 1
      fi

      echo "Found WARP container PID: $$warp_pid"

      # Clean orphan NAT rules inside warp container (for any stale subnets)
      echo "Cleaning orphan NAT rules inside WARP container..."
      nsenter -t "$$warp_pid" -n iptables -t nat -S POSTROUTING 2>/dev/null | grep -- '-j MASQUERADE' | while read -r rule; do
          s_net=$$(echo "$$rule" | sed -n 's/.*-s \([^ ]*\) -j MASQUERADE.*/\1/p')
          if [[ -z "$$s_net" ]]; then continue; fi
          if [[ "$$s_net" == "$$DOCKER_NET" ]]; then continue; fi
          echo "  Removing orphan NAT rule inside warp: $$s_net"
              del_rule=$$(echo "$$rule" | sed 's/^-A/-D/')
              nsenter -t "$$warp_pid" -n iptables -t nat $$del_rule 2>/dev/null || true
      done

      # Set up cleanup function for error handling
      cleanup() {
          echo "⚠️ Error occurred. Rolling back changes..."

          # Remove host veth if it was created
          if ip link show "$$VETH_HOST" >/dev/null 2>&1; then
              echo "Removing veth interface: $$VETH_HOST"
              ip link del "$$VETH_HOST" 2>/dev/null || true
          fi

          # Remove ip rule if it was added
          if ip rule show | grep -q "from $$DOCKER_NET lookup $$ROUTING_TABLE"; then
              echo "Removing routing rule: from $$DOCKER_NET lookup $$ROUTING_TABLE"
              ip rule del from "$$DOCKER_NET" table "$$ROUTING_TABLE" 2>/dev/null || true
          fi

          # Remove specific routes from routing table if they exist
          if ip route show table "$$ROUTING_TABLE" | grep -q "^default via $$CONT_VETH_IP dev $$VETH_HOST"; then
              echo "Removing default route from $$ROUTING_TABLE"
              ip route del default via "$$CONT_VETH_IP" dev "$$VETH_HOST" table "$$ROUTING_TABLE" 2>/dev/null || true
          fi
          if ip route show table "$$ROUTING_TABLE" | grep -q "^$$DOCKER_NET dev $$BRIDGE_NAME"; then
              echo "Removing network route from $$ROUTING_TABLE"
              ip route del "$$DOCKER_NET" dev "$$BRIDGE_NAME" table "$$ROUTING_TABLE" 2>/dev/null || true
          fi

          # Remove NAT rule on host if it exists
          if iptables -t nat -C POSTROUTING -s "$$DOCKER_NET" ! -d "$$DOCKER_NET" -j MASQUERADE 2>/dev/null; then
              echo "Removing NAT rule on host"
              iptables -t nat -D POSTROUTING -s "$$DOCKER_NET" ! -d "$$DOCKER_NET" -j MASQUERADE 2>/dev/null || true
          fi

          # Remove NAT rule inside warp container if it exists
          if [[ -n "$$warp_pid" ]] && [[ -e "/proc/$$warp_pid/ns/net" ]]; then
              if nsenter -t "$$warp_pid" -n iptables -t nat -C POSTROUTING -s "$$DOCKER_NET" -j MASQUERADE 2>/dev/null; then
                  echo "Removing NAT rule inside WARP container"
                  nsenter -t "$$warp_pid" -n iptables -t nat -D POSTROUTING -s "$$DOCKER_NET" -j MASQUERADE 2>/dev/null || true
              fi
          fi
          
          # CRITICAL: Ensure failsafe DROP rule is in place to prevent IP leaks
          if ! iptables -C FORWARD -s "$$DOCKER_NET" -j DROP -m comment --comment "warp-nat-failsafe" 2>/dev/null; then
              echo "Re-enabling failsafe DROP rule to prevent IP leaks"
              iptables -I FORWARD -s "$$DOCKER_NET" -j DROP -m comment --comment "warp-nat-failsafe" 2>/dev/null || true
          fi
      }

      # ==========================================
      # PHASE 4: CRITICAL SETUP WITH FAILSAFE
      # ==========================================
      echo ""
      echo "Phase 4: Setting up VETH tunnel and routing..."

      # Trap any error in the critical section
      trap cleanup ERR

      # CRITICAL FAILSAFE: Block all traffic from warp-nat-net by default
      # This prevents IP leaks if setup fails. Rule will be removed at the end if setup succeeds.
      echo "Installing failsafe DROP rule to prevent IP leaks during setup"
      # Add failsafe rule at the top of FORWARD chain (already cleaned in Phase 1)
      iptables -I FORWARD -s "$$DOCKER_NET" -j DROP -m comment --comment "warp-nat-failsafe"

      # Create veth pair (host side: $$VETH_HOST, container side: $$VETH_CONT)
      echo "Creating veth pair: $$VETH_HOST <-> $$VETH_CONT"
      ip link add "$$VETH_HOST" type veth peer name "$$VETH_CONT"

      # Move container end into warp namespace
      echo "Moving $$VETH_CONT into WARP container namespace"
      ip link set "$$VETH_CONT" netns "$$warp_pid"

      # Configure host end of veth
      echo "Configuring host veth: $$VETH_HOST ($$HOST_VETH_IP/30)"
      ip addr add "$$HOST_VETH_IP/30" dev "$$VETH_HOST"
      ip link set "$$VETH_HOST" up

      # Configure container end of veth
      echo "Configuring container veth: $$VETH_CONT ($$CONT_VETH_IP/30)"
      nsenter -t "$$warp_pid" -n ip addr add "$$CONT_VETH_IP/30" dev "$$VETH_CONT"
      nsenter -t "$$warp_pid" -n ip link set "$$VETH_CONT" up
      nsenter -t "$$warp_pid" -n sysctl -w net.ipv4.ip_forward=1 >/dev/null
      #nsenter -t "$$warp_pid" -n sysctl -w net.ipv4.conf.all.rp_filter=2
      #nsenter -t "$$warp_pid" -n sysctl -w net.ipv4.conf.default.rp_filter=2

      # Setup NAT inside warp container
      echo "Setting up NAT inside WARP container for $$DOCKER_NET"
      nsenter -t "$$warp_pid" -n iptables -t nat -C POSTROUTING -s "$$DOCKER_NET" -j MASQUERADE 2>/dev/null || \
      nsenter -t "$$warp_pid" -n iptables -t nat -A POSTROUTING -s "$$DOCKER_NET" -j MASQUERADE

      # Setup routing rules
      echo "Adding routing rule: from $$DOCKER_NET lookup $$ROUTING_TABLE"
      ip rule add from "$$DOCKER_NET" table "$$ROUTING_TABLE"

      # Setup routing table entries (delete specific routes first if they exist)
      echo "Configuring routes in routing table $$ROUTING_TABLE"

      # Remove existing network route if present
      if ip route show table "$$ROUTING_TABLE" | grep -q "^$$DOCKER_NET dev $$BRIDGE_NAME"; then
          echo "  Removing existing network route for $$DOCKER_NET"
          ip route del "$$DOCKER_NET" dev "$$BRIDGE_NAME" table "$$ROUTING_TABLE" 2>/dev/null || true
      fi

      # Remove existing default route if present
      if ip route show table "$$ROUTING_TABLE" | grep -q "^default via $$CONT_VETH_IP dev $$VETH_HOST"; then
          echo "  Removing existing default route"
          ip route del default via "$$CONT_VETH_IP" dev "$$VETH_HOST" table "$$ROUTING_TABLE" 2>/dev/null || true
      fi

      # Add the routes
      echo "  Adding network route: $$DOCKER_NET dev $$BRIDGE_NAME"
      ip route add "$$DOCKER_NET" dev "$$BRIDGE_NAME" table "$$ROUTING_TABLE"
      echo "  Adding default route: default via $$CONT_VETH_IP dev $$VETH_HOST"
      ip route add default via "$$CONT_VETH_IP" dev "$$VETH_HOST" table "$$ROUTING_TABLE"

      # Setup NAT on host
      echo "Setting up NAT on host for $$DOCKER_NET"
      iptables -t nat -C POSTROUTING -s "$$DOCKER_NET" ! -d "$$DOCKER_NET" -j MASQUERADE 2>/dev/null || \
      iptables -t nat -A POSTROUTING -s "$$DOCKER_NET" ! -d "$$DOCKER_NET" -j MASQUERADE

      # CRITICAL: Remove failsafe DROP rule now that routing is properly configured
      echo "Removing failsafe DROP rule - routing is now active"
      while iptables -D FORWARD -s "$$DOCKER_NET" -j DROP -m comment --comment "warp-nat-failsafe" 2>/dev/null; do 
          echo "  Removed failsafe rule"
      done

      # Disable error trap now that setup completed successfully
      trap - ERR

      # ==========================================
      # SETUP COMPLETE
      # ==========================================
      echo ""
      echo "=========================================="
      echo "✅ WARP NAT setup complete"
      echo "=========================================="
      echo "Network:        $$DOCKER_NETWORK_NAME"
      echo "Subnet:         $$DOCKER_NET"
      echo "Veth host:      $$VETH_HOST ($$HOST_VETH_IP)"
      echo "Veth container: $$VETH_CONT ($$CONT_VETH_IP)"
      echo "Routing table:  $$ROUTING_TABLE (ID: $$ROUTING_TABLE_ID)"
      echo "Bridge:         $$BRIDGE_NAME"
      echo "=========================================="

  warp-monitor.sh:
    content: |
      #!/usr/bin/env bash
      set -euo pipefail

      # Configurable via env
      DOCKER_CMD="$${DOCKER_CMD:-docker -H $${DOCKER_HOST:-unix:///var/run/docker.sock}}"
      CHECK_IMAGE="$${CHECK_IMAGE:-curlimages/curl}"   # image that includes curl
      NETWORK="$${NETWORK:-warp-nat-net}"
      SLEEP_INTERVAL="$${SLEEP_INTERVAL:-5}"                  # seconds between checks

      # Healthcheck command to run inside the ephemeral container.
      # This mirrors your warp-healthcheck logic: exit 0 when WARP active, nonzero otherwise.
      HEALTHCHECK_INSIDE='sh -c "if curl -s --max-time 4 https://cloudflare.com/cdn-cgi/trace | grep -qE \"^warp=on|warp=plus$$\"; then echo WARP_OK && exit 0; else echo WARP_NOT_OK && exit 1; fi"'

      echo "warp-monitor: checking WARP via ephemeral container on network '$${NETWORK}'."
      echo "Using image: $${CHECK_IMAGE}"
      prev_ok=1  # assume healthy initially so we don't run setup at startup
      fail_count=0
      RETRY_SETUP_AFTER="$${RETRY_SETUP_AFTER:-12}"  # retry setup after N consecutive failures

      while true; do
        echo "[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] running health probe..."
        if $${DOCKER_CMD} run --rm --network "$${NETWORK}" --entrypoint sh "$${CHECK_IMAGE}" -c "$${HEALTHCHECK_INSIDE}"; then
          # check succeeded
          if [[ "$${prev_ok}" -eq 0 ]]; then
            echo "[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] health probe recovered -> marking healthy"
          fi
          prev_ok=1
          fail_count=0
        else
          echo "[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] health probe failed (consecutive failures: $$((fail_count + 1)))"
          fail_count=$$((fail_count + 1))
          
          # Run setup on first failure OR after N consecutive failures
          if [[ "$${prev_ok}" -eq 1 ]] || [[ "$${fail_count}" -ge "$${RETRY_SETUP_AFTER}" ]]; then
          if [[ "$${prev_ok}" -eq 1 ]]; then
            echo "[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] detected healthy->unhealthy transition; running /usr/local/bin/warp-nat-setup.sh"
            else
              echo "[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] still unhealthy after $$fail_count failures; retrying /usr/local/bin/warp-nat-setup.sh"
              fail_count=0  # reset counter after retry
            fi
            # Run setup, but do not let its failure kill the monitor. Log failures.
            if /usr/local/bin/warp-nat-setup.sh; then
              echo "[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] warp-nat-setup.sh completed"
            else
              echo "[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] warp-nat-setup.sh failed (exit nonzero)."
            fi
            # mark as unhealthy until probe says otherwise
            prev_ok=0
            # Wait a little before probing again to avoid tight loops
            sleep "$${SLEEP_INTERVAL}"
            # continue to next iteration (which will probe again and wait for recovery)
          else
            echo "[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] still unhealthy ($$fail_count failures); will retry setup after $$RETRY_SETUP_AFTER consecutive failures"
          fi
        fi

        sleep "$${SLEEP_INTERVAL}"
      done

services:
  warp-net-init:
    # Creates the warp-nat-net network if it doesn't exist
    image: docker:cli
    container_name: warp-net-init
    network_mode: host
    command:
      - sh
      - -c
      - |
        # Create network if it doesn't exist
        if ! docker network inspect ${DOCKER_NETWORK_NAME:-warp-nat-net} >/dev/null 2>&1; then
          echo "Creating network ${DOCKER_NETWORK_NAME:-warp-nat-net}..."
          docker network create \
            --driver=bridge \
            --attachable \
            -o com.docker.network.bridge.name=br_${DOCKER_NETWORK_NAME:-warp-nat-net} \
            -o com.docker.network.bridge.enable_ip_masquerade=false \
            --subnet=${WARP_NAT_NET_SUBNET:-10.0.2.0/24} \
            --gateway=${WARP_NAT_NET_GATEWAY:-10.0.2.1} \
            ${DOCKER_NETWORK_NAME:-warp-nat-net}
          echo "Network created successfully"
        else
          echo "Network ${DOCKER_NETWORK_NAME:-warp-nat-net} already exists"
        fi
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
    restart: "no"

  warp-nat-gateway:
    # 🔹🔹 WARP in Docker (with NAT)🔹
    depends_on:
      warp-net-init:
        condition: service_completed_successfully
    image: docker.io/caomingjun/warp
    container_name: warp-nat-gateway
    hostname: warp-nat-gateway
    extra_hosts:
      - host.docker.internal:host-gateway
    secrets:
      - warp-license-key
    expose:
      - ${GOST_SOCKS5_PORT:-1080} # SOCKS5 proxy port
    # add removed rule back (https://github.com/opencontainers/runc/pull/3468)
    device_cgroup_rules:
      - "c 10:200 rwm"
    cap_add:
      # Docker already have them, these are for podman users
      - MKNOD
      - AUDIT_WRITE
      # additional required cap for warp, both for podman and docker
      - NET_ADMIN
    sysctls:
      - net.ipv6.conf.all.disable_ipv6=0
      - net.ipv4.conf.all.src_valid_mark=1
      - net.ipv4.ip_forward=1
      - net.ipv6.conf.all.forwarding=1
      - net.ipv6.conf.all.accept_ra=2
    volumes:
      - warp-config-data:/var/lib/cloudflare-warp
    environment:
      # If set, will add checks for host connectivity into healthchecks and automatically fix it if necessary.
      # See https://github.com/cmj2002/warp-docker/blob/main/docs/host-connectivity.md for more information.
      BETA_FIX_HOST_CONNECTIVITY: false
      # The arguments passed to GOST. The default is -L :1080, which means to listen on port 1080 in the container at the same time through HTTP and SOCKS5 protocols.
      # If you want to have UDP support or use advanced features provided by other protocols, you can modify this parameter. For more information, refer to https://v2.gost.run/en/.
      # If you modify the port number, ensure you modify the `ports:` section above.
      GOST_ARGS: ${GOST_ARGS:--L :${GOST_SOCKS5_PORT:-1080}}
      # If set, will register consumer account (WARP or WARP+, in contrast to Zero Trust) even when mdm.xml exists.
      # You usually don't need this, as mdm.xml are usually used for Zero Trust.
      # However, some users may want to adjust advanced settings in mdm.xml while still using consumer account.
      #REGISTER_WHEN_MDM_EXISTS: ""
      # If set, will work as warp mode and turn NAT on.
      # You can route L3 traffic through warp-docker to Warp.
      # See https://github.com/cmj2002/warp-docker/blob/main/docs/nat-gateway.md for more information.
      WARP_ENABLE_NAT: false
      # The license key of the WARP client, which is optional.
      # If you have subscribed to WARP+ service, you can fill in the key in this environment variable.
      # If you have not subscribed to WARP+ service, you can ignore this environment variable.
      WARP_LICENSE_KEY_FILE: /run/secrets/warp-license-key
      # The time to wait for the WARP daemon to start, in seconds.
      # If the time is too short, it may cause the WARP daemon to not start before using the proxy, resulting in the proxy not working properly.
      # If the time is too long, it may cause the container to take too long to start. If your server has poor performance, you can increase this value appropriately.
      WARP_SLEEP: 2 # The default is 2 seconds.
    healthcheck: &warp-healthcheck
      test:
        [
          "CMD-SHELL",
          "sh -c \"if curl -s https://cloudflare.com/cdn-cgi/trace | grep -qE '^warp=on|warp=plus$$'; then echo \\\"Cloudflare WARP is active.\\\" && exit 0; else echo \\\"Cloudflare WARP is not active.\\\" && exit 1; fi\"",
        ]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: always

  warp_router:
    depends_on:
      warp-nat-gateway:
        condition: service_healthy
    build:
      dockerfile_inline: |
        FROM alpine
        RUN apk update && apk add \
            bash bc curl docker-cli ipcalc iproute2 iptables jq util-linux && \
            rm -rf /var/cache/apk/*
    container_name: warp_router
    # run the monitor wrapper (it will run warp-nat-setup.sh ONLY when this container's health becomes 'unhealthy')
    command:
      - /bin/bash
      - /usr/local/bin/warp-monitor.sh
    privileged: true
    # unsure if any of these are required or not
    #    cap_add:
    #      - NET_ADMIN
    #      - NET_RAW
    #      - SYS_ADMIN
    network_mode: host
    healthcheck:
      test:
        [
          "CMD-SHELL",
          'docker run --rm --network ${DOCKER_NETWORK_NAME:-warp-nat-net} --entrypoint sh curlimages/curl -c ''if curl -s --max-time 4 https://cloudflare.com/cdn-cgi/trace | grep -qE "^warp=on|warp=plus$$"; then exit 0; else exit 1; fi''',
        ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    configs:
      - source: warp-nat-setup.sh
        target: /usr/local/bin/warp-nat-setup.sh
        mode: 700
      - source: warp-monitor.sh
        target: /usr/local/bin/warp-monitor.sh
        mode: 700
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:rw
      - /etc/iproute2/rt_tables:/etc/iproute2/rt_tables:rw
      - /proc:/proc:rw
    environment:
      DOCKER_NETWORK_NAME: ${DOCKER_NETWORK_NAME:-warp-nat-net}
      WARP_CONTAINER_NAME: ${WARP_CONTAINER_NAME:-warp-nat-gateway}
      HOST_VETH_IP: ${HOST_VETH_IP:-169.254.100.1}
      CONT_VETH_IP: ${CONT_VETH_IP:-169.254.100.2}
      ROUTING_TABLE: ${ROUTING_TABLE:-warp-nat-routing}
      VETH_HOST: ${VETH_HOST:-veth-warp} # 9 character maximum (Linux interface name limit is 15)
      ROUTER_CONTAINER_NAME: ${ROUTER_CONTAINER_NAME:-warp_router}
      WARP_NAT_NET_SUBNET: ${WARP_NAT_NET_SUBNET:-10.0.2.0/24}
      WARP_NAT_NET_GATEWAY: ${WARP_NAT_NET_GATEWAY:-10.0.2.1}
      RETRY_SETUP_AFTER: ${RETRY_SETUP_AFTER:-12} # retry setup after N consecutive health check failures
      SLEEP_INTERVAL: ${SLEEP_INTERVAL:-5} # seconds between health checks
    restart: always

  ip-checker-warp:
    # 🔹🔹 IP Checker WARP 🔹🔹
    # This is a service that checks the IP address of the container.
    # It uses the WARP network interface.
    image: docker.io/alpine
    container_name: ip-checker-warp
    networks:
      - warp-nat-net
    command:
      - /bin/sh
      - -c
      - |
        apk add --no-cache curl ipcalc
        while true; do echo "$(date): $(curl -s --max-time 4 ifconfig.me)"; sleep 5; done
    # End of Selection
    healthcheck: *warp-healthcheck
    restart: always
