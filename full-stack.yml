name: my-media-stack
services:
  aiostreams:
    cpus: 1
    container_name: aiostreams
    environment:
      ADDON_CATALOG_CACHE_TTL: "300"
      ADDON_ID: aiostreams.bolabaden.org
      ADDON_NAME: BadenAIO
      ADDON_PASSWORD: sk_4dc059c0399c43fd94c09baaf0b94da119fc526775914bf2b3a3fb6e073e26d9
      ANIME_API_RATE_LIMIT_MAX_REQUESTS: "120"
      ANIME_API_RATE_LIMIT_WINDOW: "60"
      ANIME_DB_EXTENDED_ANITRAKT_MOVIES_REFRESH_INTERVAL: "86400000"
      ANIME_DB_EXTENDED_ANITRAKT_TV_REFRESH_INTERVAL: "86400000"
      ANIME_DB_FRIBB_MAPPINGS_REFRESH_INTERVAL: "86400000"
      ANIME_DB_KITSU_IMDB_MAPPING_REFRESH_INTERVAL: "86400000"
      ANIME_DB_LEVEL_OF_DETAIL: required
      ANIME_DB_MANAMI_DB_REFRESH_INTERVAL: "604800000"
      BASE_URL: https://aiostreams.bolabaden.org
      BUILTIN_ANIMETOSHO_URL: https://feed.animetosho.org
      BUILTIN_DEBRID_INSTANT_AVAILABILITY_CACHE_TTL: "1800"
      BUILTIN_DEBRID_PLAYBACK_LINK_CACHE_TTL: "3600"
      BUILTIN_GDRIVE_PAGE_SIZE_LIMIT: "1000"
      BUILTIN_GET_TORRENT_CONCURRENCY: "100"
      BUILTIN_GET_TORRENT_TIMEOUT: "5000"
      BUILTIN_NAB_CAPABILITIES_CACHE_TTL: "1209600"
      BUILTIN_NAB_SEARCH_CACHE_TTL: "604800"
      BUILTIN_NAB_SEARCH_TIMEOUT: "30000"
      BUILTIN_PROWLARR_INDEXERS_CACHE_TTL: "1209600"
      BUILTIN_PROWLARR_SEARCH_CACHE_TTL: "604800"
      BUILTIN_PROWLARR_SEARCH_TIMEOUT: null
      BUILTIN_STREMTHRU_URL: https://stremthru.13377001.xyz
      BUILTIN_TORBOX_SEARCH_CACHE_PER_USER_SEARCH_ENGINE: "false"
      BUILTIN_TORBOX_SEARCH_SEARCH_API_TIMEOUT: "30000"
      BUILTIN_ZILEAN_URL: https://zilean.elfhosted.com
      CATALOG_API_RATE_LIMIT_MAX_REQUESTS: "5"
      CATALOG_API_RATE_LIMIT_WINDOW: "5"
      CATALOG_CACHE_TTL: "300"
      CUSTOM_HTML: ""
      DATABASE_URI: sqlite://./data/db.sqlite
      DEFAULT_ALLDEBRID_API_KEY_FILE: /run/secrets/alldebrid-api-key
      DEFAULT_DEBRIDER_API_KEY: null
      DEFAULT_DEBRIDLINK_API_KEY_FILE: /run/secrets/debridlink-api-key
      DEFAULT_EASYDEBRID_API_KEY: ""
      DEFAULT_EASYNEWS_PASSWORD: ""
      DEFAULT_EASYNEWS_USERNAME: ""
      DEFAULT_JACKETTIO_INDEXERS: '["animetosho", "anirena", "bitsearch", "eztv", "nyaasi", "thepiratebay", "therarbg", "yts"]'
      DEFAULT_MAX_CACHE_SIZE: "100000"
      DEFAULT_OFFCLOUD_API_KEY_FILE: /run/secrets/offcloud-api-key
      DEFAULT_OFFCLOUD_EMAIL: boden.crouch@gmail.com
      DEFAULT_OFFCLOUD_PASSWORD_FILE: /run/secrets/offcloud-password
      DEFAULT_PIKPAK_EMAIL: ""
      DEFAULT_PIKPAK_PASSWORD: ""
      DEFAULT_PREMIUMIZE_API_KEY_FILE: /run/secrets/premiumize-api-key
      DEFAULT_PROXY_CREDENTIALS: bolabaden:duckdns
      DEFAULT_PROXY_ID: stremthru
      DEFAULT_PUTIO_CLIENT_ID: ""
      DEFAULT_PUTIO_CLIENT_SECRET: ""
      DEFAULT_REALDEBRID_API_KEY_FILE: /run/secrets/realdebrid-api-key
      DEFAULT_SEEDR_ENCODED_TOKEN: ""
      DEFAULT_TIMEOUT: "15000"
      DEFAULT_TORBOX_API_KEY_FILE: /run/secrets/torbox-api-key
      DISABLE_RATE_LIMITS: "false"
      DISABLE_SELF_SCRAPING: "true"
      ENCRYPT_MEDIAFLOW_URLS: "true"
      ENCRYPT_STREMTHRU_URLS: "true"
      FORCE_PROXY_DISABLE_PROXIED_ADDONS: "false"
      FORCE_PROXY_ENABLED: "true"
      FORCED_DEBRIDER_API_KEY: null
      FORMAT_API_RATE_LIMIT_MAX_REQUESTS: "30"
      FORMAT_API_RATE_LIMIT_WINDOW: "5"
      LOG_FORMAT: text
      LOG_LEVEL: info
      LOG_SENSITIVE_INFO: "true"
      LOG_TIMEZONE: Etc/UTC
      MANIFEST_CACHE_TTL: "300"
      MAX_ADDONS: "100"
      MAX_GROUPS: "50"
      MAX_KEYWORD_FILTERS: "50"
      MAX_STREAM_EXPRESSION_FILTERS: "200"
      MAX_TIMEOUT: "50000"
      MEDIAFUSION_API_PASSWORD_FILE: /run/secrets/sudo-password
      META_CACHE_TTL: "300"
      MIN_TIMEOUT: "1000"
      NODE_OPTIONS: --dns-result-order=ipv4first
      PGID: "999"
      PORT: "3000"
      PRECACHE_NEXT_EPISODE_MIN_INTERVAL: "86400"
      PROXY_IP_CACHE_TTL: "900"
      PRUNE_INTERVAL: "86400"
      PRUNE_MAX_DAYS: "-1"
      PUID: "1001"
      REGEX_FILTER_ACCESS: trusted
      RPDB_API_KEY_VALIDITY_CACHE_TTL: "604800"
      SECRET_KEY: 207d3e5d2dd7edca578b121814f32ed4d73568b5059fd710637a40d310928cde
      STATIC_RATE_LIMIT_MAX_REQUESTS: "75"
      STATIC_RATE_LIMIT_WINDOW: "5"
      STREAM_API_RATE_LIMIT_MAX_REQUESTS: "10"
      STREAM_API_RATE_LIMIT_WINDOW: "5"
      STREAM_CACHE_TTL: "1"
      STREMIO_CATALOG_RATE_LIMIT_MAX_REQUESTS: "30"
      STREMIO_CATALOG_RATE_LIMIT_WINDOW: "5"
      STREMIO_MANIFEST_RATE_LIMIT_MAX_REQUESTS: "5"
      STREMIO_MANIFEST_RATE_LIMIT_WINDOW: "5"
      STREMIO_META_RATE_LIMIT_MAX_REQUESTS: "15"
      STREMIO_META_RATE_LIMIT_WINDOW: "5"
      STREMIO_STREAM_RATE_LIMIT_MAX_REQUESTS: "10"
      STREMIO_STREAM_RATE_LIMIT_WINDOW: "15"
      STREMIO_SUBTITLE_RATE_LIMIT_MAX_REQUESTS: "10"
      STREMIO_SUBTITLE_RATE_LIMIT_WINDOW: "5"
      STREMTHRU_STORE_URL: https://stremthru.bolabaden.org/stremio/store/
      STREMTHRU_TORZ_URL: https://stremthru.bolabaden.org/stremio/torz/
      SUBTITLE_CACHE_TTL: "300"
      TMDB_ACCESS_TOKEN_FILE: /run/secrets/tmdb-access-token
      TMDB_API_KEY_FILE: /run/secrets/tmdb-api-key
      TRAKT_CLIENT_ID: 052d735c61c723ed670991aee9d6322822ed6766702d66910be63a1e1a511fd0
      TRUSTED_UUIDS: ""
      TZ: America/Chicago
      UMASK: "002"
      USER_API_RATE_LIMIT_MAX_REQUESTS: "5"
      USER_API_RATE_LIMIT_WINDOW: "5"
    expose:
      - "3000"
    hostname: aiostreams
    image: ghcr.io/viren070/aiostreams
    labels:
      homepage.description: Stremio add-on that aggregates multiple sources into a single unified stream catalog
      homepage.group: Stremio Addons
      homepage.href: https://aiostreams.bolabaden.org/
      homepage.icon: aiostreams.png
      homepage.name: AIOStreams
      kuma.aiostreams.http.interval: "60"
      kuma.aiostreams.http.name: aiostreams.beatapostapita.bolabaden.org
      kuma.aiostreams.http.url: https://aiostreams.bolabaden.org
      traefik.enable: "true"
      traefik.http.routers.aiostreams.rule: Host(`aiostreams.bolabaden.org`) || Host(`aiostreams.beatapostapita.bolabaden.org`)
      traefik.http.services.aiostreams.loadbalancer.server.port: "3000"
    mem_limit: "1073741824"
    mem_reservation: "52428800"
    networks:
      publicnet: {}
      warp-nat-net:
        gw_priority: 1
    restart: always
    secrets:
      - source: aiostreams-secret-key
        target: /run/secrets/aiostreams-secret-key
      - source: aiostreams-addon-password
        target: /run/secrets/aiostreams-addon-password
      - source: tmdb-access-token
        target: /run/secrets/tmdb-access-token
      - source: tmdb-api-key
        target: /run/secrets/tmdb-api-key
      - source: realdebrid-api-key
        target: /run/secrets/realdebrid-api-key
      - source: alldebrid-api-key
        target: /run/secrets/alldebrid-api-key
      - source: premiumize-api-key
        target: /run/secrets/premiumize-api-key
      - source: debridlink-api-key
        target: /run/secrets/debridlink-api-key
      - source: torbox-api-key
        target: /run/secrets/torbox-api-key
      - source: offcloud-api-key
        target: /run/secrets/offcloud-api-key
      - source: offcloud-password
        target: /run/secrets/offcloud-password
    sysctls:
      net.ipv6.conf.all.disable_ipv6: "1"
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/stremio/addons/aiostreams/data
        target: /app/data
        bind: {}
  autokuma:
    container_name: autokuma
    environment:
      AUTOKUMA__KUMA__CALL_TIMEOUT: "5"
      AUTOKUMA__KUMA__CONNECT_TIMEOUT: "5"
      AUTOKUMA__KUMA__PASSWORD: h4L0m4St3R327
      AUTOKUMA__KUMA__URL: https://uptimekuma.bolabaden.org
      AUTOKUMA__KUMA__USERNAME: brunner56
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: autokuma
    image: ghcr.io/bigboot/autokuma
    networks:
      backend: null
    restart: always
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        read_only: true
        bind: {}
  blackbox-exporter:
    command:
      - --config.file=/etc/blackbox_exporter/config.yml
      - --log.level=info
      - --log.format=logfmt
    configs:
      - source: blackbox.yml
        target: /etc/blackbox_exporter/config.yml
        mode: "0777"
    container_name: blackbox-exporter
    expose:
      - "9115"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: blackbox-exporter
    image: docker.io/prom/blackbox-exporter
    labels:
      homepage.description: Blackbox probing of endpoints over HTTP, HTTPS, DNS, TCP and ICMP
      homepage.group: Monitoring
      homepage.href: https://blackbox.bolabaden.org/
      homepage.icon: prometheus.png
      homepage.name: Blackbox Exporter
      kuma.blackbox-exporter.http.interval: "60"
      kuma.blackbox-exporter.http.name: blackbox.beatapostapita.bolabaden.org
      kuma.blackbox-exporter.http.url: https://blackbox.bolabaden.org
      prometheus.io/path: metrics
      prometheus.io/port: "9115"
      prometheus.io/scrape: "true"
      traefik.enable: "true"
      traefik.http.routers.blackbox-exporter.rule: Host(`blackbox.bolabaden.org`) || Host(`blackbox.beatapostapita.bolabaden.org`)
      traefik.http.services.blackbox-exporter.loadbalancer.server.port: "9115"
    networks:
      backend: null
      publicnet: null
    restart: always
  bolabaden-nextjs:
    build:
      context: /home/ubuntu/my-media-stack/projects/website/bolabaden-site
      dockerfile: Dockerfile
    container_name: bolabaden-nextjs
    environment:
      ALLOW_ORIGIN: '*'
      HOSTNAME: 0.0.0.0
      NODE_ENV: production
      PGID: "999"
      PORT: "3000"
      PUID: "1001"
      TZ: America/Chicago
      UMASK: "002"
    expose:
      - "3000"
    hostname: bolabaden-nextjs
    healthcheck:
      test:
        - CMD-SHELL
        - wget -qO- http://127.0.0.1:$${PORT:-3000} > /dev/null 2>&1 || exit 1
      timeout: 10s
      start_period: 30s
    image: docker.io/bolabaden/bolabaden-nextjs
    labels:
      kuma.bolabaden-nextjs.http.interval: "30"
      kuma.bolabaden-nextjs.http.name: beatapostapita.bolabaden.org
      kuma.bolabaden-nextjs.http.url: https://bolabaden.org
      traefik.enable: "true"
      traefik.http.middlewares.bolabaden-error-pages.errors.query: /api/error/{status}
      traefik.http.middlewares.bolabaden-error-pages.errors.service: bolabaden-nextjs@docker
      traefik.http.middlewares.bolabaden-error-pages.errors.status: 400-599
      traefik.http.routers.bolabaden-nextjs.rule: Host(`bolabaden.org`) || Host(`beatapostapita.bolabaden.org`)
      traefik.http.services.bolabaden-nextjs.loadbalancer.server.port: "3000"
    networks:
      backend: null
      publicnet: null
    restart: always
  cadvisor:
    command:
      - --housekeeping_interval=30s
      - --docker_only=true
      - --disable_metrics=cpu_topology,disk,memory_numa,tcp,udp,percpu,sched,process,hugetlb,referenced_memory,resctrl,cpuset,advtcp
      - --store_container_labels=false
    container_name: cadvisor
    devices:
      - source: /dev/kmsg
        target: /dev/kmsg
        permissions: rwm
    expose:
      - "8080"
    hostname: cadvisor
    healthcheck:
      test:
        - CMD-SHELL
        - command -v wget >/dev/null || (apt-get update >/dev/null 2>&1 && apt-get install -y wget >/dev/null 2>&1); wget --no-verbose --tries=1 --spider http://127.0.0.1:8080/metrics || exit 1
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 30s
    image: gcr.io/cadvisor/cadvisor
    labels:
      deunhealth.restart.on.unhealthy: "true"
      homepage.description: Container resource usage and performance characteristics
      homepage.group: Monitoring
      homepage.href: https://cadvisor.bolabaden.org/
      homepage.icon: https://raw.githubusercontent.com/google/cadvisor/master/logo.png
      homepage.name: cAdvisor
      kuma.cadvisor.http.interval: "60"
      kuma.cadvisor.http.name: cadvisor.beatapostapita.bolabaden.org
      kuma.cadvisor.http.url: https://cadvisor.bolabaden.org
      prometheus.io/path: metrics
      prometheus.io/port: "8080"
      prometheus.io/scrape: "true"
      traefik.enable: "true"
      traefik.http.routers.cadvisor.middlewares: nginx-auth@file
      traefik.http.routers.cadvisor.rule: Host(`cadvisor.bolabaden.org`) || Host(`cadvisor.beatapostapita.bolabaden.org`)
      traefik.http.services.cadvisor.loadbalancer.server.port: "8080"
    networks:
      backend: null
      publicnet: null
    privileged: true
    restart: always
    volumes:
      - type: bind
        source: /
        target: /rootfs
        read_only: true
        bind: {}
      - type: bind
        source: /var/run
        target: /var/run
        read_only: true
        bind: {}
      - type: bind
        source: /sys
        target: /sys
        read_only: true
        bind: {}
      - type: bind
        source: /var/lib/docker/
        target: /var/lib/docker
        read_only: true
        bind: {}
      - type: bind
        source: /dev/disk/
        target: /dev/disk
        read_only: true
        bind: {}
  cloudflare-ddns:
    cap_drop:
      - all
    container_name: cloudflare-ddns
    environment:
      CLOUDFLARE_API_TOKEN_FILE: /run/secrets/cloudflare-api-token
      DOMAINS: beatapostapita.bolabaden.org,*.beatapostapita.bolabaden.org
      PROXIED: is(bolabaden.org)||is(*.bolabaden.org)
      RECORD_COMMENT: Updated by Cloudflare DDNS on server `beatapostapita.bolabaden.org`
      TTL: "1"
      TZ: America/Chicago
    image: docker.io/favonia/cloudflare-ddns:1
    network_mode: host
    read_only: true
    restart: always
    secrets:
      - source: cloudflare-api-token
        target: /run/secrets/cloudflare-api-token
    security_opt:
      - no-new-privileges:true
  crowdsec:
    configs:
      - source: crowdsec-config.yaml
        target: /etc/crowdsec/config.yaml
        mode: "0644"
      - source: crowdsec-acquis.yaml
        target: /etc/crowdsec/acquis.yaml
        mode: "0644"
      - source: crowdsec-profiles.yaml
        target: /etc/crowdsec/profiles.yaml
        mode: "0644"
      - source: crowdsec-victoriametrics.yaml
        target: /etc/crowdsec/notifications/victoriametrics.yaml
        mode: "0644"
      - source: crowdsec-email.yaml
        target: /etc/crowdsec/email.yaml
        mode: "0644"
      - source: crowdsec-file.yaml
        target: /etc/crowdsec/file.yaml
        mode: "0644"
      - source: crowdsec-splunk.yaml
        target: /etc/crowdsec/http.yaml
        mode: "0644"
      - source: crowdsec-sentinel.yaml
        target: /etc/crowdsec/sentinel.yaml
        mode: "0644"
      - source: crowdsec-auth.log
        target: /var/log/auth.log
        mode: "0644"
      - source: crowdsec-syslog
        target: /var/log/syslog
        mode: "0644"
    container_name: crowdsec
    environment:
      COLLECTIONS: crowdsecurity/appsec-crs crowdsecurity/appsec-generic-rules crowdsecurity/appsec-virtual-patching crowdsecurity/whitelist-good-actors crowdsecurity/base-http-scenarios crowdsecurity/http-cve crowdsecurity/linux crowdsecurity/sshd
      GID: "999"
      UID: "1001"
    expose:
      - "8080"
      - "7422"
      - "6060"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: crowdsec
    healthcheck:
      test:
        - CMD-SHELL
        - cscli version
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 10s
    image: docker.io/crowdsecurity/crowdsec:latest
    networks:
      backend: null
      default: null
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/traefik/crowdsec/data
        target: /var/lib/crowdsec/data
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/traefik/crowdsec/etc/crowdsec
        target: /etc/crowdsec
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/traefik/crowdsec/plugins
        target: /usr/local/lib/crowdsec/plugins
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/traefik/logs
        target: /var/log/traefik
        read_only: true
        bind: {}
  docker-gen-failover:
    command:
      - -endpoint
      - tcp://dockerproxy-rw:2375
      - -only-exposed
      - -include-stopped
      - -event-filter
      - event=start
      - -event-filter
      - event=create
      - -event-filter
      - event=expose
      - -event-filter
      - event=update
      - -event-filter
      - event=connect
      - -event-filter
      - label=traefik.enable=true
      - -container-filter
      - label=traefik.enable=true
      - -watch
      - /templates/traefik-failover-dynamic.conf.tmpl
      - /traefik/dynamic/failover-fallbacks.yaml
    configs:
      - source: traefik-failover-dynamic.conf.tmpl
        target: /templates/traefik-failover-dynamic.conf.tmpl
        mode: "0400"
    container_name: docker-gen-failover
    image: docker.io/nginxproxy/docker-gen
    networks:
      backend: null
      default: null
    restart: "no"
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/traefik/dynamic
        target: /traefik/dynamic
        bind: {}
  dockerproxy-ro:
    container_name: dockerproxy-ro
    environment:
      CONTAINERS: "1"
      DISABLE_IPV6: "0"
      EVENTS: "1"
      INFO: "1"
      NETWORKS: "1"
      PGID: "999"
      PUID: "1001"
      TZ: America/Chicago
      UMASK: "002"
    hostname: dockerproxy-ro
    healthcheck:
      test:
        - CMD-SHELL
        - wget -qO- http://127.0.0.1:2375/_ping > /dev/null 2>&1 || exit 1
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 10s
    image: docker.io/tecnativa/docker-socket-proxy
    labels:
      deunhealth.restart.on.unhealthy: "true"
    networks:
      default: null
    privileged: true
    restart: always
    userns_mode: host
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        bind: {}
  dockerproxy-rw:
    container_name: dockerproxy-rw
    environment:
      ALLOW_RESTARTS: "1"
      ALLOW_START: "1"
      ALLOW_STOP: "1"
      AUTH: "1"
      BUILD: "1"
      COMMIT: "1"
      CONFIGS: "1"
      CONTAINERS: "1"
      DISABLE_IPV6: "0"
      DISTRIBUTION: "1"
      EVENTS: "1"
      EXEC: "1"
      IMAGES: "1"
      INFO: "1"
      LOG_LEVEL: info
      NETWORKS: "1"
      NODES: "1"
      PGID: "999"
      PING: "1"
      PLUGINS: "1"
      POST: "1"
      PUID: "1001"
      SECRETS: "1"
      SERVICES: "1"
      SESSION: "1"
      SWARM: "1"
      SYSTEM: "1"
      TASKS: "1"
      TZ: America/Chicago
      UMASK: "002"
      VERSION: "1"
      VOLUMES: "1"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: dockerproxy-rw
    image: lscr.io/linuxserver/socket-proxy
    networks:
      backend: null
      default: null
    ports:
      - mode: ingress
        host_ip: 127.0.0.1
        target: 2375
        published: "2375"
        protocol: tcp
    privileged: true
    restart: always
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        bind: {}
  dozzle:
    container_name: dozzle
    depends_on:
      dockerproxy-ro:
        condition: service_started
        required: true
    environment:
      DOZZLE_ADDR: :8080
      DOZZLE_AUTH_HEADER_EMAIL: ""
      DOZZLE_AUTH_HEADER_NAME: ""
      DOZZLE_AUTH_HEADER_USER: ""
      DOZZLE_AUTH_PROVIDER: none
      DOZZLE_BASE: /
      DOZZLE_ENABLE_ACTIONS: "false"
      DOZZLE_FILTER: ""
      DOZZLE_HOSTNAME: ""
      DOZZLE_LEVEL: info
      DOZZLE_NO_ANALYTICS: "true"
      DOZZLE_REMOTE_HOST: tcp://dockerproxy-ro:2375
    expose:
      - "8080"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: dozzle
    image: docker.io/amir20/dozzle
    labels:
      homepage.description: Real-time web UI for viewing Docker container logs across the host
      homepage.group: System Monitoring
      homepage.href: https://dozzle.bolabaden.org
      homepage.icon: dozzle.png
      homepage.name: Dozzle
      kuma.dozzle.http.interval: "60"
      kuma.dozzle.http.name: dozzle.beatapostapita.bolabaden.org
      kuma.dozzle.http.url: https://dozzle.bolabaden.org
      traefik.enable: "true"
      traefik.http.routers.dozzle.middlewares: nginx-auth@file
      traefik.http.services.dozzle.loadbalancer.server.port: "8080"
    networks:
      backend: null
      default: null
      publicnet: null
    restart: always
  firecrawl:
    build:
      context: https://github.com/firecrawl/firecrawl.git#main:/apps/api
      dockerfile: Dockerfile
    cpus: 4
    command:
      - node
      - dist/src/harness.js
      - --start-docker
    container_name: firecrawl
    depends_on:
      nuq-postgres:
        condition: service_healthy
        required: true
      playwright-service:
        condition: service_started
        required: true
      rabbitmq:
        condition: service_healthy
        required: true
      redis:
        condition: service_healthy
        required: true
    environment:
      BULL_AUTH_KEY_FILE: /run/secrets/firecrawl-api-key
      ENV: local
      EXTRACT_WORKER_PORT: "3004"
      HOST: 0.0.0.0
      MODEL_EMBEDDING_NAME: ""
      MODEL_NAME: ""
      NUQ_DATABASE_URL: postgres://postgres:postgres@nuq-postgres:5432/postgres
      NUQ_RABBITMQ_URL: amqp://rabbitmq:rabbitmq@rabbitmq:5672
      OLLAMA_BASE_URL: ""
      OPENAI_API_KEY_FILE: /run/secrets/openai-api-key
      OPENAI_BASE_URL: ""
      PLAYWRIGHT_MICROSERVICE_URL: http://playwright-service:3000
      PORT: "3002"
      POSTHOG_API_KEY: ""
      POSTHOG_HOST: ""
      REDIS_RATE_LIMIT_URL: redis://redis:6379
      REDIS_URL: redis://redis:6379
      SEARCHAPI_API_KEY: ""
      SEARXNG_ENDPOINT: https://searxng.bolabaden.org
      SELF_HOSTED_WEBHOOK_URL: ""
      SERPER_API_KEY: ""
      SLACK_WEBHOOK_URL: ""
      SUPABASE_ANON_TOKEN: ""
      SUPABASE_SERVICE_TOKEN: ""
      SUPABASE_URL: ""
      TEST_API_KEY_FILE: /run/secrets/firecrawl-api-key
      USE_DB_AUTHENTICATION: "false"
      WORKER_PORT: "3005"
    expose:
      - "3002"
      - "3004"
      - "3005"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: api
    healthcheck:
      test:
        - CMD-SHELL
        - node -e "require('http').get('http://127.0.0.1:$${FIRECRAWL_INTERNAL_PORT:-3002}/v0/health/liveness', (r) => { let d=''; r.on('data',c=>d+=c); r.on('end',()=>process.exit(r.statusCode===200?0:1)); }).on('error',()=>process.exit(1));" >/dev/null 2>&1 || exit 1
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 1m0s
    image: ghcr.io/firecrawl/firecrawl
    labels:
      deunhealth.restart.on.unhealthy: "true"
      homepage.description: Firecrawl is a tool for crawling and indexing web pages.
      homepage.group: AI
      homepage.href: https://firecrawl.bolabaden.org
      homepage.icon: firecrawl.png
      homepage.name: Firecrawl
      kuma.firecrawl.http.interval: "60"
      kuma.firecrawl.http.name: firecrawl.beatapostapita.bolabaden.org
      kuma.firecrawl.http.url: https://firecrawl.bolabaden.org
      traefik.enable: "true"
      traefik.http.routers.firecrawl.rule: Host(`firecrawl-api.bolabaden.org`) || Host(`firecrawl-api.beatapostapita.bolabaden.org`)
      traefik.http.services.firecrawl.loadbalancer.server.port: "3002"
    mem_reservation: "4294967296"
    networks:
      backend: null
      publicnet: null
    pull_policy: build
    restart: always
    secrets:
      - source: openai-api-key
        target: /run/secrets/openai-api-key
      - source: firecrawl-api-key
        target: /run/secrets/firecrawl-api-key
    ulimits:
      nofile:
        soft: 65535
        hard: 65535
  flaresolverr:
    cpus: 0.6
    container_name: flaresolverr
    environment:
      BROWSER_TIMEOUT: "120000"
      CAPTCHA_SOLVER: none
      HEADLESS: "true"
      HOST: 0.0.0.0
      LOG_HTML: "false"
      LOG_LEVEL: debug
      PGID: "999"
      PORT: "8191"
      PROMETHEUS_ENABLED: "true"
      PROMETHEUS_PORT: "9090"
      PUID: "1001"
      TEST_URL: https://www.google.com
      TZ: America/Chicago
      UMASK: "002"
    expose:
      - "8191"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: flaresolverr
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://127.0.0.1:8191/health > /dev/null 2>&1 || exit 1
      timeout: 15s
      interval: 30s
      retries: 3
      start_period: 1m0s
    image: ghcr.io/flaresolverr/flaresolverr
    labels:
      homepage.description: Headless anti-bot proxy to bypass Cloudflare/JS challenges for indexers and scrapers
      homepage.group: Infrastructure
      homepage.href: https://flaresolverr.bolabaden.org/
      homepage.icon: flaresolverr.png
      homepage.name: Flaresolverr
      kuma.flaresolverr.http.interval: "30"
      kuma.flaresolverr.http.name: flaresolverr.beatapostapita.bolabaden.org
      kuma.flaresolverr.http.url: https://flaresolverr.bolabaden.org
      traefik.enable: "true"
      traefik.http.routers.flaresolverr.middlewares: nginx-auth@file
      traefik.http.routers.flaresolverr.rule: Host(`flaresolverr.bolabaden.org`) || Host(`flaresolverr.beatapostapita.bolabaden.org`)
      traefik.http.services.flaresolverr.loadbalancer.server.port: "8191"
    mem_limit: "1073741824"
    mem_reservation: "67108864"
    networks:
      backend: null
      publicnet: null
    restart: always
  gptr:
    command:
      - /bin/sh
      - -c
      - export OPENAI_API_KEY=$$(cat /run/secrets/openai-api-key 2>/dev/null || echo "") && exec /usr/bin/supervisord -c /etc/supervisor/conf.d/supervisord.conf
    container_name: gptr
    environment:
      ANTHROPIC_API_KEY_FILE: /run/secrets/anthropic-api-key
      BRAVE_API_KEY_FILE: /run/secrets/brave-api-key
      CHOKIDAR_USEPOLLING: "true"
      DEEPSEEK_API_KEY_FILE: /run/secrets/deepseek-api-key
      EXA_API_KEY_FILE: /run/secrets/exa-api-key
      FIRE_CRAWL_API_KEY_FILE: /run/secrets/firecrawl-api-key
      FIRECRAWL_API_KEY_FILE: /run/secrets/firecrawl-api-key
      GEMINI_API_KEY_FILE: /run/secrets/gemini-api-key
      GLAMA_API_KEY_FILE: /run/secrets/glama-api-key
      GROQ_API_KEY_FILE: /run/secrets/groq-api-key
      HF_TOKEN_FILE: /run/secrets/hf-token
      HUGGINGFACE_ACCESS_TOKEN_FILE: /run/secrets/hf-token
      HUGGINGFACE_API_TOKEN_FILE: /run/secrets/hf-token
      LANGCHAIN_API_KEY_FILE: /run/secrets/langchain-api-key
      LANGSMITH_API_KEY_FILE: /run/secrets/langchain-api-key
      LANGSMITH_ENDPOINT: https://api.smith.langchain.com
      LANGSMITH_TRACING: "true"
      LOGGING_LEVEL: DEBUG
      MISTRAL_API_KEY_FILE: /run/secrets/mistral-api-key
      MISTRALAI_API_KEY_FILE: /run/secrets/mistral-api-key
      NEXT_PUBLIC_GA_MEASUREMENT_ID: ""
      NEXT_PUBLIC_GPTR_API_URL: https://gptr.bolabaden.org
      OPENAI_API_KEY_FILE: /run/secrets/openai-api-key
      OPENROUTER_API_KEY_FILE: /run/secrets/openrouter-api-key
      PERPLEXITY_API_KEY_FILE: /run/secrets/perplexity-api-key
      PERPLEXITYAI_API_KEY_FILE: /run/secrets/perplexity-api-key
      REPLICATE_API_KEY_FILE: /run/secrets/replicate-api-key
      REVID_API_KEY_FILE: /run/secrets/revid-api-key
      SAMBANOVA_API_KEY_FILE: /run/secrets/sambanova-api-key
      SEARCH1API_KEY_FILE: /run/secrets/search1api-key
      SERPAPI_API_KEY_FILE: /run/secrets/serpapi-api-key
      TAVILY_API_KEY_FILE: /run/secrets/tavily-api-key
      TOGETHERAI_API_KEY_FILE: /run/secrets/togetherai-api-key
      UPSTAGE_API_KEY_FILE: /run/secrets/upstage-api-key
      UPSTAGEAI_API_KEY_FILE: /run/secrets/upstage-api-key
    expose:
      - "3000"
      - "3001"
      - "8000"
      - "8080"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: gptr
    healthcheck:
      test:
        - CMD-SHELL
        - (wget -qO- http://127.0.0.1:3000 > /dev/null 2>&1 && wget -qO- http://127.0.0.1:8000 > /dev/null 2>&1) || exit 1
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 2m0s
    image: docker.io/bolabaden/ai-researchwizard-aio-fullstack:master
    labels:
      homepage.description: Full-stack AI research and scraping toolkit with Next.js UI is a web scraper and researcher that uses AI to help you find information quickly.
      homepage.group: AI
      homepage.href: https://gptr.bolabaden.org/
      homepage.icon: gptr.png
      homepage.name: AI Research Wizard
      kuma.gptr.http.interval: "30"
      kuma.gptr.http.name: gptr.beatapostapita.bolabaden.org
      kuma.gptr.http.url: https://gptr.bolabaden.org
      traefik.enable: "true"
      traefik.http.routers.gptr-legacy.rule: Host(`gptr.bolabaden.org`) || Host(`gptr.beatapostapita.bolabaden.org`)
      traefik.http.routers.gptr-legacy.service: gptr-legacy@docker
      traefik.http.routers.gptr-mcp.rule: Host(`gptr-mcp.bolabaden.org`) || Host(`gptr-mcp.beatapostapita.bolabaden.org`)
      traefik.http.routers.gptr-mcp.service: gptr-mcp@docker
      traefik.http.routers.gptr-nextjs.rule: Host(`gptr-nextjs.bolabaden.org`) || Host(`gptr-nextjs.beatapostapita.bolabaden.org`)
      traefik.http.routers.gptr-nextjs.service: gptr-nextjs@docker
      traefik.http.services.gptr-legacy.loadbalancer.server.port: "8000"
      traefik.http.services.gptr-mcp.loadbalancer.server.port: "8080"
      traefik.http.services.gptr-nextjs.loadbalancer.server.port: "3000"
    networks:
      backend: null
      publicnet: null
    restart: always
    secrets:
      - source: anthropic-api-key
        target: /run/secrets/anthropic-api-key
      - source: brave-api-key
        target: /run/secrets/brave-api-key
      - source: deepseek-api-key
        target: /run/secrets/deepseek-api-key
      - source: exa-api-key
        target: /run/secrets/exa-api-key
      - source: firecrawl-api-key
        target: /run/secrets/firecrawl-api-key
      - source: gemini-api-key
        target: /run/secrets/gemini-api-key
      - source: glama-api-key
        target: /run/secrets/glama-api-key
      - source: groq-api-key
        target: /run/secrets/groq-api-key
      - source: hf-token
        target: /run/secrets/hf-token
      - source: langchain-api-key
        target: /run/secrets/langchain-api-key
      - source: mistral-api-key
        target: /run/secrets/mistral-api-key
      - source: openai-api-key
        target: /run/secrets/openai-api-key
      - source: openrouter-api-key
        target: /run/secrets/openrouter-api-key
      - source: perplexity-api-key
        target: /run/secrets/perplexity-api-key
      - source: replicate-api-key
        target: /run/secrets/replicate-api-key
      - source: revid-api-key
        target: /run/secrets/revid-api-key
      - source: sambanova-api-key
        target: /run/secrets/sambanova-api-key
      - source: search1api-key
        target: /run/secrets/search1api-key
      - source: serpapi-api-key
        target: /run/secrets/serpapi-api-key
      - source: tavily-api-key
        target: /run/secrets/tavily-api-key
      - source: togetherai-api-key
        target: /run/secrets/togetherai-api-key
      - source: upstage-api-key
        target: /run/secrets/upstage-api-key
    stdin_open: true
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/gptr/logs
        target: /usr/src/app/logs
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/gptr/outputs
        target: /usr/src/app/outputs
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/gptr/reports
        target: /usr/src/app/reports
        bind: {}
  grafana:
    configs:
      - source: grafana.ini
        target: /etc/grafana/grafana.ini
        mode: "0777"
      - source: grafana-datasource.yaml
        target: /etc/grafana/provisioning/datasources/grafana-datasource.yaml
        mode: "0777"
      - source: grafana-dashboard.yaml
        target: /etc/grafana/provisioning/dashboards/grafana-dashboard.yaml
        mode: "0777"
      - source: grafana-alerting.yaml
        target: /etc/grafana/provisioning/alerting/grafana-alerting.yaml
        mode: "0777"
      - source: grafana-notifications.yaml
        target: /etc/grafana/provisioning/notifiers/grafana-notifications.yaml
        mode: "0777"
      - source: grafana-plugins.yaml
        target: /etc/grafana/provisioning/plugins/grafana-plugins.yaml
        mode: "0777"
      - source: node-exporter-dashboard.json
        target: /var/lib/grafana/dashboards/system/node-exporter-dashboard.json
        mode: "0777"
      - source: cadvisor-dashboard.json
        target: /var/lib/grafana/dashboards/infrastructure/cadvisor-dashboard.json
        mode: "0777"
      - source: blackbox-dashboard.json
        target: /var/lib/grafana/dashboards/network/blackbox-dashboard.json
        mode: "0777"
      - source: traefik-dashboard.json
        target: /var/lib/grafana/dashboards/infrastructure/traefik-dashboard.json
        mode: "0777"
      - source: crowdsec-dashboard.json
        target: /var/lib/grafana/dashboards/infrastructure/crowdsec-dashboard.json
        mode: "0777"
      - source: flaresolverr-dashboard.json
        target: /var/lib/grafana/dashboards/apps/flaresolverr-dashboard.json
        mode: "0777"
      - source: victoriametrics-dashboard.json
        target: /var/lib/grafana/dashboards/infrastructure/victoriametrics-dashboard.json
        mode: "0777"
      - source: crowdsec-profiles.yaml
        target: /etc/grafana/provisioning/crowdsec-profiles.yaml
        mode: "0777"
    container_name: grafana
    depends_on:
      prometheus:
        condition: service_healthy
        required: true
    environment:
      GF_DATABASE_CONN_MAX_LIFETIME: "14400"
      GF_DATABASE_MAX_IDLE_CONN: "2"
      GF_DATABASE_MAX_OPEN_CONN: "0"
      GF_FEATURE_TOGGLES_ENABLE: ""
      GF_LOG_LEVEL: info
      GF_PATHS_PROVISIONING: /etc/grafana/provisioning
      GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS: victoriametrics-datasource
      GF_PLUGINS_PREINSTALL: grafana-piechart-panel,grafana-worldmap-panel,natel-discrete-panel,flant-statusmap-panel,vonage-status-panel,michaeldmoore-multistat-panel,grafana-polystat-panel,marcusolsson-dynamictext-panel,yesoreyeram-boomtable-panel,grafana-clock-panel,grafana-simple-json-datasource,btplc-status-dot-panel,camptocamp-prometheus-alertmanager-datasource
      GF_SECRETS_MANAGER_SECRET_KEY__FILE: /run/secrets/grafana-secret-key
      GF_SECURITY_ADMIN_PASSWORD__FILE: /run/secrets/grafana-admin-password
      GF_SECURITY_ADMIN_USER: brunner56
      GF_SECURITY_AVAILABLE_ENCRYPTION_PROVIDERS: ""
      GF_SECURITY_CONTENT_TYPE_PROTECTION: "true"
      GF_SECURITY_COOKIE_SAMESITE: lax
      GF_SECURITY_COOKIE_SECURE: "true"
      GF_SECURITY_ENCRYPTION_PROVIDER: ""
      GF_SECURITY_SECRET_KEY__FILE: /run/secrets/grafana-secret-key
      GF_SECURITY_STRICT_TRANSPORT_SECURITY: "true"
      GF_SECURITY_X_CONTENT_TYPE_OPTIONS: nosniff
      GF_SECURITY_X_XSS_PROTECTION: "true"
      GF_SERVER_DOMAIN: grafana.bolabaden.org
      GF_SERVER_ROOT_URL: https://grafana.bolabaden.org
      GF_SERVER_SERVE_FROM_SUB_PATH: "false"
      GF_SMTP_ENABLED: "false"
      GF_SMTP_FROM_ADDRESS: contact@bolabaden.org
      GF_SMTP_FROM_NAME: Grafana
      GF_SMTP_HOST: localhost:587
      GF_SMTP_PASSWORD: ""
      GF_SMTP_USER: ""
      GF_USERS_ALLOW_ORG_CREATE: "false"
      GF_USERS_ALLOW_SIGN_UP: "false"
      GF_USERS_AUTO_ASSIGN_ORG: "true"
      GF_USERS_AUTO_ASSIGN_ORG_ROLE: Viewer
    expose:
      - "3000"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: grafana
    healthcheck:
      test:
        - CMD-SHELL
        - wget --no-verbose --tries=1 --spider http://127.0.0.1:3000/api/health || exit 1
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 40s
    image: docker.io/grafana/grafana
    labels:
      homepage.description: Grafana is an open-source platform for monitoring and observability.
      homepage.group: Infrastructure
      homepage.href: https://grafana.bolabaden.org
      homepage.icon: grafana.png
      homepage.name: Grafana
      kuma.grafana.http.interval: "60"
      kuma.grafana.http.name: grafana.beatapostapita.bolabaden.org
      kuma.grafana.http.url: https://grafana.bolabaden.org
      prometheus.io/path: metrics
      prometheus.io/port: "3000"
      prometheus.io/scrape: "true"
      traefik.enable: "true"
      traefik.http.routers.grafana.rule: Host(`grafana.bolabaden.org`) || Host(`grafana.beatapostapita.bolabaden.org`)
      traefik.http.services.grafana.loadbalancer.server.port: "3000"
    networks:
      backend: null
      publicnet: null
    restart: always
    secrets:
      - source: grafana-admin-password
        target: /run/secrets/grafana-admin-password
      - source: grafana-secret-key
        target: /run/secrets/grafana-secret-key
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/grafana/dashboards
        target: /var/lib/grafana/dashboards
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/grafana/provisioning/alerting
        target: /etc/grafana/provisioning/alerting
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/grafana/provisioning/dashboards
        target: /etc/grafana/provisioning/dashboards
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/grafana/provisioning/datasources
        target: /etc/grafana/provisioning/datasources
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/grafana/provisioning/plugins
        target: /etc/grafana/provisioning/plugins
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/grafana/provisioning/notifiers
        target: /etc/grafana/provisioning/notifiers
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/grafana/logs
        target: /data/log
        bind: {}
  headscale:
    container_name: headscale
    depends_on:
      headscale-server:
        condition: service_started
        required: true
    dns:
      - 1.1.1.1
      - 1.0.0.1
      - 2606:4700:4700::1111
      - 2606:4700:4700::1001
      - 9.9.9.9
      - 2620:fe::fe
      - 8.8.8.8
      - 2001:4860:4860::8888
      - 2001:4860:4860::8844
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: headscale
    image: ghcr.io/gurucomputing/headscale-ui
    labels:
      homepage.description: Headscale UI is a web interface for Headscale, an open source implementation of Tailscale's Admin control panel.
      homepage.group: Networking
      homepage.href: https://headscale.bolabaden.org/
      homepage.icon: headscale.png
      homepage.name: Headscale UI
      traefik.enable: "true"
      traefik.http.routers.headscale.rule: (Host(`headscale.bolabaden.org`) || Host(`headscale.beatapostapita.bolabaden.org`)) && (PathPrefix(`/web`) || PathPrefix(`/web/users.html`) || PathPrefix(`/web/groups.html`) || PathPrefix(`/web/devices.html`) || PathPrefix(`/web/settings.html`))
      traefik.http.routers.headscale.service: headscale
      traefik.http.services.headscale.loadbalancer.server.port: "8080"
    networks:
      backend: null
      publicnet: null
    restart: always
  headscale-server:
    command:
      - serve
      - --config
      - /etc/headscale/config.yaml
    configs:
      - source: headscale-config
        target: /etc/headscale/config.yaml
        mode: "0644"
    container_name: headscale-server
    expose:
      - "3478"
      - "8081"
      - "8080"
      - "50443"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: headscale-server
    image: docker.io/headscale/headscale
    labels:
      traefik.enable: "true"
      traefik.http.middlewares.headscale-admin-redirect.redirectRegex.permanent: "false"
      traefik.http.middlewares.headscale-admin-redirect.redirectRegex.regex: ^https?://headscale(-server)?\.(bolabaden.org|beatapostapita\.bolabaden.org)/admin(.*)$$
      traefik.http.middlewares.headscale-admin-redirect.redirectRegex.replacement: https://headscale.bolabaden.org/web$$3
      traefik.http.middlewares.headscale-server-redirect.redirectRegex.permanent: "false"
      traefik.http.middlewares.headscale-server-redirect.redirectRegex.regex: ^https?://headscale-server\.((?:bolabaden.org|beatapostapita\.bolabaden.org))(.*)$$
      traefik.http.middlewares.headscale-server-redirect.redirectRegex.replacement: https://headscale.$$1$$2
      traefik.http.routers.headscale-metrics.rule: (Host(`headscale.bolabaden.org`) || Host(`headscale.beatapostapita.bolabaden.org`)) && PathPrefix(`/metrics`)
      traefik.http.routers.headscale-metrics.service: headscale-metrics@docker
      traefik.http.routers.headscale-server.middlewares: headscale-admin-redirect@docker,headscale-server-redirect@docker
      traefik.http.routers.headscale-server.rule: Host(`headscale-server.bolabaden.org`) || Host(`headscale-server.beatapostapita.bolabaden.org`) || Host(`headscale.bolabaden.org`) || Host(`headscale.beatapostapita.bolabaden.org`)
      traefik.http.routers.headscale-server.service: headscale-server@docker
      traefik.http.services.headscale-metrics.loadbalancer.server.port: "8080"
      traefik.http.services.headscale-server.loadbalancer.server.port: "8081"
    networks:
      backend: null
      publicnet: null
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/certs/private/bolabaden.org.key
        target: /var/lib/headscale/private.key
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/headscale/config
        target: /etc/headscale
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/headscale/lib
        target: /var/lib/headscale
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/headscale/run
        target: /var/run/headscale
        bind: {}
  homepage:
    cpus: 0.25
    configs:
      - source: gethomepage-custom.css
        target: /app/config/custom.css
        mode: "0777"
      - source: gethomepage-custom.js
        target: /app/config/custom.js
        mode: "0777"
      - source: gethomepage-docker.yaml
        target: /app/config/docker.yaml
        mode: "0777"
      - source: gethomepage-widgets.yaml
        target: /app/config/widgets.yaml
        mode: "0777"
      - source: gethomepage-settings.yaml
        target: /app/config/settings.yaml
        mode: "0777"
      - source: gethomepage-bookmarks.yaml
        target: /app/config/bookmarks.yaml
        mode: "0777"
    container_name: homepage
    depends_on:
      dockerproxy-ro:
        condition: service_healthy
        required: true
    environment:
      DOCKER_HOST: tcp://dockerproxy-ro:2375
      HOMEPAGE_ALLOWED_HOSTS: '*'
      HOMEPAGE_CUSTOM_CSS: /app/config/custom.css
      HOMEPAGE_CUSTOM_JS: /app/config/custom.js
      HOMEPAGE_VAR_HEADER_STYLE: glass
      HOMEPAGE_VAR_SEARCH_PROVIDER: duckduckgo
      HOMEPAGE_VAR_THEME: dark
      HOMEPAGE_VAR_TITLE: Bolabaden
      HOMEPAGE_VAR_WEATHER_CITY: Iowa City
      HOMEPAGE_VAR_WEATHER_LAT: "41.661129"
      HOMEPAGE_VAR_WEATHER_LONG: "-91.5302"
      HOMEPAGE_VAR_WEATHER_UNIT: fahrenheit
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: homepage
    healthcheck:
      test:
        - CMD-SHELL
        - wget -qO- http://127.0.0.1:3000 > /dev/null 2>&1 || exit 1
      timeout: 15s
      interval: 30s
      retries: 3
      start_period: 30s
    image: ghcr.io/gethomepage/homepage
    labels:
      deunhealth.restart.on.unhealthy: "true"
      kuma.homepage.http.interval: "30"
      kuma.homepage.http.name: homepage.beatapostapita.bolabaden.org
      kuma.homepage.http.url: https://homepage.bolabaden.org
      traefik.enable: "true"
      traefik.http.services.homepage.loadbalancer.server.port: "3000"
    mem_limit: "1073741824"
    mem_reservation: "134217728"
    networks:
      backend: null
      default: null
      publicnet: null
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/homepage
        target: /app/config
        bind: {}
  init_prometheus:
    container_name: init_prometheus
    entrypoint:
      - /bin/sh
      - -c
      - chown -R 65534:65534 /prometheus
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: init_prometheus
    image: docker.io/prom/prometheus
    networks:
      default: null
    restart: "no"
    user: root
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/prometheus/data
        target: /prometheus
        bind: {}
  init_victoriametrics:
    container_name: init_victoriametrics
    entrypoint:
      - /bin/sh
      - -c
      - chown -R 65534:65534 /storage && chmod -R 755 /storage
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: init_victoriametrics
    image: docker.io/victoriametrics/victoria-metrics:latest
    networks:
      default: null
    restart: "no"
    user: root
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/victoriametrics
        target: /storage
        bind: {}
  ip-checker-warp:
    command:
      - /bin/sh
      - -c
      - |
        apk add --no-cache curl ipcalc
        while true; do echo "$$(date): $$(curl -s --max-time 4 ifconfig.me)"; sleep 5; done
    container_name: ip-checker-warp
    healthcheck:
      test:
        - CMD-SHELL
        - exit 0
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 20s
    image: docker.io/alpine
    networks:
      warp-nat-net: null
    restart: always
  jackett:
    cpus: 1
    configs:
      - source: jackett-serverconfig.json
        target: /defaults/ServerConfig.json
        mode: "0644"
      - source: jackett-indexer-blacklist.txt
        target: /indexer-blacklist.txt
        mode: "0644"
      - source: jackett-init.sh
        target: /custom-cont-init.d/99-remove-slow-indexers
        mode: "0755"
    container_name: jackett
    depends_on:
      flaresolverr:
        condition: service_healthy
        required: true
    environment:
      AUTO_UPDATE: "true"
      JACKETT_API_KEY: nnx0n84pcj7umynyd2pbid2nl1zzuemz
      PGID: "999"
      PUID: "1001"
      RUN_OPTS: ""
      TZ: America/Chicago
      UMASK: "002"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: jackett
    healthcheck:
      test:
        - CMD-SHELL
        - curl -fs http://127.0.0.1:9117/api/v2.0/indexers/all/results/torznab?t=indexers&apikey=nnx0n84pcj7umynyd2pbid2nl1zzuemz > /dev/null 2>&1 || exit 1
      timeout: 10s
      interval: 30s
      start_period: 1m0s
    image: lscr.io/linuxserver/jackett
    labels:
      kuma.jackett.http.interval: "30"
      kuma.jackett.http.name: jackett.beatapostapita.bolabaden.org
      kuma.jackett.http.url: https://jackett.bolabaden.org/api/v2.0/indexers/all/results/torznab?t=indexers&apikey=nnx0n84pcj7umynyd2pbid2nl1zzuemz
      traefik.enable: "true"
      traefik.http.routers.jackett.rule: Host(`jackett.bolabaden.org`) || Host(`jackett.beatapostapita.bolabaden.org`)
      traefik.http.services.jackett.loadbalancer.server.port: "9117"
    mem_limit: "1073741824"
    mem_reservation: "134217728"
    networks:
      publicnet: {}
      warp-nat-net:
        gw_priority: 1
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/jackett/config
        target: /config
        bind: {}
      - type: bind
        source: /mnt/remote/blackhole
        target: /blackhole
        bind: {}
  litellm-postgres:
    container_name: litellm-postgres
    environment:
      POSTGRES_DB: litellm
      POSTGRES_PASSWORD: litellm
      POSTGRES_USER: litellm
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: litellm-postgres
    healthcheck:
      test:
        - CMD-SHELL
        - pg_isready -h localhost -U $$POSTGRES_USER -d $$POSTGRES_DB
      timeout: 5s
      interval: 5s
      retries: 3
    image: docker.io/postgres:16-alpine
    networks:
      backend: null
      publicnet: null
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/litellm/pgdata
        target: /var/lib/postgresql/data
        bind: {}
  logrotate-traefik:
    build:
      context: /home/ubuntu/my-media-stack/projects/reverse_proxy/logrotate-traefik
      dockerfile: Dockerfile
    cpus: 0.1
    container_name: logrotate-traefik
    environment:
      TZ: America/Chicago
    image: docker.io/bolabaden/logrotate-traefik
    mem_limit: "67108864"
    mem_reservation: "8388608"
    network_mode: none
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/traefik/logs
        target: /var/log/traefik
        bind: {}
  loki:
    command:
      - -config.file=/etc/loki/config.yaml
    configs:
      - source: loki.yaml
        target: /etc/loki/config.yaml
        mode: "0777"
    container_name: loki
    expose:
      - "3100"
    hostname: loki
    image: docker.io/grafana/loki
    networks:
      backend: null
      publicnet: null
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/loki/data
        target: /loki
        bind: {}
  mcpo:
    command:
      - --api-key
      - sk_ezKNDnp6_noyUfQ7L_wzEI3IpwWwIPogSX5WaWZWYI
      - --host
      - 0.0.0.0
      - --port
      - "8000"
      - --cors-allow-origins
      - '*'
      - --config
      - /app/config/mcp_servers.json
    configs:
      - source: mcp_servers.json
        target: /app/config/mcp_servers.json
        mode: "0444"
    container_name: mcpo
    environment:
      MCPO_API_KEY: sk_ezKNDnp6_noyUfQ7L_wzEI3IpwWwIPogSX5WaWZWYI
    expose:
      - "8000"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: mcpo
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://127.0.0.1:8000/openapi.json
      timeout: 10s
      start_period: 40s
    image: ghcr.io/open-webui/mcpo:main
    labels:
      homepage.description: MCP Orchestrator exposing model/context tools over the MCP protocol
      homepage.group: MCPO
      homepage.href: https://mcpo.bolabaden.org/
      homepage.icon: mcpo.png
      homepage.name: MCPO
      kuma.mcpo.http.interval: "30"
      kuma.mcpo.http.name: mcpo.beatapostapita.bolabaden.org
      kuma.mcpo.http.url: https://mcpo.bolabaden.org
      traefik.enable: "true"
      traefik.http.routers.mcpo.middlewares: nginx-auth@file
      traefik.http.routers.mcpo.rule: Host(`mcpo.bolabaden.org`) || Host(`mcpo.beatapostapita.bolabaden.org`)
      traefik.http.services.mcpo.loadbalancer.server.port: "8000"
    networks:
      backend: null
      publicnet: null
    restart: always
    working_dir: /app
  mongodb:
    container_name: mongodb
    expose:
      - "27017"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: mongodb
    healthcheck:
      test:
        - CMD-SHELL
        - mongosh 127.0.0.1:27017/test --quiet --eval 'db.runCommand("ping").ok' > /dev/null 2>&1 || exit 1
      timeout: 10s
      interval: 10s
      retries: 5
      start_period: 40s
    image: docker.io/mongo
    labels:
      traefik.enable: "true"
      traefik.tcp.routers.mongodb.rule: HostSNI(`mongodb.bolabaden.org`) || HostSNI(`mongodb.beatapostapita.bolabaden.org`)
      traefik.tcp.routers.mongodb.service: mongodb@docker
      traefik.tcp.routers.mongodb.tls.domains[0].main: bolabaden.org
      traefik.tcp.routers.mongodb.tls.domains[0].sans: '*.bolabaden.org,beatapostapita.bolabaden.org'
      traefik.tcp.routers.mongodb.tls.passthrough: "true"
      traefik.tcp.services.mongodb.loadbalancer.server.port: "27017"
      traefik.tcp.services.mongodb.loadbalancer.server.tls: "true"
    networks:
      backend: null
      publicnet: null
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/mongodb/data
        target: /data/db
        bind: {}
  nginx-traefik-extensions:
    cpus: 1
    configs:
      - source: nginx-traefik-extensions.conf
        target: /etc/nginx/nginx.conf
        mode: "0444"
    container_name: nginx-traefik-extensions
    environment:
      NGINX_ACCESS_LOG: /dev/stdout
      NGINX_ERROR_LOG: /dev/stderr
      NGINX_LOG_LEVEL: debug
      TZ: America/Chicago
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: nginx-traefik-extensions
    healthcheck:
      test:
        - CMD-SHELL
        - wget -qO- http://127.0.0.1:80/health > /dev/null 2>&1 || exit 1
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 20s
    image: docker.io/nginx:alpine
    labels:
      traefik.http.middlewares.nginx-auth.forwardAuth.address: http://nginx-traefik-extensions:80/auth"
      traefik.http.middlewares.nginx-auth.forwardAuth.authResponseHeaders: '["X-Auth-Method", "X-Auth-Passed", "X-Middleware-Name"]'
      traefik.http.middlewares.nginx-auth.forwardAuth.trustForwardHeader: "true"
    mem_limit: "1073741824"
    mem_reservation: "6291456"
    networks:
      backend: null
      nginx_net: null
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/traefik/nginx-middlewares/auth
        target: /etc/nginx/auth
        read_only: true
        bind: {}
  node_exporter:
    command:
      - --path.procfs=/host/proc
      - --path.rootfs=/rootfs
      - --path.sysfs=/host/sys
      - --collector.cpu.info
      - --collector.diskstats.device-exclude=^(ram|loop|fd|(h|s|v)d[a-z]|nvme\d+n\d+p)\d+$$
      - --collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)
      - --collector.interrupts
      - --collector.processes
      - --collector.systemd
      - --collector.systemd.unit-include=.*
      - --collector.systemd.unit-exclude=.+\.(automount|device|mount|scope|slice)
      - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$$
    container_name: node_exporter
    environment:
      DBUS_SESSION_BUS_ADDRESS: unix:path=/var/run/dbus/system_bus_socket
    expose:
      - "9100"
    hostname: node_exporter
    image: docker.io/prom/node-exporter
    labels:
      prometheus.io/path: metrics
      prometheus.io/port: "9100"
      prometheus.io/scrape: "true"
    networks:
      backend: null
    restart: always
    volumes:
      - type: bind
        source: /proc
        target: /host/proc
        read_only: true
        bind: {}
      - type: bind
        source: /sys
        target: /host/sys
        read_only: true
        bind: {}
      - type: bind
        source: /
        target: /rootfs
        read_only: true
        bind: {}
      - type: bind
        source: /run/dbus/system_bus_socket
        target: /var/run/dbus/system_bus_socket
        read_only: true
        bind: {}
      - type: bind
        source: /etc/machine-id
        target: /etc/machine-id
        read_only: true
        bind: {}
      - type: bind
        source: /run/udev/data
        target: /run/udev/data
        read_only: true
        bind: {}
  nuq-postgres:
    build:
      context: https://github.com/firecrawl/firecrawl.git#main:/apps/nuq-postgres
      dockerfile: Dockerfile
    container_name: nuq-postgres
    environment:
      POSTGRES_DB: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_USER: postgres
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: nuq-postgres
    healthcheck:
      test:
        - CMD-SHELL
        - pg_isready -U $$POSTGRES_USER -d $$POSTGRES_DB >/dev/null 2>&1 || exit 1
      timeout: 5s
      interval: 10s
      retries: 5
    labels:
      deunhealth.restart.on.unhealthy: "true"
      kuma.nuq-postgres.http.interval: "60"
      kuma.nuq-postgres.http.name: nuq-postgres.beatapostapita.bolabaden.org
      kuma.nuq-postgres.http.url: https://nuq-postgres.bolabaden.org
    networks:
      backend: null
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/nuq-postgres/data
        target: /var/lib/postgresql/data
        bind: {}
  playwright-service:
    build:
      context: https://github.com/firecrawl/firecrawl.git#main:/apps/playwright-service-ts
      dockerfile: Dockerfile
    container_name: playwright-service
    environment:
      BLOCK_MEDIA: "false"
      PORT: "3000"
      PROXY_PASSWORD: ""
      PROXY_SERVER: ""
      PROXY_USERNAME: ""
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: playwright-service
    healthcheck:
      test:
        - CMD-SHELL
        - apt update && apt install curl -y && apt autoremove -y && curl -f http://127.0.0.1:3000/health >/dev/null 2>&1 || exit 1
      timeout: 10s
      interval: 2s
      retries: 10
    image: ghcr.io/firecrawl/playwright-service
    labels:
      kuma.playwright-service.http.interval: "60"
      kuma.playwright-service.http.name: playwright-service.beatapostapita.bolabaden.org
      kuma.playwright-service.http.url: https://playwright-service.bolabaden.org
    networks:
      backend: null
    restart: always
  portainer:
    container_name: portainer
    expose:
      - "8000"
      - "9000"
      - "9443"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: portainer
    image: docker.io/portainer/portainer-ce
    labels:
      kuma.portainer.http.interval: "60"
      kuma.portainer.http.name: portainer.beatapostapita.bolabaden.org
      kuma.portainer.http.url: https://portainer.bolabaden.org
      traefik.enable: "true"
      traefik.http.routers.portainer.middlewares: nginx-auth@file
      traefik.http.routers.portainer.service: portainer@docker
      traefik.http.services.portainer.loadbalancer.server.port: "9000"
    networks:
      backend: null
      publicnet: null
    ports:
      - mode: ingress
        host_ip: 127.0.0.1
        target: 9443
        published: "9443"
        protocol: tcp
    restart: always
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/portainer/data
        target: /data
        bind: {}
  prometheus:
    command:
      - --config.file=/etc/prometheus/prometheus.yml
      - --log.format=logfmt
      - --log.level=info
      - --query.max-concurrency=20
      - --query.max-samples=50000000
      - --query.timeout=2m
      - --storage.tsdb.path=/prometheus
      - --storage.tsdb.retention.size=0
      - --storage.tsdb.retention.time=15d
      - --web.console.libraries=/usr/share/prometheus/console_libraries
      - --web.console.templates=/usr/share/prometheus/consoles
      - --web.enable-admin-api
      - --web.enable-lifecycle
      - --web.max-connections=512
    configs:
      - source: prometheus.yml
        target: /etc/prometheus/prometheus.yml
        mode: "0444"
      - source: alert.rules
        target: /etc/prometheus/alert.rules
        mode: "0444"
    container_name: prometheus
    depends_on:
      init_prometheus:
        condition: service_completed_successfully
        required: true
      victoriametrics:
        condition: service_healthy
        required: true
    environment:
      PROMETHEUS_QUERY_MAX_CONCURRENCY: "20"
      PROMETHEUS_QUERY_MAX_SAMPLES: "50000000"
      PROMETHEUS_QUERY_TIMEOUT: 2m
      PROMETHEUS_RETENTION_SIZE: "0"
      PROMETHEUS_RETENTION_TIME: 15d
      PROMETHEUS_WEB_MAX_CONNECTIONS: "512"
    expose:
      - "9090"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: prometheus
    healthcheck:
      test:
        - CMD-SHELL
        - wget --no-verbose --tries=1 --spider http://127.0.0.1:9090/-/healthy || exit 1
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 40s
    image: docker.io/prom/prometheus
    labels:
      homepage.description: Prometheus is an open-source monitoring system with a dimensional data model, flexible query language, efficient time series database, and modern alerting approach.
      homepage.group: Infrastructure
      homepage.href: https://prometheus.bolabaden.org
      homepage.icon: prometheus.png
      homepage.name: Prometheus
      homepage.widget.type: prometheus
      homepage.widget.url: http://prometheus:9090
      kuma.prometheus.http.interval: "60"
      kuma.prometheus.http.name: prometheus.beatapostapita.bolabaden.org
      kuma.prometheus.http.url: https://prometheus.bolabaden.org
      prometheus.io/path: /metrics
      prometheus.io/port: "9090"
      prometheus.io/scrape: "true"
      traefik.enable: "true"
      traefik.http.routers.prometheus.middlewares: nginx-auth@file
      traefik.http.routers.prometheus.rule: Host(`prometheus.bolabaden.org`) || Host(`prometheus.beatapostapita.bolabaden.org`)
      traefik.http.services.prometheus.loadbalancer.server.port: "9090"
    networks:
      backend: null
      publicnet: null
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/prometheus/data
        target: /prometheus
        bind: {}
  promtail:
    command:
      - -config.file=/etc/promtail/config.yaml
    configs:
      - source: promtail.yaml-override
        target: /etc/promtail/config.yaml
        mode: "0777"
    container_name: promtail
    depends_on:
      dockerproxy-ro:
        condition: service_started
        required: true
      loki:
        condition: service_started
        required: true
    environment:
      DOCKER_HOST: tcp://dockerproxy-ro:9323
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: promtail
    image: docker.io/grafana/promtail
    networks:
      backend: null
      default: {}
      publicnet: null
    restart: always
    volumes:
      - type: bind
        source: /var/lib/docker/containers
        target: /var/lib/docker/containers
        read_only: true
        bind: {}
      - type: bind
        source: /var/log
        target: /var/log
        read_only: true
        bind: {}
  prowlarr:
    cpus: 2
    container_name: prowlarr
    environment:
      PGID: "999"
      PUID: "1001"
      TZ: America/Chicago
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: prowlarr
    healthcheck:
      test:
        - CMD-SHELL
        - 'curl -fs -H "Authorization: Bearer a26e988eeab54322ada14b32e54eb1d6" http://127.0.0.1:9696/api/v1/system/status || exit 1'
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 2m0s
    image: lscr.io/linuxserver/prowlarr
    labels:
      kuma.prowlarr.http.interval: "60"
      kuma.prowlarr.http.name: prowlarr.beatapostapita.bolabaden.org
      kuma.prowlarr.http.url: https://prowlarr.bolabaden.org/api/v1/system/status
      traefik.enable: "true"
      traefik.http.routers.prowlarr.rule: Host(`prowlarr.bolabaden.org`) || Host(`prowlarr.beatapostapita.bolabaden.org`)
      traefik.http.services.prowlarr.loadbalancer.server.port: "9696"
    mem_limit: "6442450944"
    mem_reservation: "134217728"
    networks:
      publicnet: {}
      warp-nat-net:
        gw_priority: 1
    ports:
      - mode: ingress
        host_ip: 127.0.0.1
        target: 9696
        published: "9696"
        protocol: tcp
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/prowlarr/config
        target: /config
        bind: {}
  rabbitmq:
    command:
      - /bin/bash
      - /rabbitmq-init.sh
    container_name: rabbitmq
    environment:
      RABBITMQ_DEFAULT_PASS: rabbitmq
      RABBITMQ_DEFAULT_USER: rabbitmq
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: rabbitmq
    healthcheck:
      test:
        - CMD-SHELL
        - rabbitmq-diagnostics -q check_running >/dev/null 2>&1 || exit 1
      timeout: 5s
      interval: 10s
      retries: 5
      start_period: 30s
    image: docker.io/rabbitmq:3-management
    labels:
      deunhealth.restart.on.unhealthy: "true"
      kuma.rabbitmq.http.interval: "60"
      kuma.rabbitmq.http.name: rabbitmq.beatapostapita.bolabaden.org
      kuma.rabbitmq.http.url: https://rabbitmq.bolabaden.org
    networks:
      backend: null
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/compose/rabbitmq-init.sh
        target: /rabbitmq-init.sh
        read_only: true
        bind: {}
  rclone:
    cap_add:
      - SYS_ADMIN
    cpus: 1
    command:
      - rcd
      - --rc-web-gui
      - --rc-web-gui-no-open-browser
      - --rc-addr=:5572
      - --log-level=INFO
      - --rc-no-auth
      - --config=/config/rclone/rclone.conf
      - --cache-dir=/.cache/rclone
    configs:
      - source: rclone.conf
        target: /config/rclone/rclone.conf
        mode: "0400"
      - source: fuse.conf
        target: /etc/fuse.conf
        mode: "0400"
    container_name: rclone
    devices:
      - source: /dev/fuse
        target: /dev/fuse
        permissions: rwm
    expose:
      - "5572"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: rclone
    healthcheck:
      test:
        - CMD-SHELL
        - mountpoint -q /mnt/remote
      timeout: 3s
      interval: 5s
      retries: 2
      start_period: 15s
    image: docker.io/rclone/rclone
    labels:
      homepage.description: A web interface for Rclone.
      homepage.group: Cloud
      homepage.href: https://rclone.bolabaden.org/
      homepage.icon: rclone.png
      homepage.name: Rclone
      homepage.weight: "0"
      kuma.rclone.http.interval: "60"
      kuma.rclone.http.name: rclone.beatapostapita.bolabaden.org
      kuma.rclone.http.url: https://rclone.bolabaden.org
      traefik.enable: "true"
      traefik.http.routers.rclone.middlewares: nginx-auth@file
      traefik.http.routers.rclone.rule: Host(`rclone.bolabaden.org`) || Host(`rclone.beatapostapita.bolabaden.org`)
      traefik.http.services.rclone.loadbalancer.server.port: "5572"
    mem_limit: "1073741824"
    mem_reservation: "62914560"
    networks:
      backend: null
      publicnet: null
    restart: always
    security_opt:
      - apparmor:unconfined
    volumes:
      - type: bind
        source: /
        target: /hostfs
        bind:
          propagation: rshared
      - type: bind
        source: /etc/group
        target: /etc/group
        read_only: true
        bind: {}
      - type: bind
        source: /etc/passwd
        target: /etc/passwd
        read_only: true
        bind: {}
      - type: bind
        source: /mnt/remote
        target: /mnt/remote
        bind:
          propagation: rshared
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/rclone/.cache
        target: /.cache
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/rclone/config/rclone
        target: /config/rclone
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/rclonefm/js
        target: /var/lib/rclonefm/js
        bind: {}
  rclone-init:
    configs:
      - source: rclone-mounts.json
        target: /app/mounts.json
        mode: "0400"
    container_name: rclone-init
    depends_on:
      rclone:
        condition: service_started
        required: true
    environment:
      RCLONE_CONFIG_PATH: /config/rclone/rclone.conf
      RCLONE_PASSWORD: ""
      RCLONE_PORT: "5572"
      RCLONE_USERNAME: ""
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: rclone-init
    healthcheck:
      test:
        - CMD-SHELL
        - mountpoint -q /mnt/remote
      timeout: 3s
      interval: 5s
      retries: 2
      start_period: 15s
    image: ghcr.io/coanghel/rclone-docker-automount/rclone-init
    networks:
      backend: null
    restart: always
    volumes:
      - type: bind
        source: /mnt/remote
        target: /mnt/remote
        bind:
          propagation: rshared
  redis:
    cpus: 0.5
    command:
      - sh
      - -c
      - |
        sysctl vm.overcommit_memory=1 &> /dev/null &&
        redis-server
        --appendonly yes
        --save 60 1
        --bind 0.0.0.0
        --port 6379
        --requirepass h4L0m4St3R327
    container_name: redis
    environment:
      REDIS_DATABASE: "0"
      REDIS_HOST: redis
      REDIS_PASSWORD: h4L0m4St3R327
      REDIS_PORT: "6379"
      REDIS_USERNAME: brunner56
    expose:
      - "6379"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: redis
    healthcheck:
      test:
        - CMD-SHELL
        - redis-cli ping > /dev/null 2>&1 || exit 1
      timeout: 5s
      interval: 10s
    image: docker.io/redis:alpine
    labels:
      traefik.enable: "true"
      traefik.tcp.routers.redis.rule: HostSNI(`redis.bolabaden.org`) || HostSNI(`redis.beatapostapita.bolabaden.org`)
      traefik.tcp.routers.redis.service: redis@docker
      traefik.tcp.routers.redis.tls.domains[0].main: bolabaden.org
      traefik.tcp.routers.redis.tls.domains[0].sans: '*.bolabaden.org,beatapostapita.bolabaden.org'
      traefik.tcp.routers.redis.tls.passthrough: "true"
      traefik.tcp.services.redis.loadbalancer.server.port: "6379"
      traefik.tcp.services.redis.loadbalancer.server.tls: "true"
    mem_limit: "4294967296"
    mem_reservation: "209715200"
    networks:
      backend: null
      publicnet: null
    ports:
      - mode: ingress
        target: 6379
        published: "6379"
        protocol: tcp
    privileged: true
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/redis
        target: /data
        bind: {}
  searxng:
    container_name: searxng
    environment:
      SEARXNG_BASE_URL: http://searxng:8080
      SEARXNG_SECRET: D3sBikzyPHooYFgUPUS7kVwbkpglmQa
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: searxng
    healthcheck:
      test:
        - CMD-SHELL
        - wget --no-verbose --tries=1 --spider http://127.0.0.1:8080/ || exit 1
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 30s
    image: docker.io/searxng/searxng
    labels:
      deunhealth.restart.on.unhealthy: "true"
      homepage.description: Privacy-focused metasearch that aggregates results from many sources without tracking
      homepage.group: Search
      homepage.href: https://searxng.bolabaden.org/
      homepage.icon: searxng.png
      homepage.name: SearxNG
      kuma.searxng.http.interval: "30"
      kuma.searxng.http.name: searxng.beatapostapita.bolabaden.org
      kuma.searxng.http.url: https://searxng.bolabaden.org
      traefik.enable: "true"
      traefik.http.services.searxng.loadbalancer.server.port: "8080"
    networks:
      backend: null
      publicnet: null
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/searxng/config
        target: /etc/searxng
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/searxng/data
        target: /var/cache/searxng
        bind: {}
  session-manager:
    command:
      - sh
      - -c
      - |
        apk add python3 py3-pip docker-cli zip unzip &&
        pip install fastapi uvicorn httpx websockets docker jinja2 python-multipart --break-system-packages --root-user-action=ignore &&
        mkdir -p /tmp/templates &&
        python3 session_manager.py
    configs:
      - source: session_manager_index.html
        target: /tmp/templates/index.html
      - source: session_manager_waiting.html
        target: /tmp/templates/waiting.html
      - source: session_manager.py
        target: /session_manager.py
    container_name: session-manager
    environment:
      DEFAULT_WORKSPACE: /workspace
      DOMAIN: bolabaden.org
      EXT_PATH: /home/ubuntu/my-media-stack/volumes/extensions/holo-lsp-1.0.0.vsix
      INACTIVITY_TIMEOUT: "3600"
      SESSION_MANAGER_PORT: "8080"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: session-manager
    healthcheck:
      test:
        - CMD-SHELL
        - wget -qO- http://127.0.0.1:8080/health > /dev/null 2>&1 || exit 1
      timeout: 10s
      start_period: 30s
    image: alpine
    labels:
      traefik.enable: "true"
      traefik.http.middlewares.hololsp-redirect.redirectRegex.permanent: "false"
      traefik.http.middlewares.hololsp-redirect.redirectRegex.regex: ^https?://hololsp\.((?:bolabaden.org|beatapostapita\.bolabaden.org))(.*)$$
      traefik.http.middlewares.hololsp-redirect.redirectRegex.replacement: https://holoscript.$$1$$2
      traefik.http.middlewares.holoscripter-redirect.redirectRegex.permanent: "false"
      traefik.http.middlewares.holoscripter-redirect.redirectRegex.regex: ^https?://holoscripter\.((?:bolabaden.org|beatapostapita\.bolabaden.org))(.*)$$
      traefik.http.middlewares.holoscripter-redirect.redirectRegex.replacement: https://holoscript.$$1$$2
      traefik.http.middlewares.kotorscript-redirect.redirectRegex.permanent: "false"
      traefik.http.middlewares.kotorscript-redirect.redirectRegex.regex: ^https?://kotorscript\.((?:bolabaden.org|beatapostapita\.bolabaden.org))(.*)$$
      traefik.http.middlewares.kotorscript-redirect.redirectRegex.replacement: https://holoscript.$$1$$2
      traefik.http.middlewares.kotorscripter-redirect.redirectRegex.permanent: "false"
      traefik.http.middlewares.kotorscripter-redirect.redirectRegex.regex: ^https?://kotorscripter\.((?:bolabaden.org|beatapostapita\.bolabaden.org))(.*)$$
      traefik.http.middlewares.kotorscripter-redirect.redirectRegex.replacement: https://holoscript.$$1$$2
      traefik.http.middlewares.kscript-redirect.redirectRegex.permanent: "false"
      traefik.http.middlewares.kscript-redirect.redirectRegex.regex: ^https?://kscript\.((?:bolabaden.org|beatapostapita\.bolabaden.org))(.*)$$
      traefik.http.middlewares.kscript-redirect.redirectRegex.replacement: https://holoscript.$$1$$2
      traefik.http.middlewares.tslscript-redirect.redirectRegex.permanent: "false"
      traefik.http.middlewares.tslscript-redirect.redirectRegex.regex: ^https?://tslscript\.((?:bolabaden.org|beatapostapita\.bolabaden.org))(.*)$$
      traefik.http.middlewares.tslscript-redirect.redirectRegex.replacement: https://holoscript.$$1$$2
      traefik.http.routers.hololsp-redirect.middlewares: hololsp-redirect@docker
      traefik.http.routers.hololsp-redirect.rule: Host(`hololsp.bolabaden.org`) || Host(`hololsp.beatapostapita.bolabaden.org`)
      traefik.http.routers.hololsp-redirect.service: holoscript@docker
      traefik.http.routers.holoscript.rule: Host(`holoscript.bolabaden.org`) || Host(`holoscript.beatapostapita.bolabaden.org`)
      traefik.http.routers.holoscripter-redirect.middlewares: holoscripter-redirect@docker
      traefik.http.routers.holoscripter-redirect.rule: Host(`holoscripter.bolabaden.org`) || Host(`holoscripter.beatapostapita.bolabaden.org`)
      traefik.http.routers.holoscripter-redirect.service: holoscript@docker
      traefik.http.routers.kotorscript-redirect.middlewares: kotorscript-redirect@docker
      traefik.http.routers.kotorscript-redirect.rule: Host(`kotorscript.bolabaden.org`) || Host(`kotorscript.beatapostapita.bolabaden.org`)
      traefik.http.routers.kotorscript-redirect.service: holoscript@docker
      traefik.http.routers.kotorscripter-redirect.middlewares: kotorscripter-redirect@docker
      traefik.http.routers.kotorscripter-redirect.rule: Host(`kotorscripter.bolabaden.org`) || Host(`kotorscripter.beatapostapita.bolabaden.org`)
      traefik.http.routers.kotorscripter-redirect.service: holoscript@docker
      traefik.http.routers.kscript-redirect.middlewares: kscript-redirect@docker
      traefik.http.routers.kscript-redirect.rule: Host(`kscript.bolabaden.org`) || Host(`kscript.beatapostapita.bolabaden.org`)
      traefik.http.routers.kscript-redirect.service: holoscript@docker
      traefik.http.routers.tslscript-redirect.middlewares: tslscript-redirect@docker
      traefik.http.routers.tslscript-redirect.rule: Host(`tslscript.bolabaden.org`) || Host(`tslscript.beatapostapita.bolabaden.org`)
      traefik.http.routers.tslscript-redirect.service: holoscript@docker
      traefik.http.services.holoscript.loadbalancer.server.port: "8080"
    networks:
      backend: null
      publicnet: null
    restart: always
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/extensions
        target: /home/ubuntu/my-media-stack/volumes/extensions
        bind: {}
  stremio:
    cpus: 2
    container_name: stremio
    environment:
      AUTO_SERVER_URL: "1"
      CASTING_DISABLED: "1"
      IPADDRESS: 127.0.0.1
      NO_CORS: "0"
      SERVER_URL: https://stremio.bolabaden.org/
      WEBUI_LOCATION: https://stremio.bolabaden.org/shell/
    hostname: stremio
    healthcheck:
      test:
        - CMD-SHELL
        - command -v curl >/dev/null || (apk add --no-cache curl >/dev/null 2>&1 || apt-get update >/dev/null 2>&1 && apt-get install -y curl >/dev/null 2>&1); (curl -fs http://127.0.0.1:11470/ >/dev/null 2>&1 || curl -fsk https://127.0.0.1:12470/ >/dev/null 2>&1) || exit 1
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 1m30s
    image: ghcr.io/tsaridas/stremio-docker:main
    labels:
      homepage.description: A one-stop hub for video content aggregation, allowing you to stream movies, series, and more from various sources.
      homepage.group: Media Streaming Platforms
      homepage.href: https://stremio.bolabaden.org/
      homepage.icon: stremio.png
      homepage.name: Stremio
      kuma.stremio.http.interval: "60"
      kuma.stremio.http.name: stremio.beatapostapita.bolabaden.org
      kuma.stremio.http.url: https://stremio.bolabaden.org
      traefik.enable: "true"
      traefik.http.middlewares.stremio-shell-redirect.redirectRegex.permanent: "false"
      traefik.http.middlewares.stremio-shell-redirect.redirectRegex.regex: ^(http|https)://stremio\.(bolabaden.org|beatapostapita\.bolabaden.org)(.*)$$
      traefik.http.middlewares.stremio-shell-redirect.redirectRegex.replacement: $$1://stremio.$$2$$3
      traefik.http.middlewares.stremio-streaming-server-redirect.redirectRegex.permanent: "false"
      traefik.http.middlewares.stremio-streaming-server-redirect.redirectRegex.regex: ^(http|https)://stremio\.(bolabaden.org|beatapostapita\.bolabaden.org)(/shell[^#?]*)(\\?[^#]*?streamingServer=[^&#]*)?([^#]*)(#.*)?$$
      traefik.http.middlewares.stremio-streaming-server-redirect.redirectRegex.replacement: $$1://stremio.$$2$$3?streamingServer=https%3A%2F%2Fstremio.$$2$$5$$6
      traefik.http.middlewares.stremio-web-redirect.redirectRegex.permanent: "false"
      traefik.http.middlewares.stremio-web-redirect.redirectRegex.regex: ^(http|https)://stremio(-web)\.(bolabaden.org|beatapostapita\.bolabaden.org)(.*)$$
      traefik.http.middlewares.stremio-web-redirect.redirectRegex.replacement: $$1://stremio.$$3$$4
      traefik.http.routers.stremio-http11470.rule: (Host(`stremio.bolabaden.org`) || Host(`stremio.beatapostapita.bolabaden.org`)) && ( PathPrefix(`/hlsv2`) || PathPrefix(`/casting`) || PathPrefix(`/local-addon`) || PathPrefix(`/proxy`) || PathPrefix(`/rar`) || PathPrefix(`/zip`) || PathPrefix(`/settings`) || PathPrefix(`/create`) || PathPrefix(`/removeAll`) || PathPrefix(`/samples`) || PathPrefix(`/probe`) || PathPrefix(`/subtitlesTracks`) || PathPrefix(`/opensubHash`) || PathPrefix(`/subtitles`) || PathPrefix(`/network-info`) || PathPrefix(`/device-info`) || PathPrefix(`/get-https`) || PathPrefix(`/hwaccel-profiler`) || PathPrefix(`/status`) || PathPrefix(`/exec`) || PathPrefix(`/stream`) || PathRegexp(`^/[^/]+/(stats\\.json|create|remove|destroy)$$`) || PathRegexp(`^/[^/]+/[^/]+/(stats\\.json|hls\\.m3u8|master\\.m3u8|stream\\.m3u8|dlna|thumb\\.jpg)$$`) || PathRegexp(`^/[^/]+/[^/]+/(stream-q-[^/]+\\.m3u8|stream-[^/]+\\.m3u8|subs-[^/]+\\.m3u8)$$`) || PathRegexp(`^/[^/]+/[^/]+/(stream-q-[^/]+|stream-[^/]+)/[^/]+\\.(ts|mp4)$$`) || PathRegexp(`^/yt/[^/]+(\\.json)?$$`) || Path(`/thumb.jpg`) || Path(`/stats.json`) )
      traefik.http.routers.stremio-http11470.service: stremio-http11470
      traefik.http.routers.stremio-https12470.rule: (Host(`stremio.bolabaden.org`) || Host(`stremio.beatapostapita.bolabaden.org`)) && ( PathPrefix(`/hlsv2`) || PathPrefix(`/casting`) || PathPrefix(`/local-addon`) || PathPrefix(`/proxy`) || PathPrefix(`/rar`) || PathPrefix(`/zip`) || PathPrefix(`/settings`) || PathPrefix(`/create`) || PathPrefix(`/removeAll`) || PathPrefix(`/samples`) || PathPrefix(`/probe`) || PathPrefix(`/subtitlesTracks`) || PathPrefix(`/opensubHash`) || PathPrefix(`/subtitles`) || PathPrefix(`/network-info`) || PathPrefix(`/device-info`) || PathPrefix(`/get-https`) || PathPrefix(`/hwaccel-profiler`) || PathPrefix(`/status`) || PathPrefix(`/exec`) || PathPrefix(`/stream`) || PathRegexp(`^/[^/]+/(stats\\.json|create|remove|destroy)$$`) || PathRegexp(`^/[^/]+/[^/]+/(stats\\.json|hls\\.m3u8|master\\.m3u8|stream\\.m3u8|dlna|thumb\\.jpg)$$`) || PathRegexp(`^/[^/]+/[^/]+/(stream-q-[^/]+\\.m3u8|stream-[^/]+\\.m3u8|subs-[^/]+\\.m3u8)$$`) || PathRegexp(`^/[^/]+/[^/]+/(stream-q-[^/]+|stream-[^/]+)/[^/]+\\.(ts|mp4)$$`) || PathRegexp(`^/yt/[^/]+(\\.json)?$$`) || Path(`/thumb.jpg`) || Path(`/stats.json`) )
      traefik.http.routers.stremio-https12470.service: stremio-https12470
      traefik.http.routers.stremio.middlewares: stremio-web-redirect@docker,stremio-shell-redirect@docker,stremio-streaming-server-redirect@docker
      traefik.http.routers.stremio.rule: Host(`stremio-web.bolabaden.org`) || Host(`stremio-web.beatapostapita.bolabaden.org`) || Host(`stremio.bolabaden.org`) || Host(`stremio.beatapostapita.bolabaden.org`)
      traefik.http.routers.stremio.service: stremio
      traefik.http.services.stremio-http11470.loadbalancer.server.port: "11470"
      traefik.http.services.stremio-http11470.loadbalancer.server.scheme: http
      traefik.http.services.stremio-https12470.loadbalancer.server.port: "12470"
      traefik.http.services.stremio-https12470.loadbalancer.server.scheme: https
      traefik.http.services.stremio.loadbalancer.server.port: "8080"
      traefik.http.services.stremio.loadbalancer.server.scheme: https
    mem_limit: "2147483648"
    mem_reservation: "268435456"
    networks:
      publicnet: {}
      warp-nat-net:
        gw_priority: 1
    ports:
      - mode: ingress
        target: 11470
        published: "11470"
        protocol: tcp
      - mode: ingress
        target: 12470
        published: "12470"
        protocol: tcp
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/stremio/root/.stremio-server
        target: /root/.stremio-server
        bind: {}
  stremthru:
    container_name: stremthru
    depends_on:
      redis:
        condition: service_healthy
        required: true
    environment:
      PGID: "999"
      PUID: "1001"
      STREMTHRU_AUTH_ADMIN: brunner56
      STREMTHRU_BASE_URL: https://stremthru.bolabaden.org
      STREMTHRU_CONTENT_PROXY_CONNECTION_LIMIT: '*:10'
      STREMTHRU_DATABASE_URI: sqlite://./data/stremthru.db
      STREMTHRU_FEATURE: +anime,-stremio_p2p
      STREMTHRU_INTEGRATION_GITHUB_TOKEN: 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
      STREMTHRU_INTEGRATION_GITHUB_USER: th3w1zard1
      STREMTHRU_INTEGRATION_MDBLIST_LIST_STALE_TIME: null
      STREMTHRU_INTEGRATION_TMDB_ACCESS_TOKEN: cec876f852b9c15d2c1b436b1117dff7
      STREMTHRU_INTEGRATION_TMDB_LIST_STALE_TIME: 12h
      STREMTHRU_INTEGRATION_TRAKT_CLIENT_ID: 052d735c61c723ed670991aee9d6322822ed6766702d66910be63a1e1a511fd0
      STREMTHRU_INTEGRATION_TRAKT_CLIENT_SECRET: 1d5701574cae67023c25a3743f07669bb84ee74628fd8f0a3fc0c9cca51535c3
      STREMTHRU_INTEGRATION_TRAKT_LIST_STALE_TIME: 12h
      STREMTHRU_LOG_FORMAT: text
      STREMTHRU_LOG_LEVEL: INFO
      STREMTHRU_PROXY_AUTH_FILE: /run/secrets/stremthru-proxy-auth
      STREMTHRU_REDIS_URI: redis://redis:6379
      STREMTHRU_STORE_AUTH_FILE: /run/secrets/stremthru-store-auth
      STREMTHRU_STORE_CONTENT_PROXY: '*:true,premiumize:false'
      STREMTHRU_STREMIO_TORZ_LAZY_PULL: "true"
      TZ: America/Chicago
      UMASK: "002"
    expose:
      - "8080"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: stremthru
    healthcheck:
      test:
        - CMD-SHELL
        - wget -qO- http://127.0.0.1:8080 > /dev/null 2>&1 || exit 1
      timeout: 5s
      interval: 1m0s
      retries: 3
      start_period: 15s
    image: docker.io/muniftanjim/stremthru
    labels:
      homepage.description: Tunnel/proxy bridge for Debrid services to unify access of streaming links through a single host.
      homepage.group: Stremio Addons
      homepage.href: https://stremthru.bolabaden.org/
      homepage.icon: stremthru.png
      homepage.name: StremThru
      kuma.stremthru.http.interval: "60"
      kuma.stremthru.http.name: stremthru.beatapostapita.bolabaden.org
      kuma.stremthru.http.url: https://stremthru.bolabaden.org
      traefik.enable: "true"
      traefik.http.routers.stremthru.rule: Host(`stremthru.bolabaden.org`) || Host(`stremthru.beatapostapita.bolabaden.org`)
      traefik.http.services.stremthru.loadbalancer.server.port: "8080"
    networks:
      publicnet: {}
      warp-nat-net:
        gw_priority: 1
    restart: always
    secrets:
      - source: stremthru-proxy-auth
        target: /run/secrets/stremthru-proxy-auth
      - source: stremthru-store-auth
        target: /run/secrets/stremthru-store-auth
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/stremio/addons/stremthru/app/data
        target: /app/data
        bind: {}
  telemetry-auth:
    build:
      context: /home/ubuntu/my-media-stack/projects/kotormodsync/telemetry-auth
      dockerfile: Dockerfile
    container_name: telemetry-auth-test
    environment:
      AUTH_SERVICE_PORT: "8080"
      KOTORMODSYNC_SECRET_FILE: /run/secrets/signing_secret
      LOG_LEVEL: info
      MAX_TIMESTAMP_DRIFT: "300"
      REQUIRE_AUTH: "true"
    hostname: telemetry-auth
    healthcheck:
      test:
        - CMD-SHELL
        - wget --no-verbose --tries=1 --spider http://127.0.0.1:8080/health || exit 1
      timeout: 3s
      interval: 10s
      retries: 5
      start_period: 30s
    image: bolabaden/kotormodsync-telemetry-auth
    networks:
      default: null
    ports:
      - mode: ingress
        target: 8080
        published: "8080"
        protocol: tcp
    restart: unless-stopped
    secrets:
      - source: signing_secret
        target: /run/secrets/signing_secret
    user: "0:0"
  tinyauth:
    container_name: tinyauth
    environment:
      APP_TITLE: bolabaden.org
      APP_URL: https://auth.bolabaden.org
      COOKIE_SECURE: "true"
      GITHUB_CLIENT_ID: Ov23ct3xtNzKCw4aeqfQ
      GITHUB_CLIENT_SECRET_FILE: /run/secrets/tinyauth-github-client-secret
      GOOGLE_CLIENT_ID: 324816055390-dftm6jq2d3ifs37miivj3nj09piv0df9.apps.googleusercontent.com
      GOOGLE_CLIENT_SECRET_FILE: /run/secrets/tinyauth-google-client-secret
      LOGIN_MAX_RETRIES: "15"
      LOGIN_TIMEOUT: "300"
      OAUTH_AUTO_REDIRECT: none
      OAUTH_WHITELIST: boden.crouch@gmail.com,halomastar@gmail.com,athenajaguiar@gmail.com,dgorsch2@gmail.com,dgorsch4@gmail.com
      SECRET_FILE: /run/secrets/tinyauth-secret
      SESSION_EXPIRY: "604800"
      USERS: brunner56:$$2y$$05$$CEXtGy1XS4BlkTXEgSG64eSQlrGn.55Xm2HbiSgVSgO./WoVXXLAa,bolabaden.duckdns@gmail.com:$$2y$$05$$BbVWsqh2XkzaxvLGwd.0euD/PkkoaeFNDIhB0tTEBn4tAjGckKRzq
    expose:
      - 3000
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: auth
    image: ghcr.io/steveiliop56/tinyauth:latest
    labels:
      homepage.description: Centralized login service (email, Google, GitHub) used by Traefik auth middleware and apps
      homepage.group: Security
      homepage.href: https://auth.bolabaden.org/
      homepage.icon: https://tinyauth.app/img/logo.png
      homepage.name: TinyAuth
      kuma.tinyauth.http.interval: "60"
      kuma.tinyauth.http.name: auth.beatapostapita.bolabaden.org
      kuma.tinyauth.http.url: https://auth.bolabaden.org
      traefik.enable: "true"
      traefik.http.middlewares.tinyauth.forwardAuth.address: http://auth:3000/api/auth/traefik
      traefik.http.routers.tinyauth.rule: Host(`auth.bolabaden.org`) || Host(`auth.beatapostapita.bolabaden.org`)
      traefik.http.services.tinyauth.loadbalancer.server.port: "3000"
    networks:
      backend: null
      publicnet: null
    restart: always
    secrets:
      - source: tinyauth-secret
        target: /run/secrets/tinyauth-secret
      - source: tinyauth-google-client-secret
        target: /run/secrets/tinyauth-google-client-secret
      - source: tinyauth-github-client-secret
        target: /run/secrets/tinyauth-github-client-secret
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/traefik/tinyauth
        target: /data
        bind: {}
  traefik:
    cap_add:
      - NET_ADMIN
    command:
      - --accessLog=true
      - --accessLog.bufferingSize=0
      - --accessLog.fields.headers.defaultMode=drop
      - --accessLog.fields.headers.names.User-Agent=keep
      - --accessLog.fields.names.StartUTC=drop
      - --accessLog.filePath=/var/log/traefik/traefik.log
      - --accessLog.filters.statusCodes=100-999
      - --accessLog.format=json
      - --metrics.prometheus.buckets=0.1,0.3,1.2,5.0
      - --api.dashboard=true
      - --api.debug=true
      - --api.disableDashboardAd=true
      - --api.insecure=true
      - --api=true
      - --certificatesResolvers.letsencrypt.acme.caServer=https://acme-v02.api.letsencrypt.org/directory
      - --certificatesResolvers.letsencrypt.acme.dnsChallenge=true
      - --certificatesResolvers.letsencrypt.acme.dnsChallenge.provider=cloudflare
      - --certificatesResolvers.letsencrypt.acme.dnsChallenge.resolvers=1.1.1.1,1.0.0.1
      - --certificatesResolvers.letsencrypt.acme.email=boden.crouch@gmail.com
      - --certificatesResolvers.letsencrypt.acme.httpChallenge=false
      - --certificatesResolvers.letsencrypt.acme.httpChallenge.entryPoint=web
      - --certificatesResolvers.letsencrypt.acme.tlsChallenge=false
      - --certificatesResolvers.letsencrypt.acme.storage=/certs/acme.json
      - --entryPoints.web.address=:80
      - --entryPoints.web.http.redirections.entryPoint.scheme=https
      - --entryPoints.web.http.redirections.entryPoint.to=websecure
      - --entryPoints.web.forwardedHeaders.trustedIPs=103.21.244.0/22,103.22.200.0/22,103.31.4.0/22,104.16.0.0/13,104.24.0.0/14,108.162.192.0/18,131.0.72.0/22,141.101.64.0/18,162.158.0.0/15,172.64.0.0/13,173.245.48.0/20,188.114.96.0/20,190.93.240.0/20,197.234.240.0/22,198.41.128.0/17,2400:cb00::/32,2405:8100::/32,2405:b500::/32,2606:4700::/32,2803:f800::/32,2a06:98c0::/29,2c0f:f248::/32
      - --entryPoints.websecure.forwardedHeaders.trustedIPs=103.21.244.0/22,103.22.200.0/22,103.31.4.0/22,104.16.0.0/13,104.24.0.0/14,108.162.192.0/18,131.0.72.0/22,141.101.64.0/18,162.158.0.0/15,172.64.0.0/13,173.245.48.0/20,188.114.96.0/20,190.93.240.0/20,197.234.240.0/22,198.41.128.0/17,2400:cb00::/32,2405:8100::/32,2405:b500::/32,2606:4700::/32,2803:f800::/32,2a06:98c0::/29,2c0f:f248::/32
      - --entryPoints.websecure.address=:443
      - --entryPoints.websecure.http.encodeQuerySemiColons=true
      - --entryPoints.websecure.http.middlewares=bolabaden-error-pages@file,crowdsec@file,strip-www@file
      - --entryPoints.websecure.http.tls=true
      - --entryPoints.websecure.http.tls.certResolver=letsencrypt
      - --entryPoints.websecure.http.tls.domains[0].main=bolabaden.org
      - --entryPoints.websecure.http.tls.domains[0].sans=www.bolabaden.org,*.bolabaden.org,*.beatapostapita.bolabaden.org
      - --entryPoints.websecure.http2.maxConcurrentStreams=100
      - --entryPoints.websecure.http3
      - --global.checkNewVersion=true
      - --global.sendAnonymousUsage=false
      - --log.level=INFO
      - --ping=true
      - --providers.docker=true
      - --providers.docker.endpoint=unix:///var/run/docker.sock
      - --providers.docker.network=my-media-stack_publicnet
      - --providers.docker.defaultRule=Host(`{{ normalize .ContainerName }}.bolabaden.org`) || Host(`{{ normalize .Name }}.bolabaden.org`) || Host(`{{ normalize .ContainerName }}.beatapostapita.bolabaden.org`) || Host(`{{ normalize .Name }}.beatapostapita.bolabaden.org`)
      - --providers.docker.exposedByDefault=false
      - --providers.file.directory=/traefik/dynamic/
      - --providers.file.watch=true
      - --experimental.plugins.bouncer.modulename=github.com/maxlerebourg/crowdsec-bouncer-traefik-plugin
      - --experimental.plugins.bouncer.version=v1.4.5
      - --experimental.plugins.traefikerrorreplace.modulename=github.com/PseudoResonance/traefikerrorreplace
      - --experimental.plugins.traefikerrorreplace.version=v1.0.1
      - --serversTransport.insecureSkipVerify=true
    configs:
      - source: traefik-dynamic.yaml
        target: /traefik/dynamic/core.yaml
        mode: "0644"
    container_name: traefik
    depends_on:
      crowdsec:
        condition: service_started
        required: true
      dockerproxy-ro:
        condition: service_healthy
        required: true
    environment:
      CLOUDFLARE_API_KEY_FILE: /run/secrets/cloudflare-api-key
      CLOUDFLARE_EMAIL: boden.crouch@gmail.com
      CLOUDFLARE_ZONE_ID: 164c8d72507295b51851d9b05f0e37a1
      DOCKER_API_VERSION: "1.52"
      DOCKER_HOST: unix:///var/run/docker.sock
      LETS_ENCRYPT_EMAIL: boden.crouch@gmail.com
    expose:
      - "8080"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: traefik
    healthcheck:
      test:
        - CMD-SHELL
        - traefik healthcheck --ping
      timeout: 3s
      interval: 10s
      retries: 3
      start_period: 10s
    image: docker.io/traefik:latest
    labels:
      homepage.description: Reverse proxy entrypoint for all services with TLS, Cloudflare integration, and auth middleware
      homepage.group: Infrastructure
      homepage.href: https://traefik.bolabaden.org/dashboard
      homepage.icon: traefik.png
      homepage.name: Traefik
      homepage.widget.type: traefik
      homepage.widget.url: http://traefik:8080
      kuma.traefik.http.interval: "20"
      kuma.traefik.http.name: traefik.beatapostapita.bolabaden.org
      kuma.traefik.http.url: https://traefik.bolabaden.org/dashboard
      traefik.enable: "true"
      traefik.http.routers.traefik.rule: Host(`traefik.bolabaden.org`) || Host(`traefik.beatapostapita.bolabaden.org`)
      traefik.http.routers.traefik.service: api@internal
      traefik.http.services.traefik.loadbalancer.server.port: "8080"
    networks:
      default: null
      nginx_net: null
      publicnet: null
    ports:
      - mode: ingress
        target: 80
        published: "80"
        protocol: tcp
      - mode: ingress
        target: 443
        published: "443"
        protocol: tcp
      - mode: ingress
        target: 443
        published: "443"
        protocol: udp
    restart: always
    secrets:
      - source: cloudflare-api-key
        target: /run/secrets/cloudflare-api-key
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        read_only: true
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/traefik/dynamic
        target: /traefik/dynamic
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/traefik/certs
        target: /certs
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/traefik/plugins-local
        target: /plugins-local
        bind: {}
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/traefik/logs
        target: /var/log/traefik
        bind: {}
  victoriametrics:
    command:
      - --bigMergeConcurrency=0
      - --dedup.minScrapeInterval=0s
      - --enableTCP6=false
      - --finalMergeDelay=0s
      - --http.maxGracefulShutdownDuration=7s
      - --http.shutdownDelay=0s
      - --httpListenAddr=:8428
      - --influx.maxLineSize=262144
      - --loggerFormat=default
      - --loggerLevel=INFO
      - --memory.allowedPercent=60
      - --promscrape.maxScrapeSize=16777216
      - --retentionPeriod=1y
      - --search.maxConcurrentRequests=8
      - --search.maxMemoryPerQuery=1GB
      - --search.maxPointsPerTimeseries=30000
      - --search.maxQueryDuration=30s
      - --search.maxSeries=30000
      - --search.maxTagKeys=100000
      - --search.maxTagValues=100000
      - --search.maxUniqueTimeseries=300000
      - --smallMergeConcurrency=0
      - --storageDataPath=/storage
    container_name: victoriametrics
    depends_on:
      init_victoriametrics:
        condition: service_completed_successfully
        required: true
    environment:
      VM_INSERT_MAX_CONCURRENT_REQUESTS: "32"
      VM_MEMORY_ALLOWED_PERCENT: "60"
      VM_RETENTION_PERIOD: 1y
      VM_SEARCH_MAX_CONCURRENT_REQUESTS: "8"
    expose:
      - "8428"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: victoriametrics
    healthcheck:
      test:
        - CMD-SHELL
        - wget --no-verbose --tries=1 --spider http://127.0.0.1:8428/health || exit 1
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 40s
    image: docker.io/victoriametrics/victoria-metrics:latest
    labels:
      prometheus.io/path: /metrics
      prometheus.io/port: "8428"
      prometheus.io/scrape: "true"
    networks:
      backend: null
    restart: always
    volumes:
      - type: bind
        source: /home/ubuntu/my-media-stack/volumes/victoriametrics
        target: /storage
        bind: {}
  warp-nat-gateway:
    cap_add:
      - MKNOD
      - AUDIT_WRITE
      - NET_ADMIN
    container_name: warp-nat-gateway
    depends_on:
      warp-net-init:
        condition: service_completed_successfully
        required: true
    device_cgroup_rules:
      - c 10:200 rwm
    environment:
      BETA_FIX_HOST_CONNECTIVITY: "false"
      GOST_ARGS: -L :1080
      WARP_ENABLE_NAT: "false"
      WARP_LICENSE_KEY_FILE: ""
      WARP_SLEEP: "2"
    expose:
      - "1080"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: warp-nat-gateway
    healthcheck:
      test:
        - CMD-SHELL
        - exit 0
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 20s
    image: docker.io/caomingjun/warp
    networks:
      default: null
    restart: always
    secrets:
      - source: warp-license-key
        target: /run/secrets/warp-license-key
    sysctls:
      net.ipv4.conf.all.src_valid_mark: "1"
      net.ipv4.ip_forward: "1"
      net.ipv6.conf.all.accept_ra: "2"
      net.ipv6.conf.all.disable_ipv6: "0"
      net.ipv6.conf.all.forwarding: "1"
    volumes:
      - type: volume
        source: warp-config-data
        target: /var/lib/cloudflare-warp
        volume: {}
  warp-net-init:
    command:
      - sh
      - -c
      - |
        # Create network if it doesn't exist
        if ! docker network inspect warp-nat-net >/dev/null 2>&1; then
          echo "Creating network warp-nat-net..."
          docker network create \
            --driver=bridge \
            --attachable \
            -o com.docker.network.bridge.name=br_warp-nat-net \
            -o com.docker.network.bridge.enable_ip_masquerade=false \
            --subnet=10.0.2.0/24 \
            --gateway=10.0.2.1 \
            warp-nat-net
          echo "Network created successfully"
        else
          echo "Network warp-nat-net already exists"
        fi
    container_name: warp-net-init
    image: docker:cli
    network_mode: host
    restart: "no"
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        read_only: true
        bind: {}
  warp_router:
    build:
      context: /home/ubuntu/my-media-stack/compose
      dockerfile_inline: |
        FROM alpine
        RUN apk update && apk add \
            bash bc curl docker-cli ipcalc iproute2 iptables jq util-linux && \
            rm -rf /var/cache/apk/*
    command:
      - /bin/bash
      - /usr/local/bin/warp-monitor.sh
    configs:
      - source: warp-nat-setup.sh
        target: /usr/local/bin/warp-nat-setup.sh
        mode: "01274"
      - source: warp-monitor.sh
        target: /usr/local/bin/warp-monitor.sh
        mode: "01274"
    container_name: warp_router
    depends_on:
      warp-nat-gateway:
        condition: service_started
        required: true
    environment:
      CONT_VETH_IP: 169.254.100.2
      DOCKER_NETWORK_NAME: warp-nat-net
      HOST_VETH_IP: 169.254.100.1
      RETRY_SETUP_AFTER: "12"
      ROUTER_CONTAINER_NAME: warp_router
      ROUTING_TABLE: warp-nat-routing
      SLEEP_INTERVAL: "5"
      VETH_HOST: veth-warp
      WARP_CONTAINER_NAME: warp-nat-gateway
      WARP_NAT_NET_GATEWAY: 10.0.2.1
      WARP_NAT_NET_SUBNET: 10.0.2.0/24
    healthcheck:
      test:
        - CMD-SHELL
        - exit 0
      timeout: 10s
      interval: 30s
      retries: 3
      start_period: 20s
    network_mode: host
    privileged: true
    restart: always
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        bind: {}
      - type: bind
        source: /etc/iproute2/rt_tables
        target: /etc/iproute2/rt_tables
        bind: {}
      - type: bind
        source: /proc
        target: /proc
        bind: {}
  watchtower:
    configs:
      - source: watchtower-config.json
        target: /config.json
        mode: "0444"
    container_name: watchtower
    environment:
      DOCKER_API_VERSION: "1.24"
      DOCKER_HOST: unix:///var/run/docker.sock
      DOCKER_TLS_VERIFY: "false"
      NO_COLOR: "1"
      REPO_PASS: h4L0m4St3R327
      REPO_USER: bolabaden
      TZ: America/Chicago
      WATCHTOWER_CLEANUP: "true"
      WATCHTOWER_DEBUG: "true"
      WATCHTOWER_DISABLE_CONTAINERS: ""
      WATCHTOWER_HTTP_API_METRICS: "false"
      WATCHTOWER_HTTP_API_PERIODIC_POLLS: "false"
      WATCHTOWER_HTTP_API_TOKEN: ""
      WATCHTOWER_HTTP_API_UPDATE: "false"
      WATCHTOWER_INCLUDE_RESTARTING: "true"
      WATCHTOWER_INCLUDE_STOPPED: "true"
      WATCHTOWER_LABEL_ENABLE: "false"
      WATCHTOWER_LABEL_TAKE_PRECEDENCE: "true"
      WATCHTOWER_LOG_FORMAT: Auto
      WATCHTOWER_LOG_LEVEL: debug
      WATCHTOWER_MONITOR_ONLY: "false"
      WATCHTOWER_NO_PULL: "false"
      WATCHTOWER_NO_RESTART: "false"
      WATCHTOWER_NO_STARTUP_MESSAGE: "false"
      WATCHTOWER_NOTIFICATION_REPORT: "true"
      WATCHTOWER_NOTIFICATION_TEMPLATE: |
        {{- if .Report -}}
          {{- with .Report -}}
            {{- if ( or .Updated .Failed ) -}}
        {{len .Scanned}} Scanned, {{len .Updated}} Updated, {{len .Failed}} Failed
              {{- range .Updated}}
        - {{.Name}} ({{.ImageName}}): {{.CurrentImageID.ShortID}} updated to {{.LatestImageID.ShortID}}
              {{- end -}}
              {{- range .Skipped}}
        - {{.Name}} ({{.ImageName}}): {{.State}}: {{.Error}}
              {{- end -}}
              {{- range .Failed}}
        - {{.Name}} ({{.ImageName}}): {{.State}}: {{.Error}}
              {{- end -}}
            {{- end -}}
          {{- end -}}
        {{- else -}}
          {{range .Entries -}}{{.Message}}{{"\n"}}{{- end -}}
        {{- end -}}
      WATCHTOWER_NOTIFICATION_URL: ""
      WATCHTOWER_POLL_INTERVAL: "86400"
      WATCHTOWER_PORCELAIN: ""
      WATCHTOWER_REMOVE_VOLUMES: "false"
      WATCHTOWER_REVIVE_STOPPED: "false"
      WATCHTOWER_ROLLING_RESTART: "false"
      WATCHTOWER_RUN_ONCE: "false"
      WATCHTOWER_SCHEDULE: 0 0 6 * * *
      WATCHTOWER_SCOPE: ""
      WATCHTOWER_TIMEOUT: 10s
      WATCHTOWER_TRACE: "false"
      WATCHTOWER_WARN_ON_HEAD_FAILURE: auto
      restart: always
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: watchtower
    image: docker.io/containrrr/watchtower
    networks:
      backend: null
    volumes:
      - type: bind
        source: /var/run/docker.sock
        target: /var/run/docker.sock
        bind: {}
  whoami:
    container_name: whoami
    expose:
      - "80"
    extra_hosts:
      - host.docker.internal=host-gateway
    hostname: whoami
    image: docker.io/traefik/whoami:latest
    labels:
      homepage.description: Request echo service used to verify reverse-proxy, headers, and auth middleware
      homepage.group: Web Services
      homepage.href: https://whoami.bolabaden.org
      homepage.icon: whoami.png
      homepage.name: whoami
      kuma.whoami.http.interval: "60"
      kuma.whoami.http.name: whoami.beatapostapita.bolabaden.org
      kuma.whoami.http.url: https://whoami.bolabaden.org
      traefik.enable: "true"
      traefik.http.routers.whoami.service: whoami@docker
      traefik.http.services.whoami.loadBalancer.server.port: "80"
    networks:
      backend: null
      publicnet: null
    restart: always
networks:
  backend:
    name: my-media-stack_backend
    driver_opts:
      com.docker.network.bridge.name: br_backend
    ipam:
      config:
        - subnet: 10.0.7.0/24
          gateway: 10.0.7.1
    attachable: true
  default:
    name: my-media-stack_default
  nginx_net:
    name: my-media-stack_nginx_net
    driver_opts:
      com.docker.network.bridge.name: br_nginx_net
    ipam:
      config:
        - subnet: 10.0.8.0/24
          gateway: 10.0.8.1
    attachable: true
  publicnet:
    name: my-media-stack_publicnet
    driver_opts:
      com.docker.network.bridge.name: br_publicnet
    ipam:
      config:
        - subnet: 10.76.0.0/16
          gateway: 10.76.0.1
    attachable: true
  warp-nat-net:
    name: warp-nat-net
    driver_opts:
      com.docker.network.bridge.enable_ip_masquerade: "false"
      com.docker.network.bridge.name: br_warp-nat-net
    ipam:
      config:
        - subnet: 10.0.2.0/24
          gateway: 10.0.2.1
    external: true
    attachable: true
volumes:
  warp-config-data:
    name: my-media-stack_warp-config-data
secrets:
  aiostreams-addon-password:
    name: my-media-stack_aiostreams-addon-password
    file: /home/ubuntu/my-media-stack/secrets/aiostreams-addon-password.txt
  aiostreams-secret-key:
    name: my-media-stack_aiostreams-secret-key
    file: /home/ubuntu/my-media-stack/secrets/aiostreams-secret-key.txt
  alldebrid-api-key:
    name: my-media-stack_alldebrid-api-key
    file: /home/ubuntu/my-media-stack/secrets/alldebrid-api-key.txt
  anthropic-api-key:
    name: my-media-stack_anthropic-api-key
    file: /home/ubuntu/my-media-stack/secrets/anthropic-api-key.txt
  brave-api-key:
    name: my-media-stack_brave-api-key
    file: /home/ubuntu/my-media-stack/secrets/brave-api-key.txt
  cloudflare-api-key:
    name: my-media-stack_cloudflare-api-key
    file: /home/ubuntu/my-media-stack/secrets/cf-api-key.txt
  cloudflare-api-token:
    name: my-media-stack_cloudflare-api-token
    file: /home/ubuntu/my-media-stack/secrets/cf-api-token.txt
  debridlink-api-key:
    name: my-media-stack_debridlink-api-key
    file: /home/ubuntu/my-media-stack/secrets/debridlink-api-key.txt
  deepseek-api-key:
    name: my-media-stack_deepseek-api-key
    file: /home/ubuntu/my-media-stack/secrets/deepseek-api-key.txt
  exa-api-key:
    name: my-media-stack_exa-api-key
    file: /home/ubuntu/my-media-stack/secrets/exa-api-key.txt
  firecrawl-api-key:
    name: my-media-stack_firecrawl-api-key
    file: /home/ubuntu/my-media-stack/secrets/firecrawl-api-key.txt
  gemini-api-key:
    name: my-media-stack_gemini-api-key
    file: /home/ubuntu/my-media-stack/secrets/gemini-api-key.txt
  glama-api-key:
    name: my-media-stack_glama-api-key
    file: /home/ubuntu/my-media-stack/secrets/glama-api-key.txt
  grafana-admin-password:
    name: my-media-stack_grafana-admin-password
    file: /home/ubuntu/my-media-stack/secrets/grafana-password.txt
  grafana-secret-key:
    name: my-media-stack_grafana-secret-key
    file: /home/ubuntu/my-media-stack/secrets/grafana-secret-key.txt
  groq-api-key:
    name: my-media-stack_groq-api-key
    file: /home/ubuntu/my-media-stack/secrets/groq-api-key.txt
  hf-token:
    name: my-media-stack_hf-token
    file: /home/ubuntu/my-media-stack/secrets/hf-token.txt
  langchain-api-key:
    name: my-media-stack_langchain-api-key
    file: /home/ubuntu/my-media-stack/secrets/langchain-api-key.txt
  mistral-api-key:
    name: my-media-stack_mistral-api-key
    file: /home/ubuntu/my-media-stack/secrets/mistral-api-key.txt
  offcloud-api-key:
    name: my-media-stack_offcloud-api-key
    file: /home/ubuntu/my-media-stack/secrets/offcloud-api-key.txt
  offcloud-password:
    name: my-media-stack_offcloud-password
    file: /home/ubuntu/my-media-stack/secrets/offcloud-password.txt
  openai-api-key:
    name: my-media-stack_openai-api-key
    file: /home/ubuntu/my-media-stack/secrets/openai-api-key.txt
  openrouter-api-key:
    name: my-media-stack_openrouter-api-key
    file: /home/ubuntu/my-media-stack/secrets/openrouter-api-key.txt
  perplexity-api-key:
    name: my-media-stack_perplexity-api-key
    file: /home/ubuntu/my-media-stack/secrets/perplexity-api-key.txt
  premiumize-api-key:
    name: my-media-stack_premiumize-api-key
    file: /home/ubuntu/my-media-stack/secrets/premiumize-api-key.txt
  realdebrid-api-key:
    name: my-media-stack_realdebrid-api-key
    file: /home/ubuntu/my-media-stack/secrets/realdebrid-api-key.txt
  replicate-api-key:
    name: my-media-stack_replicate-api-key
    file: /home/ubuntu/my-media-stack/secrets/replicate-api-key.txt
  revid-api-key:
    name: my-media-stack_revid-api-key
    file: /home/ubuntu/my-media-stack/secrets/revid-api-key.txt
  sambanova-api-key:
    name: my-media-stack_sambanova-api-key
    file: /home/ubuntu/my-media-stack/secrets/sambanova-api-key.txt
  search1api-key:
    name: my-media-stack_search1api-key
    file: /home/ubuntu/my-media-stack/secrets/search1api-key.txt
  serpapi-api-key:
    name: my-media-stack_serpapi-api-key
    file: /home/ubuntu/my-media-stack/secrets/serpapi-api-key.txt
  signing_secret:
    name: my-media-stack_signing_secret
    file: /home/ubuntu/my-media-stack/secrets/signing_secret.txt
  stremthru-proxy-auth:
    name: my-media-stack_stremthru-proxy-auth
    file: /home/ubuntu/my-media-stack/secrets/stremthru-proxy-auth.txt
  stremthru-store-auth:
    name: my-media-stack_stremthru-store-auth
    file: /home/ubuntu/my-media-stack/secrets/stremthru-store-auth.txt
  tavily-api-key:
    name: my-media-stack_tavily-api-key
    file: /home/ubuntu/my-media-stack/secrets/tavily-api-key.txt
  tinyauth-github-client-secret:
    name: my-media-stack_tinyauth-github-client-secret
    file: /home/ubuntu/my-media-stack/secrets/tinyauth-github-client-secret.txt
  tinyauth-google-client-secret:
    name: my-media-stack_tinyauth-google-client-secret
    file: /home/ubuntu/my-media-stack/secrets/tinyauth-google-client-secret.txt
  tinyauth-secret:
    name: my-media-stack_tinyauth-secret
    file: /home/ubuntu/my-media-stack/secrets/tinyauth-secret.txt
  tmdb-access-token:
    name: my-media-stack_tmdb-access-token
    file: /home/ubuntu/my-media-stack/secrets/tmdb-access-token.txt
  tmdb-api-key:
    name: my-media-stack_tmdb-api-key
    file: /home/ubuntu/my-media-stack/secrets/tmdb-api-key.txt
  togetherai-api-key:
    name: my-media-stack_togetherai-api-key
    file: /home/ubuntu/my-media-stack/secrets/togetherai-api-key.txt
  torbox-api-key:
    name: my-media-stack_torbox-api-key
    file: /home/ubuntu/my-media-stack/secrets/torbox-api-key.txt
  upstage-api-key:
    name: my-media-stack_upstage-api-key
    file: /home/ubuntu/my-media-stack/secrets/upstage-api-key.txt
  warp-license-key:
    name: my-media-stack_warp-license-key
    file: /home/ubuntu/my-media-stack/secrets/warp-license-key.txt
configs:
  alert.rules:
    name: my-media-stack_alert.rules
    content: |
      groups:
      - name: homelab.rules
        rules:
        - alert: InstanceDown
          expr: up == 0
          for: 10m
          labels:
            severity: page
          annotations:
            summary: "Instance {{ $$labels.instance }} down"
            description: "{{ $$labels.instance }} of job {{ $$labels.job }} has been down for more than 10 minutes."

        - alert: HighCPUUsage
          expr: 100 - (avg by(instance) (irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High CPU usage on {{ $$labels.instance }}"
            description: "CPU usage is above 80% on {{ $$labels.instance }} for more than 5 minutes."

        - alert: HighMemoryUsage
          expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100 > 90
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High memory usage on {{ $$labels.instance }}"
            description: "Memory usage is above 90% on {{ $$labels.instance }} for more than 5 minutes."

        - alert: DiskSpaceLow
          expr: (node_filesystem_size_bytes{fstype!="tmpfs"} - node_filesystem_free_bytes{fstype!="tmpfs"}) / node_filesystem_size_bytes{fstype!="tmpfs"} * 100 > 90
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Low disk space on {{ $$labels.instance }}"
            description: "Disk usage is above 90% on {{ $$labels.instance }} {{ $$labels.mountpoint }} for more than 5 minutes."

        - alert: VictoriaMetricsDown
          expr: up{job="victoriametrics"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "VictoriaMetrics is down"
            description: "VictoriaMetrics has been down for more than 5 minutes."

        - alert: PrometheusDown
          expr: up{job="prometheus"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Prometheus is down"
            description: "Prometheus has been down for more than 5 minutes."

        - alert: GrafanaDown
          expr: up{job="grafana"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Grafana is down"
            description: "Grafana has been down for more than 5 minutes."

        - alert: CrowdSecDown
          expr: up{job="crowdsec"} == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "CrowdSec is down"
            description: "CrowdSec has been down for more than 5 minutes."

        - alert: TraefikDown
          expr: up{job="traefik"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Traefik is down"
            description: "Traefik has been down for more than 5 minutes."

        - alert: HighRequestLatency
          expr: histogram_quantile(0.95, rate(traefik_service_request_duration_seconds_bucket[5m])) > 0.5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High request latency"
            description: "95th percentile latency is above 500ms for more than 5 minutes."

        - alert: HighErrorRate
          expr: rate(traefik_service_requests_total{code=~"5.."}[5m]) / rate(traefik_service_requests_total[5m]) > 0.1
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "High error rate"
            description: "Error rate is above 10% for more than 5 minutes."

        - alert: FlaresolverrDown
          expr: up{job="flaresolverr"} == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Flaresolverr is down"
            description: "Flaresolverr has been down for more than 5 minutes."

        - alert: RedisDown
          expr: up{job="redis"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Redis is down"
            description: "Redis has been down for more than 5 minutes."

        - alert: MongoDBDown
          expr: up{job="mongodb"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "MongoDB is down"
            description: "MongoDB has been down for more than 5 minutes."

        - alert: HomepageDown
          expr: up{job="homepage"} == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Homepage is down"
            description: "Homepage has been down for more than 5 minutes."

        - alert: PortainerDown
          expr: up{job="portainer"} == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Portainer is down"
            description: "Portainer has been down for more than 5 minutes."

        - alert: HedgedocDown
          expr: up{job="hedgedoc"} == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Hedgedoc is down"
            description: "Hedgedoc has been down for more than 5 minutes."

        - alert: SearxngDown
          expr: up{job="searxng"} == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "SearxNG is down"
            description: "SearxNG has been down for more than 5 minutes."

        - alert: CodeServerDown
          expr: up{job="code-server"} == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Code Server is down"
            description: "Code Server has been down for more than 5 minutes."

        - alert: DozzleDown
          expr: up{job="dozzle"} == 0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Dozzle is down"
            description: "Dozzle has been down for more than 5 minutes."

        - alert: BolabadeneNextjsDown
          expr: up{job="bolabaden-nextjs"} == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Bolabaden NextJS is down"
            description: "Bolabaden NextJS has been down for more than 5 minutes."

        - alert: BlackboxProbeFailure
          expr: probe_success == 0
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Blackbox probe failure"
            description: "Probe of $$labels.instance has been failing for more than 5 minutes."

        - alert: HighTraefikLatency
          expr: histogram_quantile(0.99, rate(traefik_service_request_duration_seconds_bucket[5m])) > 2
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "High Traefik latency"
            description: "99th percentile latency is above 2s for more than 10 minutes."

        - alert: CrowdsecHighBanRate
          expr: increase(cs_lapi_decision_total{type="ban"}[1h]) > 100
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High CrowdSec ban rate"
            description: "More than 100 bans in the last hour."

        - alert: VictoriaMetricsHighIngestionRate
          expr: rate(vm_rows_inserted_total[5m]) > 1000000
          for: 10m
          labels:
            severity: info
          annotations:
            summary: "High VictoriaMetrics ingestion rate"
            description: "Ingestion rate is above 1M datapoints/sec for more than 10 minutes."

        - alert: VictoriaMetricsSlowInserts
          expr: rate(vm_slow_row_inserts_total[5m]) / rate(vm_rows_added_to_storage_total[5m]) > 0.1
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "VictoriaMetrics slow inserts"
            description: "More than 10% of inserts are slow, indicating possible memory pressure."

        - alert: ContainerHighCPUUsage
          expr: rate(container_cpu_usage_seconds_total[5m]) * 100 > 80
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Container high CPU usage"
            description: "Container $$labels.name has high CPU usage (>80%) for more than 5 minutes."

        - alert: ContainerHighMemoryUsage
          expr: (container_memory_usage_bytes / container_spec_memory_limit_bytes) * 100 > 90
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Container high memory usage"
            description: "Container $$labels.name has high memory usage (>90%) for more than 5 minutes."

        - alert: ContainerRestartLoop
          expr: increase(container_last_seen[1h]) > 5
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Container restart loop"
            description: "Container $$labels.name has restarted more than 5 times in the last hour."
  blackbox-dashboard.json:
    name: my-media-stack_blackbox-dashboard.json
    content: |
      {
        "__inputs": [
          {
            "name": "DS_PROMETHEUS",
            "label": "prometheus",
            "description": "",
            "type": "datasource",
            "pluginId": "prometheus",
            "pluginName": "Prometheus"
          }
        ],
        "__requires": [
          {
            "type": "grafana",
            "id": "grafana",
            "name": "Grafana",
            "version": "5.2.2"
          },
          {
            "type": "panel",
            "id": "timeseries",
            "name": "Time series",
            "version": ""
          },
          {
            "type": "datasource",
            "id": "prometheus",
            "name": "Prometheus",
            "version": "5.0.0"
          },
          {
            "type": "panel",
            "id": "stat",
            "name": "Stat",
            "version": ""
          }
        ],
        "annotations": {
          "list": [
            {
              "builtIn": 1,
              "datasource": "-- Grafana --",
              "enable": true,
              "hide": true,
              "iconColor": "rgba(0, 211, 255, 1)",
              "name": "Annotations & Alerts",
              "type": "dashboard"
            }
          ]
        },
        "description": "Prometheus Blackbox Exporter Overview",
        "editable": true,
        "gnetId": 7587,
        "graphTooltip": 0,
        "id": null,
        "panels": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "victoriametrics_uid"
            },
            "fieldConfig": {
              "defaults": {
                "color": {
                  "mode": "palette-classic"
                },
                "custom": {
                  "axisBorderShow": false,
                  "axisCenteredZero": false,
                  "axisColorMode": "text",
                  "axisLabel": "",
                  "axisPlacement": "auto",
                  "barAlignment": 0,
                  "barWidthFactor": 0.6,
                  "drawStyle": "line",
                  "fillOpacity": 10,
                  "gradientMode": "none",
                  "hideFrom": {
                    "legend": false,
                    "tooltip": false,
                    "viz": false
                  },
                  "insertNulls": false,
                  "lineInterpolation": "linear",
                  "lineWidth": 1,
                  "pointSize": 5,
                  "scaleDistribution": {
                    "type": "linear"
                  },
                  "showPoints": "never",
                  "spanNulls": false,
                  "stacking": {
                    "group": "A",
                    "mode": "none"
                  },
                  "thresholdsStyle": {
                    "mode": "off"
                  }
                },
                "mappings": [],
                "thresholds": {
                  "mode": "absolute",
                  "steps": [
                    {
                      "color": "green"
                    },
                    {
                      "color": "red",
                      "value": 80
                    }
                  ]
                },
                "unit": "s"
              },
              "overrides": []
            },
            "gridPos": {
              "h": 8,
              "w": 24,
              "x": 0,
              "y": 0
            },
            "id": 138,
            "options": {
              "legend": {
                "calcs": [],
                "displayMode": "list",
                "placement": "bottom",
                "showLegend": true
              },
              "tooltip": {
                "hideZeros": false,
                "mode": "multi",
                "sort": "desc"
              }
            },
            "targets": [
              {
                "expr": "probe_duration_seconds{instance=~\"$$target\"}",
                "format": "time_series",
                "interval": "$$interval",
                "intervalFactor": 1,
                "legendFormat": "{{ instance }}",
                "refId": "A"
              }
            ],
            "title": "Global Probe Duration",
            "type": "timeseries"
          }
        ],
        "refresh": "10s",
        "schemaVersion": 16,
        "style": "dark",
        "tags": [
          "blackbox",
          "prometheus"
        ],
        "templating": {
          "list": [
            {
              "auto": true,
              "auto_count": 10,
              "auto_min": "10s",
              "current": {
                "text": "10s",
                "value": "10s"
              },
              "hide": 0,
              "label": "Interval",
              "name": "interval",
              "options": [
                {
                  "selected": false,
                  "text": "auto",
                  "value": "$$__auto_interval_interval"
                },
                {
                  "selected": false,
                  "text": "5s",
                  "value": "5s"
                },
                {
                  "selected": true,
                  "text": "10s",
                  "value": "10s"
                }
              ],
              "query": "5s,10s,30s,1m,10m,30m,1h,6h,12h,1d,7d,14d,30d",
              "refresh": 2,
              "type": "interval"
            },
            {
              "allValue": null,
              "current": {},
              "datasource": "victoriametrics_uid",
              "hide": 0,
              "includeAll": true,
              "label": null,
              "multi": true,
              "name": "target",
              "options": [],
              "query": "label_values(probe_success, instance)",
              "refresh": 1,
              "regex": "",
              "sort": 0,
              "type": "query"
            }
          ]
        },
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "timezone": "",
        "title": "Blackbox Exporter",
        "uid": "blackbox_exporter",
        "version": 1
      }
  blackbox.yml:
    name: my-media-stack_blackbox.yml
    content: |
      modules:
        http_2xx:
          prober: http
          timeout: 5s
          http:
            valid_http_versions: ["HTTP/1.1", "HTTP/2.0"]
            valid_status_codes: []  # Defaults to 2xx
            method: GET
            headers:
              Host: vhost.bolabaden.org
              Accept-Language: en-US
            no_follow_redirects: false
            fail_if_ssl: false
            fail_if_not_ssl: false
            fail_if_body_matches_regexp:
              - "Could not connect to database"
            fail_if_body_not_matches_regexp:
              - "Download the latest version here"
            fail_if_header_matches: # Verifies that no cookies are set
              - header: Set-Cookie
                allow_missing: true
                regexp: '.*'
            fail_if_header_not_matches:
              - header: Access-Control-Allow-Origin
                regexp: '(\*|bolabaden.org)'
            tls_config:
              insecure_skip_verify: false
            preferred_ip_protocol: "ip4" # defaults to "ip6"
            ip_protocol_fallback: false  # no fallback to "ip6"
        http_post_2xx:
          prober: http
          timeout: 5s
          http:
            method: POST
            headers:
              Content-Type: application/json
            body: '{}'
        tcp_connect:
          prober: tcp
          timeout: 5s
        pop3s_banner:
          prober: tcp
          tcp:
            query_response:
              - expect: "^+OK"
            tls: true
            tls_config:
              insecure_skip_verify: false
        grpc:
          prober: grpc
          grpc:
            tls: true
            preferred_ip_protocol: "ip4"
        grpc_plain:
          prober: grpc
          grpc:
            tls: false
            service: "service1"
        ssh_banner:
          prober: tcp
          timeout: 5s
          tcp:
            query_response:
              - expect: "^SSH-2.0-"
            tls: false
        irc_banner:
          prober: tcp
          timeout: 5s
          tcp:
            query_response:
              - send: "NICK prober"
              - send: "USER prober prober prober :prober"
              - expect: "PING :([^ ]+)"
                send: "PONG :$$1"
              - expect: "^:[^ ]+ 001"
        icmp:
          prober: icmp
          timeout: 5s
          icmp:
            preferred_ip_protocol: "ip4"
            source_ip_address: "127.0.0.1"
        dns_udp:
          prober: dns
          timeout: 5s
          dns:
            query_name: "bolabaden.org"
            query_type: "A"
            valid_rcodes:
            - NOERROR
            validate_answer_rrs:
              fail_if_matches_regexp:
              - ".*127.0.0.1"
              fail_if_all_match_regexp:
              - ".*127.0.0.1"
              fail_if_not_matches_regexp:
              - "www.prometheus.io.\t300\tIN\tCNAME\tprometheus.io."
              - "prometheus.io.\t300\tIN\tA\t46.101.169.55"
            validate_authority_rrs:
              fail_if_matches_regexp:
              - ".*127.0.0.1"
            validate_additional_rrs:
              fail_if_matches_regexp:
              - ".*127.0.0.1"
        dns_soa:
          prober: dns
          dns:
            query_name: "prometheus.io"
            query_type: "SOA"
        dns_tcp:
          prober: dns
          dns:
            transport_protocol: "tcp" # defaults to "udp"
            preferred_ip_protocol: "ip4" # defaults to "ip6"
            query_name: "prometheus.io"
            query_type: "A"
  cadvisor-dashboard.json:
    name: my-media-stack_cadvisor-dashboard.json
    content: |
      {
        "__inputs": [
          {
            "name": "DS_PROMETHEUS",
            "label": "Prometheus",
            "description": "Prometheus as the datasource is obligatory",
            "type": "datasource",
            "pluginId": "prometheus",
            "pluginName": "Prometheus"
          }
        ],
        "__requires": [
          {
            "type": "grafana",
            "id": "grafana",
            "name": "Grafana",
            "version": "7.4.5"
          },
          {
            "type": "panel",
            "id": "timeseries",
            "name": "Time series",
            "version": ""
          },
          {
            "type": "datasource",
            "id": "prometheus",
            "name": "Prometheus",
            "version": "1.0.0"
          },
          {
            "type": "panel",
            "id": "table",
            "name": "Table",
            "version": ""
          }
        ],
        "annotations": {
          "list": [
            {
              "builtIn": 1,
              "datasource": "-- Grafana --",
              "enable": true,
              "hide": true,
              "iconColor": "rgba(0, 211, 255, 1)",
              "name": "Annotations & Alerts",
              "type": "dashboard"
            }
          ]
        },
        "editable": true,
        "gnetId": 14282,
        "graphTooltip": 0,
        "id": null,
        "panels": [
          {
            "collapsed": false,
            "datasource": "victoriametrics_uid",
            "gridPos": {
              "h": 1,
              "w": 24,
              "x": 0,
              "y": 0
            },
            "id": 8,
            "panels": [],
            "title": "CPU",
            "type": "row"
          },
          {
            "datasource": {
              "type": "prometheus",
              "uid": "victoriametrics_uid"
            },
            "fieldConfig": {
              "defaults": {
                "color": {
                  "mode": "palette-classic"
                },
                "custom": {
                  "axisBorderShow": false,
                  "axisCenteredZero": false,
                  "axisColorMode": "text",
                  "axisLabel": "",
                  "axisPlacement": "auto",
                  "barAlignment": 0,
                  "barWidthFactor": 0.6,
                  "drawStyle": "line",
                  "fillOpacity": 40,
                  "gradientMode": "none",
                  "hideFrom": {
                    "legend": false,
                    "tooltip": false,
                    "viz": false
                  },
                  "insertNulls": false,
                  "lineInterpolation": "linear",
                  "lineWidth": 1,
                  "pointSize": 5,
                  "scaleDistribution": {
                    "type": "linear"
                  },
                  "showPoints": "never",
                  "spanNulls": false,
                  "stacking": {
                    "group": "A",
                    "mode": "normal"
                  },
                  "thresholdsStyle": {
                    "mode": "off"
                  }
                },
                "mappings": [],
                "thresholds": {
                  "mode": "absolute",
                  "steps": [
                    {
                      "color": "green"
                    },
                    {
                      "color": "red",
                      "value": 80
                    }
                  ]
                },
                "unit": "percent"
              },
              "overrides": []
            },
            "gridPos": {
              "h": 7,
              "w": 24,
              "x": 0,
              "y": 1
            },
            "id": 15,
            "options": {
              "legend": {
                "calcs": [],
                "displayMode": "list",
                "placement": "bottom",
                "showLegend": true,
                "width": 250
              },
              "tooltip": {
                "hideZeros": false,
                "mode": "multi",
                "sort": "desc"
              }
            },
            "targets": [
              {
                "expr": "sum(rate(container_cpu_usage_seconds_total{instance=~\"$$host\",name=~\"$$container\",name=~\".+\"}[5m])) by (name) *100",
                "hide": false,
                "interval": "",
                "legendFormat": "{{name}}",
                "refId": "A"
              }
            ],
            "title": "CPU Usage",
            "type": "timeseries"
          }
        ],
        "schemaVersion": 27,
        "style": "dark",
        "tags": [
          "cadvisor",
          "docker"
        ],
        "templating": {
          "list": [
            {
              "allValue": ".*",
              "current": {},
              "datasource": "victoriametrics_uid",
              "definition": "label_values({__name__=~\"container.*\"},instance)",
              "description": null,
              "error": null,
              "hide": 0,
              "includeAll": true,
              "label": "Host",
              "multi": false,
              "name": "host",
              "options": [],
              "query": {
                "query": "label_values({__name__=~\"container.*\"},instance)",
                "refId": "Prometheus-host-Variable-Query"
              },
              "refresh": 1,
              "regex": "",
              "skipUrlSync": false,
              "sort": 5,
              "tagValuesQuery": "",
              "tags": [],
              "tagsQuery": "",
              "type": "query",
              "useTags": false
            },
            {
              "allValue": ".*",
              "current": {},
              "datasource": "victoriametrics_uid",
              "definition": "label_values({__name__=~\"container.*\", instance=~\"$$host\"},name)",
              "description": null,
              "error": null,
              "hide": 0,
              "includeAll": true,
              "label": "Container",
              "multi": false,
              "name": "container",
              "options": [],
              "query": {
                "query": "label_values({__name__=~\"container.*\", instance=~\"$$host\"},name)",
                "refId": "Prometheus-container-Variable-Query"
              },
              "refresh": 1,
              "regex": "",
              "skipUrlSync": false,
              "sort": 0,
              "tagValuesQuery": "",
              "tags": [],
              "tagsQuery": "",
              "type": "query",
              "useTags": false
            }
          ]
        },
        "time": {
          "from": "now-6h",
          "to": "now"
        },
        "timepicker": {},
        "timezone": "",
        "title": "Cadvisor Container Monitoring",
        "uid": "cadvisor_monitoring",
        "version": 1,
        "description": "Container monitoring with cAdvisor"
      }
  crowdsec-acquis.yaml:
    name: my-media-stack_crowdsec-acquis.yaml
    content: |
      filenames:
        - /var/log/auth.log
        - /var/log/syslog
      labels:
        type: syslog
      ---
      poll_without_inotify: false
      filenames:
        - /var/log/traefik/*.log
      #  - /var/log/traefik/access.log
      labels:
        type: traefik
  crowdsec-auth.log:
    name: my-media-stack_crowdsec-auth.log
    file: /home/ubuntu/my-media-stack/volumes/traefik/crowdsec/var/log/auth.log
  crowdsec-config.yaml:
    name: my-media-stack_crowdsec-config.yaml
    content: |
      common:
        log_media: stdout
        log_level: info
        log_dir: /var/log/
      config_paths:
        config_dir: /etc/crowdsec/
        data_dir: /var/lib/crowdsec/data/
        simulation_path: /etc/crowdsec/simulation.yaml
        hub_dir: /etc/crowdsec/hub/
        index_path: /etc/crowdsec/hub/.index.json
        notification_dir: /etc/crowdsec/notifications/
        plugin_dir: /usr/local/lib/crowdsec/plugins/
      crowdsec_service:
        acquisition_path: /etc/crowdsec/acquis.yaml
        acquisition_dir: /etc/crowdsec/acquis.d
        parser_routines: 1
      plugin_config:
        user: nobody
        group: nobody
      cscli:
        output: human
      db_config:
        log_level: info
        type: sqlite
        db_path: /var/lib/crowdsec/data/crowdsec.db
        flush:
          max_items: 5000
          max_age: 7d
        use_wal: false
      api:
        client:
          insecure_skip_verify: false
          credentials_path: /etc/crowdsec/local_api_credentials.yaml
        server:
          log_level: info
          listen_uri: 0.0.0.0:8080
          profiles_path: /etc/crowdsec/profiles.yaml
          trusted_ips: # IP ranges, or IPs which can have admin API access
            - 127.0.0.1
            - ::1
          online_client: # Central API credentials (to push signals and receive bad IPs)
            credentials_path: /etc/crowdsec//online_api_credentials.yaml
          enable: true
      prometheus:
        enabled: true
        level: full
        listen_addr: 0.0.0.0
        listen_port: 6060
  crowdsec-dashboard.json:
    name: my-media-stack_crowdsec-dashboard.json
    content: |
      {
        "annotations": {
          "list": [
            {
              "builtIn": 1,
              "datasource": "-- Grafana --",
              "enable": true,
              "hide": true,
              "iconColor": "rgba(0, 211, 255, 1)",
              "name": "Annotations & Alerts",
              "type": "dashboard"
            }
          ]
        },
        "description": "CrowdSec security monitoring",
        "editable": true,
        "gnetId": null,
        "graphTooltip": 0,
        "id": null,
        "panels": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "victoriametrics_uid"
            },
            "fieldConfig": {
              "defaults": {
                "color": {
                  "mode": "thresholds"
                },
                "mappings": [],
                "thresholds": {
                  "mode": "absolute",
                  "steps": [
                    {
                      "color": "green"
                    },
                    {
                      "color": "red",
                      "value": 1
                    }
                  ]
                },
                "unit": "short"
              },
              "overrides": []
            },
            "gridPos": {
              "h": 4,
              "w": 6,
              "x": 0,
              "y": 0
            },
            "id": 1,
            "options": {
              "colorMode": "value",
              "graphMode": "area",
              "justifyMode": "auto",
              "orientation": "auto",
              "reduceOptions": {
                "calcs": [
                  "lastNotNull"
                ],
                "fields": "",
                "values": false
              },
              "textMode": "auto",
              "wideLayout": true
            },
            "targets": [
              {
                "expr": "cs_lapi_decision{type=\"ban\"}",
                "format": "time_series",
                "intervalFactor": 1,
                "legendFormat": "Active Bans",
                "refId": "A"
              }
            ],
            "title": "Active Bans",
            "type": "stat"
          }
        ],
        "refresh": "30s",
        "schemaVersion": 27,
        "style": "dark",
        "tags": [
          "crowdsec",
          "security"
        ],
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "timezone": "",
        "title": "CrowdSec Security",
        "uid": "crowdsec_security",
        "version": 1
      }
  crowdsec-email.yaml:
    name: my-media-stack_crowdsec-email.yaml
    content: "type: email           # Don't change\nname: email_default   # Must match the registered plugin in the profile\n\n# One of \"trace\", \"debug\", \"info\", \"warn\", \"error\", \"off\"\nlog_level: info\n\n# group_wait:         # Time to wait collecting alerts before relaying a message to this plugin, eg \"30s\"\n# group_threshold:    # Amount of alerts that triggers a message before <group_wait> has expired, eg \"10\"\n# max_retry:          # Number of attempts to relay messages to plugins in case of error\ntimeout: 20s          # Time to wait for response from the plugin before considering the attempt a failure, eg \"10s\"\n\n#-------------------------\n# plugin-specific options\n\n# The following template receives a list of models.Alert objects\n# The output goes in the email message body\nformat: |\n  <html><body>\n  {{range . -}}\n    {{$$alert := . -}}\n    {{range .Decisions -}}\n      <p><a href=\"https://www.whois.com/whois/{{.Value}}\">{{.Value}}</a> will get <b>{{.Type}}</b> for next <b>{{.Duration}}</b> for triggering <b>{{.Scenario}}</b> on machine <b>{{$$alert.MachineID}}</b>.</p> <p><a href=\"https://app.crowdsec.net/cti/{{.Value}}\">CrowdSec CTI</a></p>\n    {{end -}}\n  {{end -}}\n  </body></html>\n\nsmtp_host: smtp.gmail.com  # example: smtp.gmail.com\nsmtp_username: boden.crouch@gmail.com\nsmtp_password: nbru aavt hmcx veqs\nsmtp_port: 587   # Common values are any of [25, 465, 587, 2525]\nauth_type: login   # Valid choices are \"none\", \"crammd5\", \"login\", \"plain\"\nsender_name: \"CrowdSec\"\nsender_email: \nemail_subject: \"CrowdSec Security Alert\"\nreceiver_emails:\n  - admin@localhost\n  - boden.crouch@gmail.com\n# - email1@gmail.com\n# - email2@gmail.com\n\n# One of \"ssltls\", \"starttls\", \"none\"\nencryption_type: \"ssltls\"\n\n# If you need to set the HELO hostname:\n# helo_host: \"localhost\"\n\n# If the email server is hitting the default timeouts (10 seconds), you can increase them here\n#\n# connect_timeout: 10s\n# send_timeout: 10s\n\n---\n\n# type: email\n# name: email_second_notification\n# ...\n"
  crowdsec-file.yaml:
    name: my-media-stack_crowdsec-file.yaml
    content: |
      # Don't change this
      type: file

      name: file_default  # this must match with the registered plugin in the profile
      log_level: info  # Options include: trace, debug, info, warn, error, off

      # This template render all events as ndjson
      format: |
        {{range . -}}
        { "time": "{{.StopAt}}", "program": "crowdsec", "alert": {{. | toJson }} }
        {{ end -}}

      group_wait: 30s  # duration to wait collecting alerts before sending to this plugin
      group_threshold: 10  # if alerts exceed this, then the plugin will be sent the message

      # Use full path EG /tmp/crowdsec_alerts.json or %TEMP%\crowdsec_alerts.json
      log_path: "/tmp/crowdsec_alerts.json"
      rotate:
        enabled: true  # Change to false if you want to handle log rotate on system basis
        max_size: 500  # in MB
        max_files: 5
        max_age: 5
        compress: true
  crowdsec-profiles.yaml:
    name: my-media-stack_crowdsec-profiles.yaml
    content: |
      name: default_ip_remediation
      #debug: true
      filters:
      - Alert.Remediation == true && Alert.GetScope() == "Ip"
      decisions:
      - type: ban
        duration: 4h
      #duration_expr: Sprintf('%dh', (GetDecisionsCount(Alert.GetValue()) + 1) * 4)
      #notifications:
      #   - email_default         # Set the required email parameters in /etc/crowdsec/notifications/email.yaml before enabling this.
      #   - http_victoriametrics  # Set the required http parameters in /etc/crowdsec/notifications/http.yaml before enabling this.
      #   - slack_default         # Set the webhook in /etc/crowdsec/notifications/slack.yaml before enabling this.
      #   - splunk_default        # Set the splunk url and token in /etc/crowdsec/notifications/splunk.yaml before enabling this.
      #   - http_default          # Set the required http parameters in /etc/crowdsec/notifications/http.yaml before enabling this.
      on_success: break
      ---
      name: default_range_remediation
      #debug: true
      filters:
      - Alert.Remediation == true && Alert.GetScope() == "Range"
      decisions:
      - type: ban
        duration: 4h
      #duration_expr: Sprintf('%dh', (GetDecisionsCount(Alert.GetValue()) + 1) * 4)
      #notifications:
      #   - email_default         # Set the required email parameters in /etc/crowdsec/notifications/email.yaml before enabling this.
      #   - http_victoriametrics  # Set the required http parameters in /etc/crowdsec/notifications/http.yaml before enabling this.
      #   - slack_default         # Set the webhook in /etc/crowdsec/notifications/slack.yaml before enabling this.
      #   - splunk_default        # Set the splunk url and token in /etc/crowdsec/notifications/splunk.yaml before enabling this.
      #   - http_default          # Set the required http parameters in /etc/crowdsec/notifications/http.yaml before enabling this.
      on_success: break
  crowdsec-sentinel.yaml:
    name: my-media-stack_crowdsec-sentinel.yaml
    content: |
      type: sentinel          # Don't change
      name: sentinel_default  # Must match the registered plugin in the profile

      # One of "trace", "debug", "info", "warn", "error", "off"
      log_level: info
      # group_wait:         # Time to wait collecting alerts before relaying a message to this plugin, eg "30s"
      # group_threshold:    # Amount of alerts that triggers a message before <group_wait> has expired, eg "10"
      # max_retry:          # Number of attempts to relay messages to plugins in case of error
      # timeout:            # Time to wait for response from the plugin before considering the attempt a failure, eg "10s"

      #-------------------------
      # plugin-specific options

      # The following template receives a list of models.Alert objects
      # The output goes in the http request body
      format: |
        {{.|toJson}}

      customer_id: XXX-XXX
      shared_key: XXXXXXX
  crowdsec-splunk.yaml:
    name: my-media-stack_crowdsec-splunk.yaml
    content: |
      type: splunk          # Don't change
      name: splunk_default  # Must match the registered plugin in the profile

      # One of "trace", "debug", "info", "warn", "error", "off"
      log_level: info

      # group_wait:         # Time to wait collecting alerts before relaying a message to this plugin, eg "30s"
      # group_threshold:    # Amount of alerts that triggers a message before <group_wait> has expired, eg "10"
      # max_retry:          # Number of attempts to relay messages to plugins in case of error
      # timeout:            # Time to wait for response from the plugin before considering the attempt a failure, eg "10s"

      #-------------------------
      # plugin-specific options

      # The following template receives a list of models.Alert objects
      # The output goes in the splunk notification
      format: |
        {{.|toJson}}

      url: <SPLUNK_HTTP_URL>
      token: <SPLUNK_TOKEN>

      ---

      # type: splunk
      # name: splunk_second_notification
      # ...
  crowdsec-syslog:
    name: my-media-stack_crowdsec-syslog
    file: /home/ubuntu/my-media-stack/volumes/traefik/crowdsec/var/log/syslog
  crowdsec-victoriametrics.yaml:
    name: my-media-stack_crowdsec-victoriametrics.yaml
    content: |
      type: http
      name: http_victoriametrics
      log_level: debug
      format: >
        {{- range $$Alert := . -}}
        {{- $$traefikRouters := GetMeta . "traefik_router_name" -}}
        {{- range .Decisions -}}
        {"metric":{"__name__":"cs_lapi_decision","instance":"my-instance","country":"{{$$Alert.Source.Cn}}","asname":"{{$$Alert.Source.AsName}}","asnumber":"{{$$Alert.Source.AsNumber}}","latitude":"{{$$Alert.Source.Latitude}}","longitude":"{{$$Alert.Source.Longitude}}","iprange":"{{$$Alert.Source.Range}}","scenario":"{{.Scenario}}","type":"{{.Type}}","duration":"{{.Duration}}","scope":"{{.Scope}}","ip":"{{.Value}}","traefik_routers":{{ printf "%q" ($$traefikRouters | uniq | join ",")}}},"values": [1],"timestamps":[{{now|unixEpoch}}000]}
        {{- end }}
        {{- end -}}
      url: http://victoriametrics:8428/api/v1/import
      method: POST
      headers:
        Content-Type: application/json
        # if you use vmauth as proxy, please uncomment next line and add your token
        # If you would like to add authentication, please read about vmauth.
        # https://docs.victoriametrics.com/victoriametrics/vmauth/?ref=blog.lrvt.de#bearer-token-auth-proxy
        # It's basically another Docker container service, which acts as proxy in front of VictoriaMetrics and enforces Bearer HTTP Authentication.
        # Authorization: ""
  flaresolverr-dashboard.json:
    name: my-media-stack_flaresolverr-dashboard.json
    content: |
      {
        "annotations": {
          "list": [
            {
              "builtIn": 1,
              "datasource": "-- Grafana --",
              "enable": true,
              "hide": true,
              "iconColor": "rgba(0, 211, 255, 1)",
              "name": "Annotations & Alerts",
              "type": "dashboard"
            }
          ]
        },
        "description": "Flaresolverr monitoring",
        "editable": true,
        "gnetId": null,
        "graphTooltip": 0,
        "id": null,
        "panels": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "victoriametrics_uid"
            },
            "fieldConfig": {
              "defaults": {
                "color": {
                  "mode": "palette-classic"
                },
                "custom": {
                  "axisBorderShow": false,
                  "axisCenteredZero": false,
                  "axisColorMode": "text",
                  "axisLabel": "",
                  "axisPlacement": "auto",
                  "barAlignment": 0,
                  "barWidthFactor": 0.6,
                  "drawStyle": "line",
                  "fillOpacity": 10,
                  "gradientMode": "none",
                  "hideFrom": {
                    "legend": false,
                    "tooltip": false,
                    "viz": false
                  },
                  "insertNulls": false,
                  "lineInterpolation": "linear",
                  "lineWidth": 1,
                  "pointSize": 5,
                  "scaleDistribution": {
                    "type": "linear"
                  },
                  "showPoints": "never",
                  "spanNulls": false,
                  "stacking": {
                    "group": "A",
                    "mode": "none"
                  },
                  "thresholdsStyle": {
                    "mode": "off"
                  }
                },
                "mappings": [],
                "thresholds": {
                  "mode": "absolute",
                  "steps": [
                    {
                      "color": "green"
                    },
                    {
                      "color": "red",
                      "value": 80
                    }
                  ]
                },
                "unit": "reqps"
              },
              "overrides": []
            },
            "gridPos": {
              "h": 8,
              "w": 12,
              "x": 0,
              "y": 0
            },
            "id": 1,
            "options": {
              "legend": {
                "calcs": [],
                "displayMode": "list",
                "placement": "bottom",
                "showLegend": true
              },
              "tooltip": {
                "hideZeros": false,
                "mode": "multi",
                "sort": "none"
              }
            },
            "targets": [
              {
                "expr": "rate(flaresolverr_requests_total[5m])",
                "format": "time_series",
                "intervalFactor": 1,
                "legendFormat": "Requests",
                "refId": "A"
              }
            ],
            "title": "Flaresolverr Request Rate",
            "type": "timeseries"
          }
        ],
        "refresh": "30s",
        "schemaVersion": 27,
        "style": "dark",
        "tags": [
          "flaresolverr"
        ],
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "timezone": "",
        "title": "Flaresolverr Monitoring",
        "uid": "flaresolverr_monitoring",
        "version": 1
      }
  fuse.conf:
    name: my-media-stack_fuse.conf
    file: /etc/fuse.conf
  gethomepage-bookmarks.yaml:
    name: my-media-stack_gethomepage-bookmarks.yaml
    content: |
      ---
      # For configuration options and examples, please see:
      # https://gethomepage.dev/configs/bookmarks

      - Developer:
        - Github:
          - abbr: GH
            href: https://github.com/bolabaden

      - Portfolio:
        - bolabaden.org:
          - abbr: BO
            href: https://bolabaden.org/
        - LinkedIn:
          - abbr: LI
            href: https://www.linkedin.com/in/boden-crouch-555897193/

      - AI/Research:
        - ai-researchwizard:
          - abbr: GP
            href: https://gptr.bolabaden.org/
        - searxng:
          - abbr: SE
            href: https://searxng.bolabaden.org/
  gethomepage-custom.css:
    name: my-media-stack_gethomepage-custom.css
    content: |
      /* Custom CSS for bolabaden.org - beatapostapita */
  gethomepage-custom.js:
    name: my-media-stack_gethomepage-custom.js
    content: |
      /* Custom JavaScript for Bolabaden */
  gethomepage-docker.yaml:
    name: my-media-stack_gethomepage-docker.yaml
    content: |
      ---
      # For configuration options and examples, please see:
      # https://gethomepage.dev/configs/docker/

      my-docker:
      #  socket: /var/run/docker.sock
        host: dockerproxy-ro
        port: 2375
  gethomepage-settings.yaml:
    name: my-media-stack_gethomepage-settings.yaml
    content: "---\n# For configuration options and examples, please see:\n# https://gethomepage.dev/configs/settings/\n\n# Weather providers disabled - API keys not configured\n# providers:\n#   openweathermap: \n#   weatherapi: \n"
  gethomepage-widgets.yaml:
    name: my-media-stack_gethomepage-widgets.yaml
    content: |
      ---
      # For configuration options and examples, please see:
      # https://gethomepage.dev/configs/info-widgets/

      - resources:
          cpu: true
          memory: true
          disk: /

      - search:
          provider: duckduckgo
          target: _blank
  grafana-alerting.yaml:
    name: my-media-stack_grafana-alerting.yaml
    content: |
      apiVersion: 1

      groups:
        - orgId: 1
          name: Homelab Rules
          folder: Alerts
          interval: 60s
          rules:
            - uid: high_cpu
              title: High CPU
              condition: A
              data:
                - refId: A
                  datasourceUid: prom_uid
                  relativeTimeRange:
                    from: 300
                    to: 0
                  model:
                    expr: avg(rate(node_cpu_seconds_total{mode!="idle"}[5m])) * 100
                    intervalMs: 60000
                    maxDataPoints: 43200
              for: 5m
              annotations:
                summary: High CPU usage detected
              labels:
                severity: critical
              noDataState: NoData
              execErrState: Error
              isPaused: false
              thresholds:
                - value: 80
                  op: gt

            - uid: high_mem
              title: High Memory
              condition: A
              data:
                - refId: A
                  datasourceUid: prom_uid
                  relativeTimeRange:
                    from: 300
                    to: 0
                  model:
                    expr: (node_memory_MemTotal_bytes - node_memory_MemAvailable_bytes) / node_memory_MemTotal_bytes * 100
                    intervalMs: 60000
                    maxDataPoints: 43200
              for: 5m
              annotations:
                summary: High memory usage detected
              labels:
                severity: critical
              noDataState: NoData
              execErrState: Error
              isPaused: false
              thresholds:
                - value: 90
                  op: gt

            - uid: low_disk
              title: Low Disk
              condition: A
              data:
                - refId: A
                  datasourceUid: prom_uid
                  relativeTimeRange:
                    from: 300
                    to: 0
                  model:
                    expr: 100 - (node_filesystem_avail_bytes / node_filesystem_size_bytes * 100)
                    intervalMs: 60000
                    maxDataPoints: 43200
              for: 5m
              annotations:
                summary: Low disk space detected
              labels:
                severity: warning
              noDataState: NoData
              execErrState: Error
              isPaused: false
              thresholds:
                - value: 90
                  op: gt

            - uid: high_temp
              title: High Temp
              condition: A
              data:
                - refId: A
                  datasourceUid: prom_uid
                  relativeTimeRange:
                    from: 300
                    to: 0
                  model:
                    expr: node_hwmon_temp_celsius
                    intervalMs: 60000
                    maxDataPoints: 43200
              for: 5m
              annotations:
                summary: High temperature detected
              labels:
                severity: warning
              noDataState: NoData
              execErrState: Error
              isPaused: false
              thresholds:
                - value: 70
                  op: gt

            - uid: high_latency
              title: High Latency
              condition: A
              data:
                - refId: A
                  datasourceUid: prom_uid
                  relativeTimeRange:
                    from: 300
                    to: 0
                  model:
                    expr: probe_duration_seconds
                    intervalMs: 60000
                    maxDataPoints: 43200
              for: 5m
              annotations:
                summary: High probe latency detected
              labels:
                severity: warning
              noDataState: NoData
              execErrState: Error
              isPaused: false
              thresholds:
                - value: 0.1
                  op: gt

            - uid: container_restart
              title: Container Restart
              condition: A
              data:
                - refId: A
                  datasourceUid: prom_uid
                  relativeTimeRange:
                    from: 60
                    to: 0
                  model:
                    expr: changes(kube_pod_container_status_restarts[5m])
                    intervalMs: 60000
                    maxDataPoints: 43200
              for: 1m
              annotations:
                summary: Container restart detected
              labels:
                severity: warning
              noDataState: NoData
              execErrState: Error
              isPaused: false
              thresholds:
                - value: 0
                  op: gt

            - uid: slow_query
              title: Slow DB Query
              condition: A
              data:
                - refId: A
                  datasourceUid: mysql_uid
                  relativeTimeRange:
                    from: 300
                    to: 0
                  model:
                    expr: mysql_global_status_slow_queries
                    intervalMs: 60000
                    maxDataPoints: 43200
              for: 5m
              annotations:
                summary: Slow database queries detected
              labels:
                severity: warning
              noDataState: NoData
              execErrState: Error
              isPaused: false
              thresholds:
                - value: 5
                  op: gt

            - uid: high_power
              title: High Power
              condition: A
              data:
                - refId: A
                  datasourceUid: prom_uid
                  relativeTimeRange:
                    from: 300
                    to: 0
                  model:
                    expr: power_usage_watts
                    intervalMs: 60000
                    maxDataPoints: 43200
              for: 5m
              annotations:
                summary: High power usage detected
              labels:
                severity: info
              noDataState: NoData
              execErrState: Error
              isPaused: false
              thresholds:
                - value: 500
                  op: gt

            - uid: backup_fail
              title: Backup Failed
              condition: A
              data:
                - refId: A
                  datasourceUid: prom_uid
                  relativeTimeRange:
                    from: 60
                    to: 0
                  model:
                    expr: backup_status
                    intervalMs: 60000
                    maxDataPoints: 43200
              for: 1m
              annotations:
                summary: Backup failed
              labels:
                severity: critical
              noDataState: NoData
              execErrState: Error
              isPaused: false
              thresholds:
                - value: 0
                  op: eq
  grafana-dashboard.yaml:
    name: my-media-stack_grafana-dashboard.yaml
    content: |
      apiVersion: 1
      providers:
        - name: 'System Monitoring'
          orgId: 1
          folder: 'System Monitoring'
          folderUid: 'system_monitoring'
          type: file
          disableDeletion: false
          updateIntervalSeconds: 10
          allowUiUpdates: true
          options:
            path: /var/lib/grafana/dashboards/system
            foldersFromFilesStructure: false
        - name: 'Infrastructure'
          orgId: 1
          folder: 'Infrastructure'
          folderUid: 'infrastructure'
          type: file
          disableDeletion: false
          updateIntervalSeconds: 10
          allowUiUpdates: true
          options:
            path: /var/lib/grafana/dashboards/infrastructure
            foldersFromFilesStructure: false
        - name: 'Application Monitoring'
          orgId: 1
          folder: 'Application Monitoring'
          folderUid: 'app_monitoring'
          type: file
          disableDeletion: false
          updateIntervalSeconds: 10
          allowUiUpdates: true
          options:
            path: /var/lib/grafana/dashboards/apps
            foldersFromFilesStructure: false
        - name: 'Network Monitoring'
          orgId: 1
          folder: 'Network Monitoring'
          folderUid: 'network_monitoring'
          type: file
          disableDeletion: false
          updateIntervalSeconds: 10
          allowUiUpdates: true
          options:
            path: /var/lib/grafana/dashboards/network
            foldersFromFilesStructure: false
  grafana-datasource.yaml:
    name: my-media-stack_grafana-datasource.yaml
    content: |
      apiVersion: 1
      datasources:
        - name: Prometheus
          type: prometheus
          access: proxy
          uid: prometheus_uid
          url: http://prometheus:9090
          isDefault: false
          editable: false
          jsonData:
            timeInterval: "5s"
            queryTimeout: "60s"
            httpMethod: "POST"
            exemplarTraceIdDestinations:
              - name: trace_id
                datasourceUid: jaeger_uid
          secureJsonData:
            # Add basic auth if needed
            # basicAuthPassword: "password"
        - name: VictoriaMetrics
          type: prometheus
          access: proxy
          uid: victoriametrics_uid
          url: http://victoriametrics:8428
          isDefault: true
          editable: false
          jsonData:
            timeInterval: "5s"
            queryTimeout: "60s"
            httpMethod: "POST"
            customQueryParameters: ""
            manageAlerts: true
            alertmanagerUid: "alertmanager_uid"
            prometheusType: "Prometheus"
            prometheusVersion: "2.40.0"
            cacheLevel: "High"
            disableMetricsLookup: false
            incrementalQuerying: true
            incrementalQueryOverlapWindow: "10m"
          secureJsonData:
            # Add basic auth if needed for VictoriaMetrics
            # basicAuthPassword: "$${VICTORIAMETRICS_PASSWORD:-}"
        - name: VictoriaMetrics-Native
          type: victoriametrics-datasource
          access: proxy
          url: http://victoriametrics:8428
          isDefault: false
          editable: false
          jsonData:
            timeInterval: "5s"
            queryTimeout: "60s"
            httpMethod: "POST"
            lookback: "1m"
            resolution: "1/2"
          secureJsonData:
            # Add basic auth if needed
            # basicAuthPassword: ""
        - name: Loki
          type: loki
          access: proxy
          uid: loki_uid
          url: http://loki:3100
          editable: false
          jsonData:
            maxLines: 1000
            timeout: "60s"
            derivedFields:
              - matcherRegex: "traceID=(\\w+)"
                name: "TraceID"
                url: "$${__value.raw}"
                datasourceUid: "jaeger_uid"
          secureJsonData:
            # Add basic auth if needed
            # basicAuthPassword: "h4L0m4St3R327"
        - name: Alertmanager
          type: alertmanager
          access: proxy
          uid: alertmanager_uid
          url: http://alertmanager:9093
          editable: false
          jsonData:
            implementation: "prometheus"
            handleGrafanaManagedAlerts: true
          secureJsonData:
            # Add basic auth if needed
            # basicAuthPassword: ""
        - name: Flaresolverr
          type: prometheus
          access: proxy
          uid: flaresolverr_uid
          url: https://flaresolverr.bolabaden.org
          editable: false
          jsonData:
            timeInterval: "15s"
            queryTimeout: "60s"
            httpMethod: "GET"
            prometheusType: "Prometheus"
        - name: CrowdSec
          type: prometheus
          access: proxy
          uid: crowdsec_uid
          url: http://crowdsec:6060
          editable: false
          jsonData:
            timeInterval: "15s"
            queryTimeout: "60s"
            httpMethod: "GET"
            prometheusType: "Prometheus"
  grafana-notifications.yaml:
    name: my-media-stack_grafana-notifications.yaml
    content: |
      apiVersion: 1
      notifiers:
        - name: webhook-default
          type: webhook
          uid: webhook_default_uid
          settings:
            url: http://localhost:3000/webhook
            httpMethod: POST
            maxAlerts: 0
            username: ""
            password: ""
            title: "Grafana Alert"
            message: "{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}"
        - name: email-default
          type: email
          uid: email_default_uid
          settings:
            addresses: "admin@localhost"
            singleEmail: false
            subject: "Grafana Alert - {{ .GroupLabels.alertname }}"
            message: "{{ range .Alerts }}{{ .Annotations.summary }}\\n{{ .Annotations.description }}{{ end }}"
      contactPoints:
        - orgId: 1
          name: "default-contact-point"
          receivers:
            - uid: "default_receiver_uid"
              type: "email"
              settings:
                addresses: ["admin@localhost"]
                subject: "Grafana Alert Notification"
                message: |
                  {{ define \"__alert_list\" }}{{ range . }}
                  Alert: {{ .Annotations.summary }}
                  Description: {{ .Annotations.description }}
                  Labels: {{ range .Labels.SortedPairs }}{{ .Name }}={{ .Value }}, {{ end }}
                  {{ end }}{{ end }}
                  {{ if gt (len .Alerts.Firing) 0 }}**FIRING**
                  {{ template \"__alert_list\" .Alerts.Firing }}{{ end }}
                  {{ if gt (len .Alerts.Resolved) 0 }}**RESOLVED**
                  {{ template \"__alert_list\" .Alerts.Resolved }}{{ end }}
            - uid: "webhook_receiver_uid"
              type: "webhook"
              settings:
                url: "http://localhost:3000/webhook"
                httpMethod: "POST"
                maxAlerts: 0
      policies:
        - orgId: 1
          receiver: "default-contact-point"
          group_by: ["alertname", "cluster", "service"]
          group_wait: "30s"
          group_interval: "5m"
          repeat_interval: "1h"
          routes:
            - receiver: "default-contact-point"
              object_matchers:
                - ["severity", "=", "critical"]
              group_wait: "10s"
              repeat_interval: "30m"
            - receiver: "default-contact-point"
              object_matchers:
                - ["severity", "=", "warning"]
              group_wait: "30s"
              repeat_interval: "2h"
  grafana-plugins.yaml:
    name: my-media-stack_grafana-plugins.yaml
    content: |
      apiVersion: 1
      apps:
  grafana.ini:
    name: my-media-stack_grafana.ini
    content: "##################### Grafana Configuration Defaults #####################\n#\n# Do not modify this file in grafana installs\n#\n\n# possible values : production, development\napp_mode = production\n\n# instance name, defaults to HOSTNAME environment variable value or hostname if HOSTNAME var is empty\ninstance_name = $$HOSTNAME\n\n#################################### Paths ###############################\n[paths]\n# Path to where grafana can store temp files, sessions, and the sqlite3 db (if that is used)\ndata = data\n\n# Temporary files in `data` directory older than given duration will be removed\ntemp_data_lifetime = 24h\n\n# Directory where grafana can store logs\nlogs = data/log\n\n# Directory where grafana will automatically scan and look for plugins\nplugins = data/plugins\n\n# folder that contains provisioning config files that grafana will apply on startup and while running.\nprovisioning = conf/provisioning\n\n# Directories that are permitted to contain local repositories.\n# This is a list. Each entry is delimited by a pipe (|). No leading or trailing spaces are supported.\n# These do not need to be absolute paths, in which case they'll be relative to the path where you are running Grafana.\n# Empty entries will return an error, unless the string is just a single pipe.\n# Example: permitted_provisioning_paths = /tmp|/etc/grafana/repositories|conf/provisioning\npermitted_provisioning_paths = devenv/dev-dashboards|conf/provisioning\n\n#################################### Server ##############################\n[server]\n# Protocol (http, https, h2, socket)\nprotocol = http\n\n# Minimum TLS version allowed. By default, this value is empty. Accepted values are: TLS1.2, TLS1.3. If nothing is set TLS1.2 would be taken\nmin_tls_version = \"\"\n\n# The ip address to bind to, empty will bind to all interfaces\nhttp_addr =\n\n# The http port to use\nhttp_port = 3000\n\n# The public facing domain name used to access grafana from a browser\ndomain = localhost\n\n# Redirect to correct domain if host header does not match domain\n# Prevents DNS rebinding attacks\nenforce_domain = false\n\n# The full public facing url\nroot_url = %(protocol)s://%(domain)s:%(http_port)s/\n\n# Serve Grafana from subpath specified in `root_url` setting. By default it is set to `false` for compatibility reasons.\nserve_from_sub_path = false\n\n# Log web requests\nrouter_logging = true\n\n# the path relative working path\nstatic_root_path = public\n\n# enable gzip\nenable_gzip = false\n\n# https certs & key file\ncert_file =\ncert_key =\ncert_pass =\n\n# Certificates file watch interval\ncerts_watch_interval =\n\n# Unix socket gid\n# Changing the gid of a file without privileges requires that the target group is in the group of the process and that the process is the file owner\n# It is recommended to set the gid as http server user gid\n# Not set when the value is -1\nsocket_gid = -1\n\n# Unix socket mode\nsocket_mode = 0660\n\n# Unix socket path\nsocket = /tmp/grafana.sock\n\n# CDN Url\ncdn_url = \n\n# Sets the maximum time in minutes before timing out read of an incoming request and closing idle connections.\n# `0` means there is no timeout for reading the request.\nread_timeout = 0\n\n# This setting enables you to specify additional headers that the server adds to HTTP(S) responses.\n[server.custom_response_headers]\n#exampleHeader1 = exampleValue1\n#exampleHeader2 = exampleValue2\n\n[environment]\n# Sets whether the local file system is available for Grafana to use. Default is true for backward compatibility.\nlocal_file_system_available = true\n\n#################################### GRPC Server #########################\n[grpc_server]\nnetwork = \"tcp\"\naddress = \"127.0.0.1:10000\"\nuse_tls = false\ncert_file =\nkey_file =\n# this will log the request and response for each unary gRPC call\nenable_logging = false\n\n# Maximum size of a message that can be received in bytes. If not set, uses the gRPC default (4MiB).\nmax_recv_msg_size = \n\n# Maximum size of a message that can be sent in bytes. If not set, uses the gRPC default (unlimited).\nmax_send_msg_size = \n\n#################################### Database ############################\n[database]\n# You can configure the database connection by specifying type, host, name, user and password\n# as separate properties or as on string using the url property.\n\n# Either \"mysql\", \"postgres\" or \"sqlite3\", it's your choice\ntype = sqlite3\nhost = 127.0.0.1:3306\nname = grafana\nuser = root\n# If the password contains # or ; you have to wrap it with triple quotes. Ex \"\"\"#password;\"\"\"\npassword = \n# Use either URL or the previous fields to configure the database\n# Example: mysql://user:secret@host:port/database\nurl = \n\n# Set to true or false to enable or disable high availability mode.\n# When it's set to false some functions will be simplified and only run in-process\n# instead of relying on the database.\n#\n# Only set it to false if you run only a single instance of Grafana.\nhigh_availability = false\n\n# Max idle conn setting default is 2\nmax_idle_conn = 2\n\n# Max conn setting default is 0 (mean not set)\nmax_open_conn = 0\n\n# Connection Max Lifetime default is 14400 (means 14400 seconds or 4 hours)\nconn_max_lifetime = 14400\n\n# Set to true to log the sql calls and execution times.\nlog_queries = \n\n# For \"postgres\", use either \"disable\", \"require\" or \"verify-full\"\n# For \"mysql\", use either \"true\", \"false\", or \"skip-verify\".\nssl_mode = disable\n\n# For \"postgres\", use either \"1\" to enable or \"0\" to disable SNI\nssl_sni = 0\n\n# Database drivers may support different transaction isolation levels.\n# Currently, only \"mysql\" driver supports isolation levels.\n# If the value is empty - driver's default isolation level is applied.\n# For \"mysql\" use \"READ-UNCOMMITTED\", \"READ-COMMITTED\", \"REPEATABLE-READ\" or \"SERIALIZABLE\".\nisolation_level = \n\nca_cert_path = \nclient_key_path = \nclient_cert_path = \nserver_cert_name = \n\n# For \"sqlite3\" only, path relative to data_path setting\npath = grafana.db\n\n# For \"sqlite3\" only. cache mode setting used for connecting to the database\ncache_mode = private\n\n# For \"sqlite3\" only. Enable/disable Write-Ahead Logging, https://sqlite.org/wal.html. Default is false.\nwal = false\n\n# For \"mysql\" and \"postgres\". Lock the database for the migrations, default is true.\nmigration_locking = true\n\n# For \"mysql\" and \"postgres\" only if migrationLocking is set. How many seconds to wait before failing to lock the database for the migrations, default is 0.\nlocking_attempt_timeout_sec = 0\n\n# For \"sqlite\" only. How many times to retry query in case of database is locked failures. Default is 0 (disabled).\nquery_retries = 0\n\n# For \"sqlite\" only. How many times to retry transaction in case of database is locked failures. Default is 5.\ntransaction_retries = 5\n\n# Set to true to add metrics and tracing for database queries.\ninstrument_queries = false\n\n#################################### Cache server #############################\n[remote_cache]\n# Either \"redis\", \"memcached\" or \"database\" default is \"database\"\ntype = database\n\n# cache connectionstring options\n# database: will use Grafana primary database.\n# redis: config like redis server e.g. `addr=127.0.0.1:6379,pool_size=100,db=0,username=grafana,password=grafanaRocks,ssl=false`. Only addr is required. ssl may be 'true', 'false', or 'insecure'.\n# memcache: 127.0.0.1:11211\nconnstr = \n\n# prefix prepended to all the keys in the remote cache\nprefix = \n\n# This enables encryption of values stored in the remote cache\nencryption = \n\n#################################### Data proxy ###########################\n[dataproxy]\n\n# This enables data proxy logging, default is false\nlogging = false\n\n# How long the data proxy waits to read the headers of the response before timing out, default is 30 seconds.\n# This setting also applies to core backend HTTP data sources where query requests use an HTTP client with timeout set.\ntimeout = 30\n\n# How long the data proxy waits to establish a TCP connection before timing out, default is 10 seconds.\ndialTimeout = 10\n\n# How many seconds the data proxy waits before sending a keepalive request.\nkeep_alive_seconds = 30\n\n# How many seconds the data proxy waits for a successful TLS Handshake before timing out.\ntls_handshake_timeout_seconds = 10\n\n# How many seconds the data proxy will wait for a server's first response headers after\n# fully writing the request headers if the request has an \"Expect: 100-continue\"\n# header. A value of 0 will result in the body being sent immediately, without\n# waiting for the server to approve.\nexpect_continue_timeout_seconds = 1\n\n# Optionally limits the total number of connections per host, including connections in the dialing,\n# active, and idle states. On limit violation, dials will block.\n# A value of zero (0) means no limit.\nmax_conns_per_host = 0\n\n# The maximum number of idle connections that Grafana will keep alive.\nmax_idle_connections = 100\n\n# How many seconds the data proxy keeps an idle connection open before timing out.\nidle_conn_timeout_seconds = 90\n\n# If enabled and user is not anonymous, data proxy will add X-Grafana-User header with username into the request.\nsend_user_header = false\n\n# Limit the amount of bytes that will be read/accepted from responses of outgoing HTTP requests.\nresponse_limit = 0\n\n# Limits the number of rows that Grafana will process from SQL data sources.\nrow_limit = 1000000\n\n# Sets a custom value for the `User-Agent` header for outgoing data proxy requests. If empty, the default value is `Grafana/<BuildVersion>` (for example `Grafana/9.0.0`).\nuser_agent = \n\n#################################### Analytics ###########################\n[analytics]\n# Server reporting, sends usage counters to stats.grafana.org every 24 hours.\n# No ip addresses are being tracked, only simple counters to track\n# running instances, dashboard and error counts. It is very helpful to us.\n# Change this option to false to disable reporting.\nreporting_enabled = true\n\n# The name of the distributor of the Grafana instance. Ex hosted-grafana, grafana-labs\nreporting_distributor = grafana-labs\n\n# Set to false to disable all checks to https://grafana.com\n# for new versions of grafana. The check is used\n# in some UI views to notify that a grafana update exists.\n# This option does not cause any auto updates, nor send any information\n# only a GET request to https://grafana.com/api/grafana/versions/stable to get the latest version.\ncheck_for_updates = true\n\n# Set to false to disable all checks to https://grafana.com\n# for new versions of plugins. The check is used\n# in some UI views to notify that a plugin update exists.\n# This option does not cause any auto updates, nor send any information\n# only a GET request to https://grafana.com to get the latest versions.\ncheck_for_plugin_updates = true\n\n# Google Analytics universal tracking code, only enabled if you specify an id here\ngoogle_analytics_ua_id = \n\n# Google Analytics 4 tracking code, only enabled if you specify an id here\ngoogle_analytics_4_id = \n\n# When Google Analytics 4 Enhanced event measurement is enabled, we will try to avoid sending duplicate events and let Google Analytics 4 detect navigation changes, etc.\ngoogle_analytics_4_send_manual_page_views = false\n\n# Google Tag Manager ID, only enabled if you specify an id here\ngoogle_tag_manager_id = \n\n# Rudderstack write key, enabled only if rudderstack_data_plane_url is also set\nrudderstack_write_key = \n\n# Rudderstack data plane url, enabled only if rudderstack_write_key is also set\nrudderstack_data_plane_url = \n\n# Rudderstack SDK url, optional, only valid if rudderstack_write_key and rudderstack_data_plane_url is also set\nrudderstack_sdk_url = \n\n# Rudderstack Config url, optional, used by Rudderstack SDK to fetch source config\nrudderstack_config_url = \n\n# Rudderstack Integrations URL, optional. Only valid if you pass the SDK version 1.1 or higher\nrudderstack_integrations_url = \n\n# Intercom secret, optional, used to hash user_id before passing to Intercom via Rudderstack\nintercom_secret = \n\n# Application Insights connection string. Specify an URL string to enable this feature.\napplication_insights_connection_string = \n\n# Optional. Specifies an Application Insights endpoint URL where the endpoint string is wrapped in backticks ``.\napplication_insights_endpoint_url = \n\n# Controls if the UI contains any links to user feedback forms\nfeedback_links_enabled = true \n\n# Static context that is being added to analytics events\nreporting_static_context = \n\n# Logs interaction events to the browser javascript console, intended for development only\nbrowser_console_reporter = false\n\n#################################### Security ############################\n[security]\n# disable creation of admin user on first start of grafana\ndisable_initial_admin_creation = false\n\n# default admin user, created on startup\nadmin_user = brunner56\n\n# default admin password, can be changed before first start of grafana, or in profile settings\nadmin_password = admin\n\n# default admin email, created on startup\nadmin_email = admin@localhost\n\n# used for signing\nsecret_key = SW2YcwTIb9zpOOhoPsMm\n\n# current key provider used for envelope encryption, default to static value specified by secret_key\n# Using empty/default provider instead of secretKey.v1 to avoid configuration issues\nencryption_provider = \n\n# list of configured key providers, space separated (Enterprise only): e.g., awskms.v1 azurekv.v1\navailable_encryption_providers = \n\n# disable gravatar profile images\ndisable_gravatar = false\n\n# data source proxy whitelist (ip_or_domain:port separated by spaces)\ndata_source_proxy_whitelist = \n\n# disable protection against brute force login attempts\ndisable_brute_force_login_protection = false\n\n# max number of failed login attempts before user gets locked\nbrute_force_login_protection_max_attempts = 5\n\n# disable protection against brute force login attempts by IP address\ndisable_ip_address_login_protection = true\n\n# set to true if you host Grafana behind HTTPS. default is false.\ncookie_secure = false\n\n# set cookie SameSite attribute. defaults to `lax`. can be set to \"lax\", \"strict\", \"none\" and \"disabled\"\ncookie_samesite = lax\n\n# set to true if you want to allow browsers to render Grafana in a <frame>, <iframe>, <embed> or <object>. default is false.\nallow_embedding = false\n\n# Set to true if you want to enable http strict transport security (HSTS) response header.\n# HSTS tells browsers that the site should only be accessed using HTTPS.\nstrict_transport_security = false\n\n# Sets how long a browser should cache HSTS. Only applied if strict_transport_security is enabled.\nstrict_transport_security_max_age_seconds = 86400\n\n# Set to true if to enable HSTS preloading option. Only applied if strict_transport_security is enabled.\nstrict_transport_security_preload = false\n\n# Set to true if to enable the HSTS includeSubDomains option. Only applied if strict_transport_security is enabled.\nstrict_transport_security_subdomains = false\n\n# Set to true to enable the X-Content-Type-Options response header.\n# The X-Content-Type-Options response HTTP header is a marker used by the server to indicate that the MIME types advertised\n# in the Content-Type headers should not be changed and be followed.\nx_content_type_options = true\n\n# Set to true to enable the X-XSS-Protection header, which tells browsers to stop pages from loading\n# when they detect reflected cross-site scripting (XSS) attacks.\nx_xss_protection = true\n\n# Enable adding the Content-Security-Policy header to your requests.\n# CSP allows to control resources the user agent is allowed to load and helps prevent XSS attacks.\ncontent_security_policy = false\n\n# Set Content Security Policy template used when adding the Content-Security-Policy header to your requests.\n# $$NONCE in the template includes a random nonce.\n# $$ROOT_PATH is server.root_url without the protocol.\ncontent_security_policy_template = \"\"\"script-src 'self' 'unsafe-eval' 'unsafe-inline' 'strict-dynamic' $$NONCE;object-src 'none';font-src 'self';style-src 'self' 'unsafe-inline' blob:;img-src * data:;base-uri 'self';connect-src 'self' grafana.com ws://$$ROOT_PATH wss://$$ROOT_PATH;manifest-src 'self';media-src 'none';form-action 'self';\"\"\"\n\n# Enable adding the Content-Security-Policy-Report-Only header to your requests.\n# Allows you to monitor the effects of a policy without enforcing it.\ncontent_security_policy_report_only = false\n\n# Set Content Security Policy Report Only template used when adding the Content-Security-Policy-Report-Only header to your requests.\n# $$NONCE in the template includes a random nonce.\n# $$ROOT_PATH is server.root_url without the protocol.\ncontent_security_policy_report_only_template = \"\"\"script-src 'self' 'unsafe-eval' 'unsafe-inline' 'strict-dynamic' $$NONCE;object-src 'none';font-src 'self';style-src 'self' 'unsafe-inline' blob:;img-src * data:;base-uri 'self';connect-src 'self' grafana.com ws://$$ROOT_PATH wss://$$ROOT_PATH;manifest-src 'self';media-src 'none';form-action 'self';\"\"\"\n\n# Controls if old angular plugins are supported or not.\nangular_support_enabled = false\n\n# The CSRF check will be executed even if the request has no login cookie.\ncsrf_always_check = false\n\n# Comma-separated list of plugins ids that will be loaded inside the frontend sandbox\n# Currently behind the feature flag pluginsFrontendSandbox\nenable_frontend_sandbox_for_plugins = \n\n# Comma-separated list of paths for POST/PUT URL in actions. Empty will allow anything that is not on the same origin\nactions_allow_post_url = \n\n[security.encryption]\n# Defines the time-to-live (TTL) for decrypted data encryption keys stored in memory (cache).\n# Please note that small values may cause performance issues due to a high frequency decryption operations.\ndata_keys_cache_ttl = 15m\n\n# Defines the frequency of data encryption keys cache cleanup interval.\n# On every interval, decrypted data encryption keys that reached the TTL are removed from the cache.\ndata_keys_cache_cleanup_interval = 1m\n\n#################################### Snapshots ###########################\n[snapshots]\n# set to false to remove snapshot functionality\nenabled = true\n\n# snapshot sharing options\nexternal_enabled = true\nexternal_snapshot_url = https://snapshots.raintank.io\nexternal_snapshot_name = Publish to snapshots.raintank.io\n\n# Set to true to enable this Grafana instance act as an external snapshot server and allow unauthenticated requests for\n# creating and deleting snapshots.\npublic_mode = false\n\n#################################### Dashboards ##################\n\n[dashboards]\n# Number dashboard versions to keep (per dashboard). Default: 20, Minimum: 1\nversions_to_keep = 20\n\n# Minimum dashboard refresh interval. When set, this will restrict users to set the refresh interval of a dashboard lower than given interval. Per default this is 5 seconds.\n# The interval string is a possibly signed sequence of decimal numbers, followed by a unit suffix (ms, s, m, h, d), e.g. 30s or 1m.\nmin_refresh_interval = 5s\n\n# Path to the default home dashboard. If this value is empty, then Grafana uses StaticRootPath + \"dashboards/home.json\"\ndefault_home_dashboard_path = \n\n# Dashboards UIDs to report performance metrics for. * can be used to report metrics for all dashboards\ndashboard_performance_metrics = \n\n# Maximum number of series that will be showed in a single panel. Users can opt in to rendering all series. Default is 0 (unlimited).\npanel_series_limit = 0\n\n################################### Data sources #########################\n[datasources]\n# Upper limit of data sources that Grafana will return. This limit is a temporary configuration and it will be deprecated when pagination will be introduced on the list data sources API.\ndatasource_limit = 5000\n\n# Number of queries to be executed concurrently. Only for the datasource supports concurrency.\n# For now only Loki and InfluxDB (with influxql) are supporting concurrency behind the feature flags.\n# Check datasource documentations for enabling concurrency.\nconcurrent_query_count = 10\n\n# Default behavior for the \"Manage alerts via Alerting UI\" toggle when configuring a data source.\n# It only works if the data source's `jsonData.manageAlerts` prop does not contain a previously configured value.\ndefault_manage_alerts_ui_toggle = true\n\n# Default behavior for the \"Allow as recording rules target\" toggle when configuring a data source.\n# It only works if the data source's `jsonData.allowAsRecordingRulesTarget` prop does not contain a previously configured value.\ndefault_allow_recording_rules_target_alerts_ui_toggle = true\n\n################################### SQL Data Sources #####################\n[sql_datasources]\n# Default maximum number of open connections maintained in the connection pool\n# when connecting to SQL based data sources\nmax_open_conns_default = 100\n\n# Default maximum number of idle connections maintained in the connection pool\n# when connecting to SQL based data sources\nmax_idle_conns_default = 100\n\n# Default maximum connection lifetime used when connecting\n# to SQL based data sources.\nmax_conn_lifetime_default = 14400\n\n#################################### Users ###############################\n[users]\n# disable user signup / registration\nallow_sign_up = false\n\n# Allow non admin users to create organizations\nallow_org_create = false\n\n# Set to true to automatically assign new users to the default organization (id 1)\nauto_assign_org = true\n\n# Set this value to automatically add new users to the provided organization (if auto_assign_org above is set to true)\nauto_assign_org_id = 1\n\n# Default role new users will be automatically assigned\nauto_assign_org_role = Viewer\n\n# Require email validation before sign up completes\nverify_email_enabled = false\n\n# Redirect to default OrgId after login\nlogin_default_org_id = \n\n# Background text for the user field on the login page\nlogin_hint = \npassword_hint =\n\n# Default UI theme (\"dark\" or \"light\" or \"system\")\ndefault_theme = dark\n\n# Default UI language (supported IETF language tag, such as en-US)\ndefault_language = en-US\n\n# Path to a custom home page. Users are only redirected to this if the default home dashboard is used. It should match a frontend route and contain a leading slash.\nhome_page = \n\n# External user management\nexternal_manage_link_url = \nexternal_manage_link_name = \nexternal_manage_info =\n\n# Deprecated: Assign your viewers to editors.\n# Viewers can edit/inspect dashboard settings in the browser. But not save the dashboard.\nviewers_can_edit = false\n\n# Deprecated: Assign your editors to admins.\n# Editors can administrate dashboard, folders and teams they create\neditors_can_admin = false\n\n# The duration in time a user invitation remains valid before expiring. This setting should be expressed as a duration. Examples: 6h (hours), 2d (days), 1w (week). Default is 24h (24 hours). The minimum supported duration is 15m (15 minutes).\nuser_invite_max_lifetime_duration = 24h\n\n# The duration in time a verification email, used to update the email address of a user, remains valid before expiring. This setting should be expressed as a duration. Examples: 6h (hours), 2d (days), 1w (week). Default is 1h (1 hour).\nverification_email_max_lifetime_duration = 1h\n\n# Frequency of updating a user's last seen time. The minimum supported duration is 5m (5 minutes). The maximum supported duration is 1h (1 hour)\nlast_seen_update_interval = 15m\n\n# Enter a comma-separated list of usernames to hide them in the Grafana UI. These users are shown to Grafana admins and to themselves.\nhidden_users = \n\n[secretscan]\n# Enable secretscan feature\nenabled = false\n\n# Interval to check for token leaks\ninterval = 5m\n\n# base URL of the grafana token leak check service\nbase_url = https://secret-scanning.grafana.net\n\n# URL to send outgoing webhooks to in case of detection\noncall_url = \n\n# Whether to revoke the token if a leak is detected or just send a notification\nrevoke = true\n\n[service_accounts]\n# When set, Grafana will not allow the creation of tokens with expiry greater than this setting.\ntoken_expiration_day_limit = \n\n[auth]\n# Login cookie name\nlogin_cookie_name = grafana_session\n\n# Disable usage of Grafana's built-in login solution.\ndisable_login = false\n\n# The maximum lifetime (duration) an authenticated user can be inactive before being required to login at next visit. Default is 7 days (7d). This setting should be expressed as a duration, e.g. 5m (minutes), 6h (hours), 10d (days), 2w (weeks), 1M (month). The lifetime resets at each successful token rotation (token_rotation_interval_minutes).\nlogin_maximum_inactive_lifetime_duration = \n\n# The maximum lifetime (duration) an authenticated user can be logged in since login time before being required to login. Default is 30 days (30d). This setting should be expressed as a duration, e.g. 5m (minutes), 6h (hours), 10d (days), 2w (weeks), 1M (month).\nlogin_maximum_lifetime_duration = \n\n# How often should auth tokens be rotated for authenticated users when being active. The default is each 10 minutes.\ntoken_rotation_interval_minutes = 10\n\n# Set to true to disable (hide) the login form, useful if you use OAuth\ndisable_login_form = false\n\n# Set to true to disable the sign out link in the side menu. Useful if you use auth.proxy or auth.jwt.\ndisable_signout_menu = false\n\n# URL to redirect the user to after sign out\nsignout_redirect_url = \n\n# Set to true to attempt login with OAuth automatically, skipping the login screen.\n# This setting is ignored if multiple OAuth providers are configured.\n# Deprecated, use auto_login option for specific provider instead.\noauth_auto_login = false\n\n# OAuth state max age cookie duration in seconds. Defaults to 600 seconds.\noauth_state_cookie_max_age = 600\n\n# Sets a custom oAuth error message. This is useful if you need to point the users to a specific location for support.\noauth_login_error_message = oauth.login.error\n\n# Minimum wait time in milliseconds for the server lock retry mechanism.\n# The server lock retry mechanism is used to prevent multiple Grafana instances from\n# simultaneously refreshing OAuth tokens. This mechanism waits at least this amount\n# of time before retrying to acquire the server lock. There are 5 retries in total.\n# The wait time between retries is calculated as random(n, n + 500)\noauth_refresh_token_server_lock_min_wait_ms = 1000\n\n# limit of api_key seconds to live before expiration\napi_key_max_seconds_to_live = 1\n\n# Set to true to enable SigV4 authentication option for HTTP-based datasources\nsigv4_auth_enabled = false\n\n# Set to true to enable verbose logging of SigV4 request signing\nsigv4_verbose_logging = false\n\n# Set to true to enable Azure authentication option for HTTP-based datasources\nazure_auth_enabled = false\n\n# Use email lookup in addition to the unique ID provided by the IdP\noauth_allow_insecure_email_lookup = false\n\n# Set to true to include id of identity as a response header\nid_response_header_enabled = false\n\n# Prefix used for the id response header, X-Grafana-Identity-Id\nid_response_header_prefix = X-Grafana\n\n# List of identity namespaces to add id response headers for, separated by space.\n# Available namespaces are user, api-key and service-account.\n# The header value will encode the namespace (\"user:<id>\", \"api-key:<id>\", \"service-account:<id>\")\nid_response_header_namespaces = user api-key service-account\n\n# Enables the use of managed service accounts for plugin authentication\n# This feature currently **only supports single-organization deployments**\nmanaged_service_accounts_enabled = false\n\n#################################### Passwordless Auth ###########################\n[auth.passwordless]\nenabled = false\ncode_expiration = 20m\n\n#################################### SSO Settings ###########################\n[sso_settings]\n# interval for reloading the SSO Settings from the database\n# useful in high availability setups running multiple Grafana instances\n# set to 0 to disable this feature\nreload_interval = 1m\n\n# List of providers that can be configured through the SSO Settings API and UI.\nconfigurable_providers = github gitlab google generic_oauth azuread okta\n\n#################################### Anonymous Auth ######################\n[auth.anonymous]\n# enable anonymous access\nenabled = false\n\n# specify organization name that should be used for unauthenticated users\norg_name = Main Org.\n\n# specify role for unauthenticated users\norg_role = Viewer\n\n# mask the Grafana version number for unauthenticated users\nhide_version = false\n\n# number of devices in total\ndevice_limit = \n\n#################################### GitHub Auth #########################\n[auth.github]\nname = GitHub\nicon = github\nenabled = false\nallow_sign_up = true\nauto_login = false\nclient_id = some_id\nclient_secret =\nscopes = user:email,read:org\nauth_url = https://github.com/login/oauth/authorize\ntoken_url = https://github.com/login/oauth/access_token\napi_url = https://api.github.com/user\nsignout_redirect_url = \nallowed_domains = \nteam_ids = \nallowed_organizations = \nrole_attribute_path = \nrole_attribute_strict = false\norg_mapping = \nallow_assign_grafana_admin = false\nskip_org_role_sync = false\ntls_skip_verify_insecure = false\ntls_client_cert = \ntls_client_key = \ntls_client_ca = \n# GitHub OAuth apps does not provide refresh tokens and the access tokens never expires.\nuse_refresh_token = false\n\n#################################### GitLab Auth #########################\n[auth.gitlab]\nname = GitLab\nicon = gitlab\nenabled = false\nallow_sign_up = true\nauto_login = false\nclient_id = some_id\nclient_secret = \nscopes = openid email profile\nauth_url = https://gitlab.com/oauth/authorize\ntoken_url = https://gitlab.com/oauth/token\napi_url = https://gitlab.com/api/v4\nsignout_redirect_url = \nallowed_domains = \nallowed_groups = \nrole_attribute_path = \nrole_attribute_strict = false\norg_mapping = \nallow_assign_grafana_admin = false\nskip_org_role_sync = false\ntls_skip_verify_insecure = false\ntls_client_cert = \ntls_client_key = \ntls_client_ca = \nuse_pkce = true\nuse_refresh_token = true\n\n#################################### Google Auth #########################\n[auth.google]\nname = Google\nicon = google\nenabled = false\nallow_sign_up = true\nauto_login = false\nclient_id = some_client_id\nclient_secret = \nscopes = openid email profile\nauth_url = https://accounts.google.com/o/oauth2/v2/auth\ntoken_url = https://oauth2.googleapis.com/token\napi_url = https://openidconnect.googleapis.com/v1/userinfo\nsignout_redirect_url = \nallowed_domains = \nvalidate_hd = true\nhosted_domain = \nallowed_groups = \nrole_attribute_path = \nrole_attribute_strict = false\norg_mapping = \nallow_assign_grafana_admin = false\nskip_org_role_sync = true\ntls_skip_verify_insecure = false\ntls_client_cert = \ntls_client_key = \ntls_client_ca = \nuse_pkce = true\nuse_refresh_token = true\n\n#################################### Grafana.com Auth ####################\n# legacy key names (so they work in env variables)\n[auth.grafananet]\nenabled = false\nallow_sign_up = true\nclient_id = some_id\nclient_secret = \nscopes = user:email\nallowed_organizations = \nuse_refresh_token = false\n\n[auth.grafana_com]\nname = Grafana.com\nicon = grafana\nenabled = false\nallow_sign_up = true\nauto_login = false\nclient_id = some_id\nclient_secret = \nscopes = user:email\nallowed_organizations = \nskip_org_role_sync = false\nuse_refresh_token = false\n\n#################################### Azure AD OAuth #######################\n[auth.azuread]\nname = Microsoft\nicon = microsoft\nenabled = false\nallow_sign_up = true\nauto_login = false\nclient_authentication = \nclient_id = some_client_id\nclient_secret = \nmanaged_identity_client_id = \nfederated_credential_audience = \nworkload_identity_token_file = /var/run/secrets/azure/tokens/azure-identity-token\nscopes = openid email profile\nauth_url = https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/authorize\ntoken_url = https://login.microsoftonline.com/<tenant-id>/oauth2/v2.0/token\nsignout_redirect_url = \nallowed_domains = \nallowed_groups = \nallowed_organizations = \nrole_attribute_strict = false\norg_mapping = \nallow_assign_grafana_admin = false\nforce_use_graph_api = false\ntls_skip_verify_insecure = false\ntls_client_cert = \ntls_client_key = \ntls_client_ca = \nuse_pkce = true\nskip_org_role_sync = false\nuse_refresh_token = true\n\n#################################### Okta OAuth #######################\n[auth.okta]\nname = Okta\nicon = okta\nenabled = false\nallow_sign_up = true\nauto_login = false\nclient_id = some_id\nclient_secret = \nscopes = openid profile email groups\nauth_url = https://<tenant-id>.okta.com/oauth2/v1/authorize\ntoken_url = https://<tenant-id>.okta.com/oauth2/v1/token\napi_url = https://<tenant-id>.okta.com/oauth2/v1/userinfo\nsignout_redirect_url = \nallowed_domains = \nallowed_groups = \nrole_attribute_path = \nrole_attribute_strict = false\norg_attribute_path = \norg_mapping = \nallow_assign_grafana_admin = false\nskip_org_role_sync = false\ntls_skip_verify_insecure = false\ntls_client_cert = \ntls_client_key = \ntls_client_ca = \nuse_pkce = true\nuse_refresh_token = false\n\n#################################### Generic OAuth #######################\n[auth.generic_oauth]\nname = OAuth\nicon = signin\nenabled = false\nallow_sign_up = true\nauto_login = false\nclient_id = some_id\nclient_secret = \nscopes = user:email\nempty_scopes = false\nemail_attribute_name = email:primary\nemail_attribute_path = \nlogin_attribute_path = \nname_attribute_path = \nrole_attribute_path = \nrole_attribute_strict = false\norg_attribute_path = \norg_mapping = \ngroups_attribute_path = \nid_token_attribute_name = \nteam_ids_attribute_path = \nauth_url = \ntoken_url = \napi_url = \nsignout_redirect_url = \nteams_url = \nallowed_domains = \nallowed_groups = \nteam_ids = \nallowed_organizations = \ntls_skip_verify_insecure = false\ntls_client_cert = \ntls_client_key = \ntls_client_ca = \nuse_pkce = false\nauth_style = \nallow_assign_grafana_admin = false\nskip_org_role_sync = false\nuse_refresh_token = false\n\n#################################### Basic Auth ##########################\n[auth.basic]\nenabled = true\n# This setting will enable a stronger password policy for user's password under basic auth.\n# The password will need to comply with the following password policy\n# 1. Have a minimum of 12 characters\n# 2. Composed by at least 1 uppercase character\n# 3. Composed by at least 1 lowercase character\n# 4. Composed by at least 1 digit character\n# 5. Composed by at least 1 symbol character\npassword_policy = false\n\n#################################### Auth Proxy ##########################\n[auth.proxy]\nenabled = false\nheader_name = X-WEBAUTH-USER\nheader_property = username\nauto_sign_up = true\nsync_ttl = 15\nwhitelist = \nheaders = \nheaders_encoded = false\nenable_login_token = false\n\n#################################### Auth JWT ##########################\n[auth.jwt]\nenabled = false\nenable_login_token = false\nheader_name = \nemail_claim = \nusername_claim = \nemail_attribute_path = \nusername_attribute_path = \njwk_set_url = \njwk_set_file = \ncache_ttl = 60m\nexpect_claims = {}\nkey_file = \nkey_id = \nrole_attribute_path = \nrole_attribute_strict = false\norg_attribute_path = \norg_mapping = \ngroups_attribute_path = \nauto_sign_up = false\nurl_login = false\nallow_assign_grafana_admin = false\nskip_org_role_sync = false\ntls_skip_verify_insecure = false\n\n#################################### Auth LDAP ###########################\n[auth.ldap]\nenabled = false\nconfig_file = /etc/grafana/ldap.toml\nallow_sign_up = true\nskip_org_role_sync = false\n\n# LDAP background sync (Enterprise only)\n# At 1 am every day\nsync_cron = \"0 1 * * *\"\nactive_sync_enabled = true\n\n#################################### AWS #####################################\n[aws]\n# Enter a comma-separated list of allowed AWS authentication providers.\n# Options are: default (AWS SDK Default), keys (Access && secret key), credentials (Credentials field), ec2_iam_role (EC2 IAM Role)\nallowed_auth_providers = default,keys,credentials\n\n# Allow AWS users to assume a role using temporary security credentials.\n# If true, assume role will be enabled for all AWS authentication providers that are specified in aws_auth_providers\nassume_role_enabled = true\n\n# Specify max no of pages to be returned by the ListMetricPages API\nlist_metrics_page_limit = 500\n\n# Experimental, for use in Grafana Cloud only. Please do not set.\nexternal_id = \n\n# Sets the expiry duration of an assumed role.\n# This setting should be expressed as a duration. Examples: 6h (hours), 10d (days), 2w (weeks), 1M (month).\nsession_duration = \"15m\"\n\n# Set the plugins that will receive AWS settings for each request (via plugin context)\n# By default this will include all Grafana Labs owned AWS plugins, or those that make use of AWS settings (ElasticSearch, Prometheus).\nforward_settings_to_plugins = cloudwatch, grafana-athena-datasource, grafana-redshift-datasource, grafana-x-ray-datasource, grafana-timestream-datasource, grafana-iot-sitewise-datasource, grafana-iot-twinmaker-app, grafana-opensearch-datasource, aws-datasource-provisioner, elasticsearch, prometheus, grafana-amazonprometheus-datasource, grafana-aurora-datasource\n\n#################################### Azure ###############################\n[azure]\n# Azure cloud environment where Grafana is hosted\n# Possible values are AzureCloud, AzureChinaCloud, AzureUSGovernment and AzureGermanCloud\n# Default value is AzureCloud (i.e. public cloud)\ncloud = AzureCloud\n\n# A customized list of Azure cloud settings and properties, used by data sources which need this information when run in non-standard azure environments\n# When specified, this list will replace the default cloud list of AzureCloud, AzureChinaCloud, AzureUSGovernment and AzureGermanCloud\nclouds_config = \n\n# Specifies whether Grafana hosted in Azure service with Managed Identity configured (e.g. Azure Virtual Machines instance)\n# If enabled, the managed identity can be used for authentication of Grafana in Azure services\n# Disabled by default, needs to be explicitly enabled\nmanaged_identity_enabled = false\n\n# Client ID to use for user-assigned managed identity\n# Should be set for user-assigned identity and should be empty for system-assigned identity\nmanaged_identity_client_id = \n\n# Specifies whether Azure AD Workload Identity authentication should be enabled in datasources that support it\n# For more documentation on Azure AD Workload Identity, review this documentation:\n# https://azure.github.io/azure-workload-identity/docs/\n# Disabled by default, needs to be explicitly enabled\nworkload_identity_enabled = false\n\n# Tenant ID of the Azure AD Workload Identity\n# Allows to override default tenant ID of the Azure AD identity associated with the Kubernetes service account\nworkload_identity_tenant_id = \n\n# Client ID of the Azure AD Workload Identity\n# Allows to override default client ID of the Azure AD identity associated with the Kubernetes service account\nworkload_identity_client_id = \n\n# Custom path to token file for the Azure AD Workload Identity\n# Allows to set a custom path to the projected service account token file\nworkload_identity_token_file = \n\n# Specifies whether user identity authentication (on behalf of currently signed-in user) should be enabled in datasources\n# that support it (requires AAD authentication)\n# Disabled by default, needs to be explicitly enabled\nuser_identity_enabled = false\n\n# Specifies whether user identity authentication fallback credentials should be enabled in data sources\n# Enabling this allows data source creators to provide fallback credentials for backend initiated requests\n# e.g. alerting, recorded queries etc.\n# Enabled by default, needs to be explicitly disabled\n# Will not have any effect if user identity is disabled above\nuser_identity_fallback_credentials_enabled = true\n\n# Override token URL for Azure Active Directory\n# By default is the same as token URL configured for AAD authentication settings\nuser_identity_token_url = \n\n# Override client authentication method for Azure Active Directory\n# By default is the same as client authentication method configured for AAD authentication settings\nuser_identity_client_authentication = \n\n# Override ADD application ID which would be used to exchange users token to an access token for the datasource\n# By default is the same as used in AAD authentication or can be set to another application (for OBO flow)\nuser_identity_client_id = \n\n# Override the AAD application client secret\n# By default is the same as used in AAD authentication or can be set to another application (for OBO flow)\nuser_identity_client_secret = \n\n# Override the AAD managed identity client ID\n# By default is the same as used in AAD authentication or can be set to another managed identity (for OBO flow)\nuser_identity_managed_identity_client_id = \n\n# Override the AAD federated credential audience\n# By default is the same as used in AAD authentication or can be set to another audience (for OBO flow)\nuser_identity_federated_credential_audience = \n\n# Allows the usage of a custom token request assertion when Grafana is behind an authentication proxy\n# In most cases this will not need to be used. To enable this set the value to \"username\"\n# The default is empty and any other value will not enable this functionality\nusername_assertion = \n\n# Set the plugins that will receive Azure settings for each request (via plugin context)\n# By default this will include all Grafana Labs owned Azure plugins, or those that make use of Azure settings (Azure Monitor, Azure Data Explorer, Prometheus, MSSQL, Azure Prometheus).\nforward_settings_to_plugins = grafana-azure-monitor-datasource, prometheus, grafana-azure-data-explorer-datasource, mssql, grafana-azureprometheus-datasource\n\n# Specifies whether Entra password auth can be used for the MSSQL data source\n# Disabled by default, needs to be explicitly enabled\nazure_entra_password_credentials_enabled = false\n\n#################################### Role-based Access Control ###########\n[rbac]\n# If enabled, cache permissions in a in memory cache\npermission_cache = true\n\n# Reset basic roles permissions on boot\n# Warning left to true, basic roles permissions will be reset on every boot\nreset_basic_roles = false\n\n# Validate permissions' action and scope on role creation and update\npermission_validation_enabled = true\n\n#################################### SMTP / Emailing #####################\n[smtp]\nenabled = false\nhost = localhost:25\nuser = \n# If the password contains # or ; you have to wrap it with triple quotes. Ex \"\"\"#password;\"\"\"\npassword = \ncert_file = \nkey_file = \nskip_verify = false\nfrom_address = admin@grafana.localhost\nfrom_name = Grafana\nehlo_identity = \nstartTLS_policy = \nenable_tracing = false\n\n[smtp.static_headers]\n# Include custom static headers in all outgoing emails\n\n[emails]\nwelcome_email_on_sign_up = false\ntemplates_pattern = emails/*.html, emails/*.txt\ncontent_types = text/html\n\n#################################### Logging ##########################\n[log]\n# Either \"console\", \"file\", \"syslog\". Default is console and file\n# Use space to separate multiple modes, e.g. \"console file\"\nmode = console file\n\n# Either \"debug\", \"info\", \"warn\", \"error\". Default is \"info\"\nlevel = info\n\n# optional settings to set different levels for specific loggers. Ex filters = sqlstore:debug\nfilters = \n\n# Set the default error message shown to users. This message is displayed instead of sensitive backend errors which should be obfuscated.\nuser_facing_default_error = \"please inspect Grafana server log for details\"\n\n# For \"console\" mode only\n[log.console]\nlevel = info\n\n# log line format, valid options are text, console and json\nformat = console\n\n# For \"file\" mode only\n[log.file]\nlevel = info\n\n# log line format, valid options are text, console and json\nformat = text\n\n# This enables automated log rotate(switch of following options), default is true\nlog_rotate = true\n\n# Max line number of single file, default is 1000000\nmax_lines = 1000000\n\n# Max size shift of single file, default is 28 means 1 << 28, 256MB\nmax_size_shift = 28\n\n# Segment log daily, default is true\ndaily_rotate = true\n\n# Expired days of log file(delete after max days), default is 7\nmax_days = 7\n\n[log.syslog]\nlevel = info\n\n# log line format, valid options are text, console and json\nformat = text\n\n# Syslog network type and address. This can be udp, tcp, or unix. If left blank, the default unix endpoints will be used.\nnetwork = \naddress = \n\n# Syslog facility. user, daemon and local0 through local7 are valid.\nfacility = \n\n# Syslog tag. By default, the process' argv[0] is used.\ntag = \n\n[log.frontend]\n# Should Faro javascript agent be initialized\nenabled = false\n\n# Custom HTTP endpoint to send events to. Default will log the events to stdout.\ncustom_endpoint = \n\n# Requests per second limit enforced per an extended period, for Grafana backend log ingestion endpoint (/log).\nlog_endpoint_requests_per_second_limit = 3\n\n# Max requests accepted per short interval of time for Grafana backend log ingestion endpoint (/log)\nlog_endpoint_burst_limit = 15\n\n# Enables all Faro default instrumentation by using `getWebInstrumentations`. Overrides other instrumentation flags.\ninstrumentations_all_enabled = false\n\n# Should error instrumentation be enabled, only affects Grafana Javascript Agent\ninstrumentations_errors_enabled = true\n\n# Should console instrumentation be enabled, only affects Grafana Javascript Agent\ninstrumentations_console_enabled = false\n\n# Should webvitals instrumentation be enabled, only affects Grafana Javascript Agent\ninstrumentations_webvitals_enabled = false\n\n# Should tracing instrumentation be enabled, only affects Grafana Javascript Agent\ninstrumentations_tracing_enabled = false\n\n# level of internal logging for debugging Grafana Javascript Agent.\n# possible values are: 0 = OFF, 1 = ERROR, 2 = WARN, 3 = INFO, 4 = VERBOSE\n# more details: https://github.com/grafana/faro-web-sdk/blob/v1.3.7/docs/sources/tutorials/quick-start-browser.md#how-to-activate-debugging\ninternal_logger_level = 0\n\n# Api Key, only applies to Grafana Javascript Agent provider\napi_key = \n\n#################################### Usage Quotas ########################\n[quota]\nenabled = false\n\n#### set quotas to -1 to make unlimited. ####\n# limit number of users per Org.\norg_user = 10\n\n# limit number of dashboards per Org.\norg_dashboard = 100\n\n# limit number of data_sources per Org.\norg_data_source = 10\n\n# limit number of api_keys per Org.\norg_api_key = 10\n\n# limit number of alerts per Org.\norg_alert_rule = 100\n\n# limit number of orgs a user can create.\nuser_org = 10\n\n# Global limit of users.\nglobal_user = 1\n\n# global limit of orgs.\nglobal_org = 1\n\n# global limit of dashboards\nglobal_dashboard = -1\n\n# global limit of api_keys\nglobal_api_key = 1\n\n# global limit on number of logged in users.\nglobal_session = 1\n\n# global limit of alerts\nglobal_alert_rule = 1\n\n# global limit of files uploaded to the SQL DB\nglobal_file = 1000\n\n# global limit of correlations\nglobal_correlations = 1\n\n# Limit of the number of alert rules per rule group.\n# This is not strictly enforced yet, but will be enforced over time.\nalerting_rule_group_rules = 100\n\n# Limit the number of query evaluation results per alert rule.\n# If the condition query of an alert rule produces more results than this limit,\n# the evaluation results in an error.\nalerting_rule_evaluation_results = 1\n\n#################################### Unified Alerting ####################\n[unified_alerting]\n# Enable the Alerting sub-system and interface.\nenabled = false\n\n# Comma-separated list of organization IDs for which to disable unified alerting. Only supported if unified alerting is enabled.\ndisabled_orgs = \n\n# Specify how long to wait for the alerting service to initialize\ninitialization_timeout = 30s\n\n# Specify the frequency of polling for admin config changes.\n# The interval string is a possibly signed sequence of decimal numbers, followed by a unit suffix (ms, s, m, h, d), e.g. 30s or 1m.\nadmin_config_poll_interval = 60s\n\n# Specify the frequency of polling for Alertmanager config changes.\n# The interval string is a possibly signed sequence of decimal numbers, followed by a unit suffix (ms, s, m, h, d), e.g. 30s or 1m.\nalertmanager_config_poll_interval = 60s\n\n# Maximum number of active and pending silences that a tenant can have at once. Default: 0 (no limit).\nalertmanager_max_silences_count = 0\n\n# Maximum silence size in bytes. Default: 0 (no limit).\nalertmanager_max_silence_size_bytes = 0\n\n# Redis server address or addresses. It can be a single Redis address if using Redis standalone,\n# or a list of comma-separated addresses if using Redis Cluster/Sentinel.\nha_redis_address = \n\n# Set to true when using Redis in Cluster mode. Mutually exclusive with ha_redis_sentinel_mode_enabled.\nha_redis_cluster_mode_enabled = false\n\n# Set to true when using Redis in Sentinel mode. Mutually exclusive with ha_redis_cluster_mode_enabled.\nha_redis_sentinel_mode_enabled = false\n\n# Redis Sentinel master name. Only applicable when ha_redis_sentinel_mode_enabled is set to true.\nha_redis_sentinel_master_name = \n\n# The username that should be used to authenticate with Redis.\nha_redis_username = \n\n# The password that should be used to authenticate with Redis.\nha_redis_password = \n\n# The username that should be used to authenticate with Redis Sentinel.\n# Only applicable when ha_redis_sentinel_mode_enabled is set to true.\nha_redis_sentinel_username = \n\n# The password that should be used to authenticate with Redis Sentinel.\n# Only applicable when ha_redis_sentinel_mode_enabled is set to true.\nha_redis_sentinel_password = \n\n# The Redis database. The default value is 0.\nha_redis_db = 0\n\n# A prefix that is used for every key or channel that is created on the Redis server as part of HA for alerting.\n# Useful if you plan to share Redis with multiple Grafana instances.\nha_redis_prefix = \n\n# The name of the cluster peer to use as an identifier. If none is provided, a random one is generated.\nha_redis_peer_name = \n\n# The maximum number of simultaneous Redis connections.\nha_redis_max_conns = 5\n\n# Enable TLS on the client used to communicate with the Redis server. This should be set to true\n# if using any of the other ha_redis_tls_* fields.\nha_redis_tls_enabled = false\n\n# Path to the PEM-encoded TLS client certificate file used to authenticate with the Redis server.\n# Required if using Mutual TLS.\nha_redis_tls_cert_path = \n\n# Path to the PEM-encoded TLS private key file. Also requires the client certificate to be configured.\n# Required if using Mutual TLS.\nha_redis_tls_key_path = \n\n# Path to the PEM-encoded CA certificates file. If not set, the host's root CA certificates are used.\nha_redis_tls_ca_path = \n\n# Overrides the expected name of the Redis server certificate.\nha_redis_tls_server_name = \n\n# Skips validating the Redis server certificate.\nha_redis_tls_insecure_skip_verify = false\n\n# Overrides the default TLS cipher suite list.\nha_redis_tls_cipher_suites = \n\n# Overrides the default minimum TLS version.\n# Allowed values: VersionTLS10, VersionTLS11, VersionTLS12, VersionTLS13\nha_redis_tls_min_version = \n\n# Listen address/hostname and port to receive unified alerting messages for other Grafana instances. The port is used for both TCP and UDP. It is assumed other Grafana instances are also running on the same port.\nha_listen_address = \"0.0.0.0:9094\"\n\n# Explicit address/hostname and port to advertise other Grafana instances. The port is used for both TCP and UDP.\nha_advertise_address = \n\n# Comma-separated list of initial instances (in a format of host:port) that will form the HA cluster. Configuring this setting will enable High Availability mode for alerting.\nha_peers = \"\"\n\n# Time to wait for an instance to send a notification via the Alertmanager. In HA, each Grafana instance will\n# be assigned a position (e.g. 0, 1). We then multiply this position with the timeout to indicate how long should\n# each instance wait before sending the notification to take into account replication lag.\n# The interval string is a possibly signed sequence of decimal numbers, followed by a unit suffix (ms, s, m, h, d), e.g. 30s or 1m.\nha_peer_timeout = 15s\n\n# The label is an optional string to include on each packet and stream.\n# It uniquely identifies the cluster and prevents cross-communication\n# issues when sending gossip messages in an enviromenet with multiple clusters.\nha_label = \n\n# The interval between sending gossip messages. By lowering this value (more frequent) gossip messages are propagated\n# across cluster more quickly at the expense of increased bandwidth usage.\n# The interval string is a possibly signed sequence of decimal numbers, followed by a unit suffix (ms, s, m, h, d), e.g. 30s or 1m.\nha_gossip_interval = 200ms\n\n# Length of time to attempt to reconnect to a lost peer. Recommended to be short (<15m) when Grafana is running in a Kubernetes cluster.\n# The string is a possibly signed sequence of decimal numbers, followed by a unit suffix (ms, s, m, h, d), e.g. 30s or 1m.\nha_reconnect_timeout = 6h\n\n# The interval between gossip full state syncs. Setting this interval lower (more frequent) will increase convergence speeds\n# across larger clusters at the expense of increased bandwidth usage.\n# The interval string is a possibly signed sequence of decimal numbers, followed by a unit suffix (ms, s, m, h, d), e.g. 30s or 1m.\nha_push_pull_interval = 60s\n\n# Enable or disable alerting rule execution. The alerting UI remains visible.\nexecute_alerts = true\n\n# Alert evaluation timeout when fetching data from the datasource.\n# The timeout string is a possibly signed sequence of decimal numbers, followed by a unit suffix (ms, s, m, h, d), e.g. 30s or 1m.\nevaluation_timeout = 30s\n\n# Number of times we'll attempt to evaluate an alert rule before giving up on that evaluation. The default value is 3.\nmax_attempts = 3\n\n# Minimum interval to enforce between rule evaluations. Rules will be adjusted if they are less than this value or if they are not multiple of the scheduler interval (10s). Higher values can help with resource management as we'll schedule fewer evaluations over time.\n# The interval string is a possibly signed sequence of decimal numbers, followed by a unit suffix (ms, s, m, h, d), e.g. 30s or 1m.\nmin_interval = 10s\n\n# This is an experimental option to add parallelization to saving alert states in the database.\n# It configures the maximum number of concurrent queries per rule evaluated. The default value is 1\n# (concurrent queries per rule disabled).\nmax_state_save_concurrency = 1\n\n# If the feature flag 'alertingSaveStatePeriodic' is enabled, this is the interval that is used to persist the alerting instances to the database.\n# The interval string is a possibly signed sequence of decimal numbers, followed by a unit suffix (ms, s, m, h, d), e.g. 30s or 1m.\nstate_periodic_save_interval = 5m\n\n# If the feature flag 'alertingSaveStatePeriodic' is enabled, this is the size of the batch that is saved to the database at once.\nstate_periodic_save_batch_size = 1\n\n# Disables the smoothing of alert evaluations across their evaluation window.\n# Rules will evaluate in sync.\ndisable_jitter = false\n\n# Retention period for Alertmanager notification log entries.\nnotification_log_retention = 5d\n\n# Duration for which a resolved alert state transition will continue to be sent to the Alertmanager.\nresolved_alert_retention = 15m\n\n# Defines the limit of how many alert rule versions\n# should be stored in the database for each alert rule in an organization including the current one.\n# 0 value means no limit\nrule_version_record_limit = 0\n\n# The retention period for deleted alerting rules.\n# Determines how long deleted rules are retained before being permanently removed.\n# The retention duration must be specified using a time format with unit suffixes\n# such as ms, s, m, h, d (e.g., 30d for 30 days).\n# Default: 30d\n# 0 value means that rules are deleted permanently immediately.\ndeleted_rule_retention = 30d\n\n[unified_alerting.screenshots]\n# Enable screenshots in notifications. You must have either installed the Grafana image rendering\n# plugin, or set up Grafana to use a remote rendering service.\n# For more information on configuration options, refer to [rendering].\ncapture = false\n\n# The timeout for capturing screenshots. If a screenshot cannot be captured within the timeout then\n# the notification is sent without a screenshot. The maximum duration is 30 seconds. This timeout\n# should be less than the minimum Interval of all Evaluation Groups to avoid back pressure on alert\n# rule evaluation.\ncapture_timeout = 10s\n\n# The maximum number of screenshots that can be taken at the same time. This option is different from\n# concurrent_render_request_limit as max_concurrent_screenshots sets the number of concurrent screenshots\n# that can be taken at the same time for all firing alerts where as concurrent_render_request_limit sets\n# the total number of concurrent screenshots across all Grafana services.\nmax_concurrent_screenshots = 5\n\n# Uploads screenshots to the local Grafana server or remote storage such as Azure, S3 and GCS. Please\n# see [external_image_storage] for further configuration options. If this option is false then\n# screenshots will be persisted to disk for up to temp_data_lifetime.\nupload_external_image_storage = false\n\n[unified_alerting.reserved_labels]\n# Comma-separated list of reserved labels added by the Grafana Alerting engine that should be disabled.\n# For example: `disabled_labels=grafana_folder`\ndisabled_labels = \n\n[unified_alerting.state_history]\n# Enable the state history functionality in Unified Alerting. The previous states of alert rules will be visible in panels and in the UI.\nenabled = true\n\n# Select which pluggable state history backend to use. Either \"annotations\", \"loki\", \"prometheus\", or \"multiple\"\n# \"loki\" writes state history to an external Loki instance.\n# \"prometheus\" writes state history as GRAFANA_ALERTS metrics to a Prometheus-compatible data source.\n# \"multiple\" allows history to be written to multiple backends at once.\n# Defaults to \"annotations\".\nbackend = \n\n# For \"multiple\" only.\n# Indicates the main backend used to serve state history queries.\n# Either \"annotations\" or \"loki\"\nprimary = \n\n# For \"multiple\" only.\n# Comma-separated list of additional backends to write state history data to.\nsecondaries = \n\n# For \"loki\" only.\n# URL of the external Loki instance.\n# Either \"loki_remote_url\", or both of \"loki_remote_read_url\" and \"loki_remote_write_url\" is required for the \"loki\" backend.\nloki_remote_url = \n\n# For \"loki\" only.\n# URL of the external Loki's read path. To be used in configurations where Loki has separated read and write URLs.\n# Either \"loki_remote_url\", or both of \"loki_remote_read_url\" and \"loki_remote_write_url\" is required for the \"loki\" backend.\nloki_remote_read_url = \n\n# For \"loki\" only.\n# URL of the external Loki's write path. To be used in configurations where Loki has separated read and write URLs.\n# Either \"loki_remote_url\", or both of \"loki_remote_read_url\" and \"loki_remote_write_url\" is required for the \"loki\" backend.\nloki_remote_write_url = \n\n# For \"loki\" only.\n# Optional tenant ID to attach to requests sent to Loki.\nloki_tenant_id = \n\n# For \"loki\" only.\n# Optional username for basic authentication on requests sent to Loki. Can be left blank to disable basic auth.\nloki_basic_auth_username = \n\n# For \"loki\" only.\n# Optional password for basic authentication on requests sent to Loki. Can be left blank.\nloki_basic_auth_password = \n\n# For \"loki\" only.\n# Optional max query length for queries sent to Loki. Default is 721h which matches the default Loki value.\nloki_max_query_length = 721h\n\n# For \"loki\" only.\n# Maximum size in bytes for queries sent to Loki. This limit is applied to user provided filters as well as system defined ones, e.g. applied by access control.\n# If filter exceeds the limit, API returns error with code \"alerting.state-history.loki.requestTooLong\".\n# Default is 64kb\nloki_max_query_size = 65536\n\n# For \"prometheus\" only.\n# Target datasource UID for writing GRAFANA_ALERTS metrics.\nprometheus_target_datasource_uid = \n\n# For \"prometheus\" only.\n# Metric name for the GRAFANA_ALERTS metric. Default is \"GRAFANA_ALERTS\".\nprometheus_metric_name = GRAFANA_ALERTS\n\n# For \"prometheus\" only.\n# Timeout for writing GRAFANA_ALERTS metrics to the target datasource. Default is 10s.\nprometheus_write_timeout = 10s\n\n[unified_alerting.state_history.external_labels]\n# Optional extra labels to attach to outbound state history records or log streams.\n# Any number of label key-value-pairs can be provided.\n#\n# ex.\n# mylabelkey = mylabelvalue\n\n[unified_alerting.state_history.annotations]\n# Controls retention of annotations automatically created while evaluating alert rules.\n# Alert state history backend must be configured to be annotations (see setting [unified_alerting.state_history].backend).\n\n# Configures how long alert annotations are stored for. Default is 0, which keeps them forever.\n# This setting should be expressed as a duration. Ex 6h (hours), 10d (days), 2w (weeks), 1M (month).\nmax_age = \n\n# Configures max number of alert annotations that Grafana stores. Default value is 0, which keeps all alert annotations.\nmax_annotations_to_keep = \n\n[unified_alerting.prometheus_conversion]\n# Configuration options for converting Prometheus alerting and recording rules to Grafana rules.\n# These settings affect rules created via the Prometheus conversion API.\n\n# Offset the rule evaluation time for imported rules by a specified duration in the past.\n# This offset is applied and saved to the rule query during the conversion process from Prometheus to Grafana format.\n# The setting only affects rules imported after the configuration change is made and does not modify existing rules.\n# Accepts duration formats like: 30s, 1m, 1h.\nrule_query_offset = 1m\n\n[recording_rules]\n# Enable recording rules.\nenabled = true\n\n# Request timeout for recording rule writes.\ntimeout = 10s\n\n# Default data source UID to write to if not specified in the rule definition.\ndefault_datasource_uid = \n\n# Optional custom headers to include in recording rule write requests.\n[recording_rules.custom_headers]\n# exampleHeader = exampleValue\n\n[remote.alertmanager]\n# URL of the remote Alertmanager that will replace the internal one.\n# This URL should be the root path, Grafana will automatically append an \"/alertmanager\" suffix for certain HTTP calls.\n# Required if `enabled` is set to `true`.\nurl = \n\n# Tenant ID to use in requests to the Alertmanager.\n# It will also be used for the basic auth username if a password is configured.\ntenant = \n\n# Optional password for basic authentication.\n# If not present, the tenant ID will be set in the X-Scope-OrgID header.\npassword = \n\nsync_interval = 5m\n\n# Timeout for the HTTP client. Default is 30 seconds.\ntimeout = 30s\n\n#################################### Annotations #########################\n[annotations]\n# Configures the batch size for the annotation clean-up job. This setting is used for dashboard, API, and alert annotations.\ncleanupjob_batchsize = 100\n\n# Enforces the maximum allowed length of the tags for any newly introduced annotations. It can be between 500 and 4096 inclusive (which is the respective's column length). Default value is 500.\n# Setting it to a higher value would impact performance therefore is not recommended.\ntags_length = 500\n\n[annotations.dashboard]\n# Dashboard annotations means that annotations are associated with the dashboard they are created on.\n\n# Configures how long dashboard annotations are stored. Default is 0, which keeps them forever.\n# This setting should be expressed as a duration. Examples: 6h (hours), 10d (days), 2w (weeks), 1M (month).\nmax_age = \n\n# Configures max number of dashboard annotations that Grafana stores. Default value is 0, which keeps all dashboard annotations.\nmax_annotations_to_keep = \n\n[annotations.api]\n# API annotations means that the annotations have been created using the API without any\n# association with a dashboard.\n\n# Configures how long Grafana stores API annotations. Default is 0, which keeps them forever.\n# This setting should be expressed as a duration. Examples: 6h (hours), 10d (days), 2w (weeks), 1M (month).\nmax_age = \n\n# Configures max number of API annotations that Grafana keeps. Default value is 0, which keeps all API annotations.\nmax_annotations_to_keep = \n\n#################################### Explore #############################\n[explore]\n# Enable the Explore section\nenabled = true\n\n# set the default offset for the time picker\ndefaultTimeOffset = 1h\n\n# hides the download logs button in Explore\nhide_logs_download = false\n\n#################################### Help #############################\n[help]\n# Enable the Help section\nenabled = true\n\n#################################### Profile #############################\n[profile]\n# Enable the Profile section\nenabled = true\n\n#################################### News #############################\n[news]\n# Enable the news feed section\nnews_feed_enabled = true\n\n#################################### Query #############################\n[query]\n# Set the number of data source queries that can be executed concurrently in mixed queries. Default is the number of CPUs.\nconcurrent_query_limit = \n\n#################################### Query History #############################\n[query_history]\n# Enable the Query history\nenabled = true\n\n#################################### Short Links #############################\n[short_links]\n# Short links that are never accessed will be deleted as cleanup. Time is set up in days. The default is 7 days. Maximum value is 365.\n# 0 means they will be deleted approximately every 10 minutes. A negative value (such as -1) will disable expiration.\nexpire_time = 7\n\n#################################### Internal Grafana Metrics ############\n# Metrics available at HTTP URL /metrics and /metrics/plugins/:pluginId\n[metrics]\nenabled = true\ninterval_seconds = 10\n# Disable total stats (stat_totals_*) metrics to be generated\ndisable_total_stats = false\n# The interval at which the total stats collector will update the stats. Default is 1800 seconds.\ntotal_stats_collector_interval_seconds = 1800\n\n#If both are set, basic auth will be required for the metrics endpoints.\nbasic_auth_username = \nbasic_auth_password = \n\n# Metrics environment info adds dimensions to the `grafana_environment_info` metric, which\n# can expose more information about the Grafana instance.\n[metrics.environment_info]\nexampleLabel1 = \nexampleLabel2 = \n\n# Send internal Grafana metrics to graphite\n[metrics.graphite]\n# Enable by setting the address setting (ex localhost:2003)\naddress = \nprefix = prod.grafana.%(instance_name)s.\n\n#################################### Grafana.com integration  ##########################\n[grafana_net]\nurl = https://grafana.com\n\n[grafana_com]\nurl = https://grafana.com\napi_url = https://grafana.com/api\nsso_api_token = \n\n#################################### Distributed tracing ############\n# Opentracing is deprecated use opentelemetry instead\n[tracing.jaeger]\n# jaeger destination (ex localhost:6831)\naddress = \n# tag that will always be included in when creating new spans. ex (tag1:value1,tag2:value2)\nalways_included_tag = \n# Type specifies the type of the sampler: const, probabilistic, rateLimiting, or remote\nsampler_type = const\n# jaeger samplerconfig param\n# for \"const\" sampler, 0 or 1 for always false/true respectively\n# for \"probabilistic\" sampler, a probability between 0 and 1\n# for \"rateLimiting\" sampler, the number of spans per second\n# for \"remote\" sampler, param is the same as for \"probabilistic\"\n# and indicates the initial sampling rate before the actual one\n# is received from the mothership\nsampler_param = 1\n# sampling_server_url is the URL of a sampling manager providing a sampling strategy.\nsampling_server_url = \n# Whether or not to use Zipkin span propagation (x-b3- HTTP headers).\nzipkin_propagation = false\n# Setting this to true disables shared RPC spans.\n# Not disabling is the most common setting when using Zipkin elsewhere in your infrastructure.\ndisable_shared_zipkin_spans = false\n\n[tracing.opentelemetry]\n# attributes that will always be included in when creating new spans. ex (key1:value1,key2:value2)\ncustom_attributes = \n# Type specifies the type of the sampler: const, probabilistic, rateLimiting, or remote\nsampler_type = \n# Sampler configuration parameter\n# for \"const\" sampler, 0 or 1 for always false/true respectively\n# for \"probabilistic\" sampler, a probability between 0.0 and 1.0\n# for \"rateLimiting\" sampler, the number of spans per second\n# for \"remote\" sampler, param is the same as for \"probabilistic\"\n#   and indicates the initial sampling rate before the actual one\n#   is received from the sampling server (set at sampling_server_url)\nsampler_param = \n# specifies the URL of the sampling server when sampler_type is remote\nsampling_server_url = \n\n[tracing.opentelemetry.jaeger]\n# jaeger destination (ex http://localhost:14268/api/traces)\naddress = \n# Propagation specifies the text map propagation format: w3c, jaeger\npropagation = \n\n# This is a configuration for OTLP exporter with GRPC protocol\n[tracing.opentelemetry.otlp]\n# otlp destination (ex localhost:4317)\naddress = \n# Propagation specifies the text map propagation format: w3c, jaeger\npropagation = \n# Toggles the insecure communication setting, defaults to `true`.\n# When set to `false`, the OTLP client will use TLS credentials with the default system cert pool for communication.\ninsecure = \n\n#################################### External Image Storage ##############\n[external_image_storage]\n# Used for uploading images to public servers so they can be included in slack/email messages.\n# You can choose between (s3, webdav, gcs, azure_blob, local)\nprovider = \n\n[external_image_storage.s3]\nendpoint = \npath_style_access = \nbucket_url = \nbucket = \nregion = \npath = \naccess_key = \nsecret_key = \n\n[external_image_storage.webdav]\nurl = \nusername = \npassword = \npublic_url = \n\n[external_image_storage.gcs]\nkey_file = \nbucket = \npath = \nenable_signed_urls = false\nsigned_url_expiration = \n\n[external_image_storage.azure_blob]\naccount_name = \naccount_key = \ncontainer_name = \nsas_token_expiration_days = \n\n[external_image_storage.local]\n# does not require any configuration\n\n[rendering]\n# Options to configure a remote HTTP image rendering service, e.g. using https://github.com/grafana/grafana-image-renderer.\n# URL to a remote HTTP image renderer service, e.g. http://localhost:8081/render, will enable Grafana to render panels and dashboards to PNG-images using HTTP requests to an external service.\nserver_url = \n# If the remote HTTP image renderer service runs on a different server than the Grafana server you may have to configure this to a URL where Grafana is reachable, e.g. http://grafana.domain/.\n# The `callback_url` can also be configured to support usage of the image renderer running as a plugin with support for SSL / HTTPS. For example https://localhost:3000/.\ncallback_url = \n# An auth token that will be sent to and verified by the renderer. The renderer will deny any request without an auth token matching the one configured on the renderer side.\nrenderer_token = \n# Concurrent render request limit affects when the /render HTTP endpoint is used. Rendering many images at the same time can overload the server,\n# which this setting can help protect against by only allowing a certain amount of concurrent requests.\nconcurrent_render_request_limit = 30\n# Determines the lifetime of the render key used by the image renderer to access and render Grafana.\n# This setting should be expressed as a duration. Examples: 10s (seconds), 5m (minutes), 2h (hours).\n# Default is 5m. This should be more than enough for most deployments.\n# Change the value only if image rendering is failing and you see `Failed to get the render key from cache` in Grafana logs.\nrender_key_lifetime = 5m\n# Default width for panel screenshot\ndefault_image_width = 1000\n# Default height for panel screenshot\ndefault_image_height = 500\n# Default scale for panel screenshot\ndefault_image_scale = 1\n\n[panels]\n# here for to support old env variables, can remove after a few months\nenable_alpha = false\ndisable_sanitize_html = false\n\n[plugins]\nenable_alpha = false\napp_tls_skip_verify_insecure = false\n# Enter a comma-separated list of plugin identifiers to identify plugins to load even if they are unsigned. Plugins with modified signatures are never loaded.\nallow_loading_unsigned_plugins = \n# Enable or disable installing / uninstalling / updating plugins directly from within Grafana.\nplugin_admin_enabled = true\nplugin_admin_external_manage_enabled = false\nplugin_catalog_url = https://grafana.com/grafana/plugins/\n# Enter a comma-separated list of plugin identifiers to hide in the plugin catalog.\nplugin_catalog_hidden_plugins = \n# Log all backend requests for core and external plugins.\nlog_backend_requests = false\n# Disable download of the public key for verifying plugin signature.\npublic_key_retrieval_disabled = false\n# Force download of the public key for verifying plugin signature on startup. If disabled, the public key will be retrieved every 10 days.\n# Requires public_key_retrieval_disabled to be false to have any effect.\npublic_key_retrieval_on_startup = false\n# Enter a comma-separated list of plugin identifiers to avoid loading (including core plugins). These plugins will be hidden in the catalog.\ndisable_plugins = \n# Comma separated list of plugin ids for which angular deprecation UI should be disabled\nhide_angular_deprecation = \n# Comma separated list of plugin ids for which environment variables should be forwarded. Used only when feature flag pluginsSkipHostEnvVars is enabled.\nforward_host_env_vars = \n# Comma separated list of plugin ids to install as part of the startup process.\n# These will be installed, by default, asynchronously (in the background) while starting Grafana.\npreinstall = \n# Comma separated list of plugin ids to install before the startup process\n# These will be installed before starting Grafana. Useful when used with provisioning.\npreinstall_sync = \n# Disables preinstall feature. It has the same effect as setting preinstall to an empty list.\npreinstall_disabled = false\n# Update strategy for plugins.\n# Available options: \"latest\", \"minor\"\nupdate_strategy = minor\n\n#################################### Grafana Live ##########################################\n[live]\n# max_connections to Grafana Live WebSocket endpoint per Grafana server instance. See Grafana Live docs\n# if you are planning to make it higher than default 100 since this can require some OS and infrastructure\n# tuning. 0 disables Live, -1 means unlimited connections.\nmax_connections = 100\n\n# message_size_limit is the maximum size in bytes of Websocket messages from clients. Defaults to 64KB.\n# The limit can be disabled by setting it to -1.\nmessage_size_limit = 65536\n\n# allowed_origins is a comma-separated list of origins that can establish connection with Grafana Live.\n# If not set then origin will be matched over root_url. Supports wildcard symbol \"*\".\nallowed_origins = \n\n# engine defines an HA (high availability) engine to use for Grafana Live. By default no engine used - in\n# this case Live features work only on a single Grafana server.\n# Available options: \"redis\".\nha_engine = \n\n# ha_engine_address sets a connection address for Live HA engine. Depending on engine type address format can differ.\n# For now we only support Redis connection address in \"host:port\" format.\nha_engine_address = 127.0.0.1:6379\n\n# ha_engine_password allows setting an optional password to authenticate with the engine\nha_engine_password = \n\n# ha_prefix is a prefix for keys in the HA engine. It's used to separate keys for different Grafana instances.\nha_prefix = \n\n#################################### Grafana Image Renderer Plugin ##########################\n[plugin.grafana-image-renderer]\n# Instruct headless browser instance to use a default timezone when not provided by Grafana, e.g. when rendering panel image of alert.\n# See ICUs metaZones.txt (https://cs.chromium.org/chromium/src/third_party/icu/source/data/misc/metaZones.txt) for a list of supported\n# timezone IDs. Fallbacks to TZ environment variable if not set.\nrendering_timezone = America/Chicago\n\n# Instruct headless browser instance to use a default language when not provided by Grafana, e.g. when rendering panel image of alert.\n# Please refer to the HTTP header Accept-Language to understand how to format this value, e.g. 'fr-CH, fr;q=0.9, en;q=0.8, de;q=0.7, *;q=0.5'.\nrendering_language = \n\n# Instruct headless browser instance to use a default device scale factor when not provided by Grafana, e.g. when rendering panel image of alert.\n# Default is 1. Using a higher value will produce more detailed images (higher DPI), but will require more disk space to store an image.\nrendering_viewport_device_scale_factor = \n\n# Instruct headless browser instance whether to ignore HTTPS errors during navigation. Per default HTTPS errors are not ignored. Due to\n# the security risk it's not recommended to ignore HTTPS errors.\nrendering_ignore_https_errors = \n\n# Instruct headless browser instance whether to capture and log verbose information when rendering an image. Default is false and will\n# only capture and log error messages. When enabled, debug messages are captured and logged as well.\n# For the verbose information to be included in the Grafana server log you have to adjust the rendering log level to debug, configure\n# [log].filter = rendering:debug.\nrendering_verbose_logging = \n\n# Instruct headless browser instance whether to output its debug and error messages into running process of remote rendering service.\n# Default is false. This can be useful to enable (true) when troubleshooting.\nrendering_dumpio = \n\n# Instruct headless browser instance whether to register metrics for the duration of every rendering step. Default is false.\n# This can be useful to enable (true) when optimizing the rendering mode settings to improve the plugin performance or when troubleshooting.\nrendering_timing_metrics = \n\n# This is a configuration for OTLP exporter with HTTP protocol, set this URL to enable tracing (ex: http://localhost:4318/v1/traces). Default to empty (tracing disabled).\nrendering_tracing_url = \n\n# Additional arguments to pass to the headless browser instance. Default is --no-sandbox. The list of Chromium flags can be found\n# here (https://peter.sh/experiments/chromium-command-line-switches/). Multiple arguments is separated with comma-character.\nrendering_args = \n\n# You can configure the plugin to use a different browser binary instead of the pre-packaged version of Chromium.\n# Please note that this is not recommended, since you may encounter problems if the installed version of Chrome/Chromium is not\n# compatible with the plugin.\nrendering_chrome_bin = \n\n# Instruct how headless browser instances are created. Default is 'default' and will create a new browser instance on each request.\n# Mode 'clustered' will make sure that only a maximum of browsers/incognito pages can execute concurrently.\n# Mode 'reusable' will have one browser instance and will create a new incognito page on each request.\nrendering_mode = \n\n# When rendering_mode = clustered, you can instruct how many browsers or incognito pages can execute concurrently. Default is 'browser'\n# and will cluster using browser instances.\n# Mode 'context' will cluster using incognito pages.\nrendering_clustering_mode = \n# When rendering_mode = clustered, you can define the maximum number of browser instances/incognito pages that can execute concurrently. Default is '5'.\nrendering_clustering_max_concurrency = \n# When rendering_mode = clustered, you can specify the duration a rendering request can take before it will time out. Default is `30` seconds.\nrendering_clustering_timeout = \n\n# Limit the maximum viewport width, height and device scale factor that can be requested.\nrendering_viewport_max_width = \nrendering_viewport_max_height = \nrendering_viewport_max_device_scale_factor = \n\n# Change the listening host and port of the gRPC server. Default host is 127.0.0.1 and default port is 0 and will automatically assign\n# a port not in use.\ngrpc_host = 127.0.0.1\ngrpc_port = 0\n\n[enterprise]\nlicense_path = \n\n[feature_toggles]\n# there are currently two ways to enable feature toggles in the `grafana.ini`.\n# you can either pass an array of feature you want to enable to the `enable` field or\n# configure each toggle by setting the name of the toggle to true/false. Toggles set to true/false\n# will take precedence over toggles in the `enable` list.\n\n# enable = feature1,feature2\nenable = \n\n# Some features are enabled by default, see:\n# https://grafana.com/docs/grafana/next/setup-grafana/configure-grafana/feature-toggles/\n# To enable features by default, set `Expression:  \"true\"` in:\n# https://github.com/grafana/grafana/blob/main/pkg/services/featuremgmt/registry.go\n\n# feature1 = true\n# feature2 = false\n\n[feature_toggles.openfeature]\n# This is EXPERIMENTAL. Please, do not use this section\nprovider = static\n\n[feature_toggles.openfeature.context]\n# This is EXPERIMENTAL. Please, do not use this section\n# instance = \"grafana\"\n# version = 11.0.0\n\n[time_picker]\n# Custom quick ranges for the time picker. Each quick range has a display name, a from value, and a to value.\n# Format: [{\"from\":\"now-5m\",\"to\":\"now\",\"display\":\"Last 5 minutes\"},{\"from\":\"now-15m\",\"to\":\"now\",\"display\":\"Last 15 minutes\"}]\nquick_ranges = \n\n[date_formats]\n# For information on what formatting patterns that are supported https://momentjs.com/docs/#/displaying/\n\n# Default system date format used in time range picker and other places where full time is displayed\nfull_date = YYYY-MM-DD HH:mm:ss\n\n# Used by graph and other places where we only show small intervals\ninterval_second = HH:mm:ss\ninterval_minute = HH:mm\ninterval_hour = MM/DD HH:mm\ninterval_day = MM/DD\ninterval_month = YYYY-MM\ninterval_year = YYYY\n\n# Experimental feature\nuse_browser_locale = false\n\n# Default timezone for user preferences. Options are 'browser' for the browser local timezone or a timezone name from IANA Time Zone database, e.g. 'UTC' or 'Europe/Amsterdam' etc.\ndefault_timezone = browser\n\n[expressions]\n# Enable or disable the expressions functionality.\nenabled = true\n\n[geomap]\n# Set the JSON configuration for the default basemap\ndefault_baselayer_config = \n\n# Enable or disable loading other base map layers\nenable_custom_baselayers = true\n\n#################################### Support Bundles #####################################\n[support_bundles]\n# Enable support bundle creation (default: true)\nenabled = true\n# Only server admins can generate and view support bundles (default: true)\nserver_admin_only = true\n# If set, bundles will be encrypted with the provided public keys separated by whitespace\npublic_keys = \"\"\n\n#################################### Storage ################################################\n\n[storage]\n# Allow uploading SVG files without sanitization.\nallow_unsanitized_svg_upload = false\n\n#################################### Search ################################################\n\n[search]\n# Defines the number of dashboards loaded at once in a batch during a full reindex.\n# This is a temporary settings that might be removed in the future.\ndashboard_loading_batch_size = 200\n\n# Defines the frequency of a full search reindex.\n# This is a temporary settings that might be removed in the future.\nfull_reindex_interval = 5m\n\n# Defines the frequency of partial index updates based on recent changes such as dashboard updates.\n# This is a temporary settings that might be removed in the future.\nindex_update_interval = 10s\n\n# Move an app plugin referenced by its id (including all its pages) to a specific navigation section\n# Format: <Plugin ID> = <Section ID> <Sort Weight>\n[navigation.app_sections]\n\n# Move a specific app plugin page (referenced by its `path` field) to a specific navigation section\n# Format: <Page URL> = <Section ID> <Sort Weight>\n[navigation.app_standalone_pages]\n\n#################################### Secure Socks5 Datasource Proxy #####################################\n[secure_socks_datasource_proxy]\nenabled = false\nroot_ca_cert = \nclient_key = \nclient_cert = \nserver_name = \n# The address of the socks5 proxy datasources should connect to\nproxy_address = \n# Determines if the secure socks proxy should be shown on the datasources page, defaults to true if the feature is enabled\nshow_ui = true\n# Disables TLS in the secure socks proxy\nallow_insecure = false\n\n################################## Feature Management ##############################################\n# Options to configure the experimental Feature Toggle Admin Page feature, which is behind the `featureToggleAdminPage` feature toggle. Use at your own risk.\n[feature_management]\n# Allows editing of feature toggles in the feature management page\nallow_editing = false\n\n# Allow customization of URL for the controller that manages feature toggles\nupdate_webhook = \n\n# Allow configuring an auth token for feature management update requests\nupdate_webhook_token = \n\n# Hides specific feature toggles from the feature management page\nhidden_toggles = \n\n# Disables updating specific feature toggles in the feature management page\nread_only_toggles = \n\n#################################### Public Dashboards #####################################\n[public_dashboards]\n# Set to false to disable public dashboards\nenabled = true\n\n###################################### Cloud Migration ######################################\n[cloud_migration]\n# Set to true to enable target-side migration UI\nis_target = false\n# Token used to send requests to grafana com\ngcom_api_token = \n# How long to wait for a request sent to gms to start a snapshot to complete\nstart_snapshot_timeout = 5s\n# How long to wait for a request sent to gms to validate a key to complete\nvalidate_key_timeout = 5s\n# How long to wait for a request sent to gms to get a snapshot status to complete\nget_snapshot_status_timeout = 5s\n# How long to wait for a request sent to gms to create a presigned upload url\ncreate_upload_url_timeout = 5s\n# How long to wait for a request sent to gms to report an event\nreport_event_timeout = 5s\n# How long to wait for a request to fetch an instance to complete\nfetch_instance_timeout = 5s\n# How long to wait for a request to create an access policy to complete\ncreate_access_policy_timeout = 5s\n# How long to wait for a request to create to fetch an access policy to complete\nfetch_access_policy_timeout = 5s\n# How long to wait for a request to create to delete an access policy to complete\ndelete_access_policy_timeout = 5s\n# The domain name used to access cms\ndomain = grafana.net\n# Folder used to store snapshot files. Defaults to the home dir\nsnapshot_folder = \n# How frequently should the frontend UI poll for changes while resources are migrating\nfrontend_poll_interval = 2s\n# Controls how the Alert Rules are migrated. Available choices: \"paused\" and \"unchanged\". Default: \"paused\".\n# With \"paused\", all Alert Rules will be created in Paused state. This is helpful to avoid double notifications.\n# With \"unchanged\", all Alert Rules will be created with the pause state unchanged coming from the source instance.\nalert_rules_state = \"paused\"\n\n###################################### Secrets Manager ######################################\n[secrets_manager]\n# Used for signing\nsecret_key = SW2YcwTIb9zpOOhoPsMm\n# Current key provider used for envelope encryption, default to static value specified by secret_key\n# Using empty/default provider instead of secretKey.v1 to avoid configuration issues\nencryption_provider = \n# List of configured key providers, space separated (Enterprise only): e.g., awskms.v1 azurekv.v1\navailable_encryption_providers = \n\n################################## Frontend development configuration ###################################\n# Warning! Any settings placed in this section will be available on `process.env.frontend_dev_{foo}` within frontend code\n# Any values placed here may be accessible to the UI. Do not place sensitive information here.\n[frontend_dev]\n# Should UI tests fail when console log/warn/erroring?\n# Does not affect the result when running on CI - only for allowing devs to choose this behaviour locally\nfail_tests_on_console = true\n# Whether to enable betterer eslint rules for local development\n# Useful if you want to always see betterer rules that we're trying to fix so they're more prevalent\nbetterer_eslint_rules = false\n"
  headscale-config:
    name: my-media-stack_headscale-config
    content: |
      ---
      # headscale will look for a configuration file named `config.yaml` (or `config.json`) in the following order:
      #
      # - `/etc/headscale`
      # - `~/.headscale`
      # - current working directory

      # The url clients will connect to.
      # Typically this will be a domain like:
      #
      # https://myheadscale.example.com:443
      #
      #server_url: https://headscale-server.bolabaden.org:443
      server_url: http://headscale-server:8080

      # Address to listen to / bind to on the server
      #
      listen_addr: 0.0.0.0:8081

      # Address to listen to /metrics and /debug, you may want
      # to keep this endpoint private to your internal network
      metrics_listen_addr: 127.0.0.1:8080

      # Address to listen for gRPC.
      # gRPC is used for controlling a headscale server
      # remotely with the CLI
      # Note: Remote access _only_ works if you have
      # valid certificates.

      grpc_listen_addr: 0.0.0.0:9090

      # Allow the gRPC admin interface to run in INSECURE
      # mode. This is not recommended as the traffic will
      # be unencrypted. Only enable if you know what you
      # are doing.
      grpc_allow_insecure: false

      # Private key used encrypt the traffic between headscale
      # and Tailscale clients.
      # The private key file which will be
      # autogenerated if it's missing
      private_key_path: /var/lib/headscale/private.key

      # The Noise section includes specific configuration for the
      # TS2021 Noise protocol
      noise:
        # The Noise private key is used to encrypt the traffic between headscale and
        # Tailscale clients when using the new Noise-based protocol. A missing key
        # will be automatically generated.
        private_key_path: /var/lib/headscale/noise_private.key

      # List of IP prefixes to allocate tailaddresses from.
      # Each prefix consists of either an IPv4 or IPv6 address,
      # and the associated prefix length, delimited by a slash.
      # It must be within IP ranges supported by the Tailscale
      # client - i.e., subnets of 100.64.0.0/10 and fd7a:115c:a1e0::/48.
      # See below:
      # IPv6: https://github.com/tailscale/tailscale/blob/22ebb25e833264f58d7c3f534a8b166894a89536/net/tsaddr/tsaddr.go#LL81C52-L81C71
      # IPv4: https://github.com/tailscale/tailscale/blob/22ebb25e833264f58d7c3f534a8b166894a89536/net/tsaddr/tsaddr.go#L33
      # Any other range is NOT supported, and it will cause unexpected issues.
      prefixes:
        v4: 100.64.0.0/10
        v6: fd7a:115c:a1e0::/48

        # Strategy used for allocation of IPs to nodes, available options:
        # - sequential (default): assigns the next free IP from the previous given IP.
        # - random: assigns the next free IP from a pseudo-random IP generator (crypto/rand).
        allocation: random

      # DERP is a relay system that Tailscale uses when a direct
      # connection cannot be established.
      # https://tailscale.com/blog/how-tailscale-works/#encrypted-tcp-relays-derp
      #
      # headscale needs a list of DERP servers that can be presented
      # to the clients.
      derp:
        server:
          # If enabled, runs the embedded DERP server and merges it into the rest of the DERP config
          # The Headscale server_url defined above MUST be using https, DERP requires TLS to be in place
          enabled: false

          # Region ID to use for the embedded DERP server.
          # The local DERP prevails if the region ID collides with other region ID coming from
          # the regular DERP config.
          region_id: 999

          # Region code and name are displayed in the Tailscale UI to identify a DERP region
          region_code: "headscale"
          region_name: "Headscale Embedded DERP"

          # Listens over UDP at the configured address for STUN connections - to help with NAT traversal.
          # When the embedded DERP server is enabled stun_listen_addr MUST be defined.
          #
          # For more details on how this works, check this great article: https://tailscale.com/blog/how-tailscale-works/
          stun_listen_addr: "0.0.0.0:3478"

          # Private key used to encrypt the traffic between headscale DERP and
          # Tailscale clients. A missing key will be automatically generated.
          private_key_path: /var/lib/headscale/derp_server_private.key

          # This flag can be used, so the DERP map entry for the embedded DERP server is not written automatically,
          # it enables the creation of your very own DERP map entry using a locally available file with the parameter DERP.paths
          # If you enable the DERP server and set this to false, it is required to add the DERP server to the DERP map using DERP.paths
          automatically_add_embedded_derp_region: true

          # For better connection stability (especially when using an Exit-Node and DNS is not working),
          # it is possible to optionally add the public IPv4 and IPv6 address to the Derp-Map using:
          ipv4: 100.64.0.2
          ipv6: fd7a:115c:a1e0::2

        # List of externally available DERP maps encoded in JSON
        urls:
          - https://controlplane.tailscale.com/derpmap/default

        # Locally available DERP map files encoded in YAML
        #
        # This option is mostly interesting for people hosting
        # their own DERP servers:
        # https://tailscale.com/kb/1118/custom-derp-servers/
        #
        # paths:
        #   - /etc/headscale/derp-example.yaml
        paths: []

        # If enabled, a worker will be set up to periodically
        # refresh the given sources and update the derpmap
        # will be set up.
        auto_update_enabled: true

        # How often should we check for DERP updates?
        update_frequency: 24h

      # Disables the automatic check for headscale updates on startup
      disable_check_updates: false

      # Time before an inactive ephemeral node is deleted?
      ephemeral_node_inactivity_timeout: 30m

      database:
        # Database type. Available options: sqlite, postgres
        # Please note that using Postgres is highly discouraged as it is only supported for legacy reasons.
        # All new development, testing and optimisations are done with SQLite in mind.
        type: sqlite

        # Enable debug mode. This setting requires the log.level to be set to "debug" or "trace".
        debug: false

        # GORM configuration settings.
        gorm:
          # Enable prepared statements.
          prepare_stmt: true

          # Enable parameterized queries.
          parameterized_queries: true

          # Skip logging "record not found" errors.
          skip_err_record_not_found: true

          # Threshold for slow queries in milliseconds.
          slow_threshold: 1000

        # SQLite config
        sqlite:
          path: /var/lib/headscale/db.sqlite

          # Enable WAL mode for SQLite. This is recommended for production environments.
          # https://www.sqlite.org/wal.html
          write_ahead_log: true

          # Maximum number of WAL file frames before the WAL file is automatically checkpointed.
          # https://www.sqlite.org/c3ref/wal_autocheckpoint.html
          # Set to 0 to disable automatic checkpointing.
          wal_autocheckpoint: 1000

      log:
        # Output formatting for logs: text or json
        format: text
        level: info

      ## Policy
      # headscale supports Tailscale's ACL policies.
      # Please have a look to their KB to better
      # understand the concepts: https://tailscale.com/kb/1018/acls/
      policy:
        # The mode can be "file" or "database" that defines
        # where the ACL policies are stored and read from.
        mode: file
        # If the mode is set to "file", the path to a
        # HuJSON file containing ACL policies.
        path: ""

      ## DNS
      #
      # headscale supports Tailscale's DNS configuration and MagicDNS.
      # Please have a look to their KB to better understand the concepts:
      #
      # - https://tailscale.com/kb/1054/dns/
      # - https://tailscale.com/kb/1081/magicdns/
      # - https://tailscale.com/blog/2021-09-private-dns-with-magicdns/
      #
      # Please note that for the DNS configuration to have any effect,
      # clients must have the `--accept-dns=true` option enabled. This is the
      # default for the Tailscale client. This option is enabled by default
      # in the Tailscale client.
      #
      # Setting _any_ of the configuration and `--accept-dns=true` on the
      # clients will integrate with the DNS manager on the client or
      # overwrite /etc/resolv.conf.
      # https://tailscale.com/kb/1235/resolv-conf
      #
      # If you want stop Headscale from managing the DNS configuration
      # all the fields under `dns` should be set to empty values.
      dns:
        # Whether to use [MagicDNS](https://tailscale.com/kb/1081/magicdns/).
        magic_dns: true

        # Defines the base domain to create the hostnames for MagicDNS.
        # This domain _must_ be different from the server_url domain.
        # `base_domain` must be a FQDN, without the trailing dot.
        # The FQDN of the hosts will be
        # `hostname.base_domain` (e.g., _myhost.example.com_).
        base_domain: myscale.bolabaden.org

        # Whether to use the local DNS settings of a node (default) or override the
        # local DNS settings and force the use of Headscale's DNS configuration.
        override_local_dns: false

        # List of DNS servers to expose to clients.
        nameservers:
          global:
            - 1.1.1.1
            - 1.0.0.1
            - 2606:4700:4700::1111
            - 2606:4700:4700::1001

            # NextDNS (see https://tailscale.com/kb/1218/nextdns/).
            # "abc123" is example NextDNS ID, replace with yours.
            # - https://dns.nextdns.io/abc123

          # Split DNS (see https://tailscale.com/kb/1054/dns/),
          # a map of domains and which DNS server to use for each.
          split:
            {
      #        "headscale.bolabaden.org": [
      #          "1.1.1.1",
      #          "1.0.0.1",
      #          "2606:4700:4700::1111",
      #          "2606:4700:4700::1001"
      #        ]
            }
            # foo.bar.com:
            #   - 1.1.1.1
            # darp.headscale.net:
            #   - 1.1.1.1
            #   - 8.8.8.8

        # Set custom DNS search domains. With MagicDNS enabled,
        # your tailnet base_domain is always the first search domain.
        search_domains: []

        # Extra DNS records
        # so far only A and AAAA records are supported (on the tailscale side)
        # See: docs/ref/dns.md
        extra_records: []
        #   - name: "grafana.myvpn.example.com"
        #     type: "A"
        #     value: "100.64.0.3"
        #
        #   # you can also put it in one line
        #   - { name: "prometheus.myvpn.example.com", type: "A", value: "100.64.0.3" }
        #
        # Alternatively, extra DNS records can be loaded from a JSON file.
        # Headscale processes this file on each change.
        # extra_records_path: /var/lib/headscale/extra-records.json

      # Unix socket used for the CLI to connect without authentication
      # Note: for production you will want to set this to something like:
      unix_socket: /var/run/headscale/headscale.sock
      unix_socket_permission: "0770"

      # Logtail configuration
      # Logtail is Tailscales logging and auditing infrastructure, it allows the control panel
      # to instruct tailscale nodes to log their activity to a remote server.
      logtail:
        # Enable logtail for this headscales clients.
        # As there is currently no support for overriding the log server in headscale, this is
        # disabled by default. Enabling this will make your clients send logs to Tailscale Inc.
        enabled: false

      # Enabling this option makes devices prefer a random port for WireGuard traffic over the
      # default static port 41641. This option is intended as a workaround for some buggy
      # firewall devices. See https://tailscale.com/kb/1181/firewalls/ for more information.
      randomize_client_port: false
  jackett-indexer-blacklist.txt:
    name: my-media-stack_jackett-indexer-blacklist.txt
    content: |
      # List of slow/broken indexers to exclude from Jackett provisioning
      # These indexers cause timeouts (100+ seconds) and degrade Homepage performance
      # Remove these to ensure fast widget API responses
      audiobookbay
      audiobook bay
      ebookbay
      ebook bay
      animetosho
      anime tosho
      internetarchive
      internet archive
  jackett-init.sh:
    name: my-media-stack_jackett-init.sh
    content: |
      #!/usr/bin/with-contenv bash
      # Jackett initialization script - provisions config and removes slow indexers

      # Copy default ServerConfig if not exists
      mkdir -p /config/Jackett /config/Jackett/Indexers
      if [ ! -f /config/Jackett/ServerConfig.json ]; then
        cp /defaults/ServerConfig.json /config/Jackett/ServerConfig.json
        chown abc:abc /config/Jackett/ServerConfig.json
        echo "Provisioned Jackett ServerConfig.json"
      fi

      # Remove blacklisted indexers on every startup (self-healing)
      if [ -f /indexer-blacklist.txt ]; then
        while IFS= read -r indexer || [ -n "$$indexer" ]; do
          # Skip comments and empty lines
          case "$$indexer" in
            \#*|"") continue ;;
          esac
          # Remove the indexer files (both active and backups) - use exact match
          removed=0
          for file in /config/Jackett/Indexers/*.json*; do
            basename=$$(basename "$$file")
            if echo "$$basename" | grep -qi "$$indexer"; then
              rm -f "$$file" && echo "Removed slow indexer file: $$basename" && removed=1
            fi
          done
        done < /indexer-blacklist.txt
      fi
  jackett-serverconfig.json:
    name: my-media-stack_jackett-serverconfig.json
    content: |
      {
        "Port": 9117,
        "LocalBindAddress": "127.0.0.1",
        "AllowExternal": true,
        "AllowCORS": true,
        "APIKey": "$${JACKETT_API_KEY:-nnx0n84pcj7umynyd2pbid2nl1zzuemz}",
        "AdminPassword": "",
        "InstanceId": "jackett-bolabaden-$${DOMAIN}",
        "BlackholeDir": "/blackhole",
        "UpdateDisabled": false,
        "UpdatePrerelease": false,
        "BasePathOverride": "",
        "BaseUrlOverride": "",
        "CacheEnabled": true,
        "CacheTtl": 2100,
        "CacheMaxResultsPerIndexer": 1000,
        "FlareSolverrUrl": "http://flaresolverr:8191",
        "FlareSolverrMaxTimeout": 55000,
        "OmdbApiKey": "",
        "OmdbApiUrl": "",
        "ProxyType": -1,
        "ProxyUrl": "",
        "ProxyPort": null,
        "ProxyUsername": "",
        "ProxyPassword": "",
        "ProxyIsAnonymous": true
      }
  loki.yaml:
    name: my-media-stack_loki.yaml
    content: |
      # This is a complete configuration to deploy Loki backed by the filesystem.
      # The index will be shipped to the storage via tsdb-shipper.
      auth_enabled: false

      server:
        http_listen_port: 3100

      common:
        ring:
          instance_addr: 127.0.0.1
          kvstore:
            store: inmemory
        replication_factor: 1
        path_prefix: /tmp/loki

      schema_config:
        configs:
        - from: 2020-05-15
          store: tsdb
          object_store: filesystem
          schema: v13
          index:
            prefix: index_
            period: 24h

      storage_config:
        filesystem:
          directory: /tmp/loki/chunks
      limits_config:
        volume_enabled: true
  mcp_servers.json:
    name: my-media-stack_mcp_servers.json
    content: |
      {
        "mcpServers": {
          "curl": {
            "command": "npx",
            "args": [
              "-y",
              "@mcp-get-community/server-curl"
            ]
          },
          "firecrawl": {
            "command": "npx",
            "args": [
              "-y",
              "firecrawl-mcp"
            ],
            "env": {
              "FIRECRAWL_API_KEY": "fc-e0e316bead3d497fb24d42ea21d5daf6",
              "FIRE_CRAWL_API_KEY": "fc-e0e316bead3d497fb24d42ea21d5daf6",
              "FIRECRAWL_API_URL": "https://firecrawl-api.bolabaden.org",
              "FIRE_CRAWL_API_URL": "https://firecrawl-api.bolabaden.org"
            }
          },
          "goalStory": {
            "command": "npx",
            "args": [
              "-y",
              "goalstory-mcp",
              "https://prod-goalstory-rqc2.encr.app",
              "c5e7d1ea44eab701bae258695253914c2579d19c637694e03136e3867e081195"
            ],
            "transportType": "stdio"
          },
          "tavily": {
            "command": "npx",
            "args": [
              "-y",
              "tavily-mcp"
            ],
            "env": {
              "TAVILY_API_KEY": "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"
            }
          }
        }
      }
  nginx-traefik-extensions.conf:
    name: my-media-stack_nginx-traefik-extensions.conf
    content: "user nginx;\nworker_processes auto;\n\nerror_log /dev/stderr warn;\npid /var/run/nginx.pid;\n\nevents {\n    worker_connections 1024;\n    use epoll;\n    multi_accept on;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n    log_format main '\\n\\r$$time_iso8601 | $$status | $$remote_addr | $$http_host | $$request | $${request_time}ms | '\n                    'auth_method=\"$$auth_method\" | $$http_user_agent | '\n                    'request_method=$$request_method | '\n                    'request_uri=$$request_uri | '\n                    'query_string=$$query_string | '\n                    'content_type=$$content_type | '\n                    'server_protocol=$$server_protocol | '\n                    'request_scheme=$$scheme | '\n                    '\\n\\rheaders: {'\n                      '\"accept\":\"$$http_accept\",'\n                      '\"accept_encoding\":\"$$http_accept_encoding\",'\n                      '\"cookie\":\"$$http_cookie\",'\n                      '\"x_forwarded_for\":\"$$http_x_forwarded_for\",'\n                      '\"x_forwarded_port\":\"$$http_x_forwarded_port\",'\n                      '\"x_forwarded_proto\":\"$$http_x_forwarded_proto\",'\n                      '\"x_forwarded_host\":\"$$http_x_forwarded_host\",'\n                      '\"x_real_ip\":\"$$http_x_real_ip\",'\n                      '\"x_api_key\":\"$$http_x_api_key\",'\n                    '}';\n\n    # Output all access logs to stdout for Docker console visibility\n    access_log /dev/stdout main;\n    error_log /dev/stderr warn;\n\n    # Basic settings\n    sendfile on;\n    tcp_nopush on;\n    tcp_nodelay on;\n    keepalive_timeout 65;\n    types_hash_max_size 2048;\n    server_tokens off;\n    \n    # Fix for long API keys in map module\n    map_hash_bucket_size 128;\n\n    # Rate limiting zones\n    limit_req_zone $$binary_remote_addr zone=auth:10m rate=10r/s;\n\n    set_real_ip_from 10.0.6.0/24;\n    set_real_ip_from 10.0.7.0/24;\n    real_ip_header X-Forwarded-For;\n    real_ip_recursive on;\n\n    geo $$ip_whitelisted {\n        default 0;\n        10.0.6.0/24 1;\n        10.0.7.0/24     1;\n    }\n\n    map $$http_x_api_key $$api_key_valid {\n        default 0;\n        \"sk_IQys9kpENSiYY8lFuCslok3PauKBRSzeGprmvPfiMWAM9neeXoSqCZW7pMlWKbqPrwtF33kh1F73vf7D4PBpVfZJ1reHEL8d6ny6J03Ho\" 1;\n        # Add more API keys here as needed\n    }\n\n    upstream tinyauth {\n        server auth:3000;\n    }\n\n    server {\n        listen 80 default_server;\n        server_name _;\n\n        set $$auth_passed 0;\n        set $$auth_method \"none\";\n\n        if ($$api_key_valid = 1) {\n            set $$auth_passed 1;\n            set $$auth_method \"api_key\";\n        }\n\n        if ($$ip_whitelisted = 1) {\n            set $$auth_passed 1;\n            set $$auth_method \"ip_whitelist\";\n        }\n\n        location /auth {\n            limit_req zone=auth burst=20 nodelay;\n            if ($$auth_passed = 1) {\n                add_header X-Auth-Method \"$$auth_method\" always;\n                add_header X-Auth-Passed \"true\" always;\n                return 200 \"OK\";\n            }\n\n            proxy_pass http://tinyauth/api/auth/traefik;\n            proxy_pass_request_body off;\n            proxy_set_header Content-Length \"\";\n            proxy_set_header X-Original-URI $$http_x_original_uri;\n            proxy_set_header X-Original-Method $$http_x_original_method;\n            proxy_set_header X-Real-IP $$remote_addr;\n            proxy_set_header X-Forwarded-For $$proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $$scheme;\n            proxy_set_header X-Forwarded-Host $$http_x_forwarded_host;\n            add_header X-Auth-Method \"tinyauth\" always;\n            access_log /dev/stdout main;\n        }\n\n        location /health {\n            access_log /dev/stdout main;\n            return 200 \"nginx service healthy\\n\";\n            add_header Content-Type text/plain;\n        }\n\n        location / {\n            access_log /dev/stdout main;\n            return 200 \"nginx service healthy\\n\";\n            add_header Content-Type text/plain;\n        }\n    }\n}\n"
  node-exporter-dashboard.json:
    name: my-media-stack_node-exporter-dashboard.json
    content: |
      {
        "__inputs": [
          {
            "name": "DS_PROMETHEUS",
            "label": "prometheus",
            "description": "",
            "type": "datasource",
            "pluginId": "prometheus",
            "pluginName": "Prometheus"
          }
        ],
        "__requires": [
          {
            "type": "panel",
            "id": "gauge",
            "name": "Gauge",
            "version": ""
          },
          {
            "type": "grafana",
            "id": "grafana",
            "name": "Grafana",
            "version": "11.6.1"
          },
          {
            "type": "datasource",
            "id": "prometheus",
            "name": "Prometheus",
            "version": "1.0.0"
          },
          {
            "type": "panel",
            "id": "stat",
            "name": "Stat",
            "version": ""
          },
          {
            "type": "panel",
            "id": "timeseries",
            "name": "Time series",
            "version": ""
          }
        ],
        "annotations": {
          "list": [
            {
              "builtIn": 1,
              "datasource": {
                "type": "datasource",
                "uid": "grafana"
              },
              "enable": true,
              "hide": true,
              "iconColor": "rgba(0, 211, 255, 1)",
              "name": "Annotations & Alerts",
              "target": {
                "limit": 100,
                "matchAny": false,
                "tags": [],
                "type": "dashboard"
              },
              "type": "dashboard"
            }
          ]
        },
        "editable": true,
        "fiscalYearStartMonth": 0,
        "graphTooltip": 1,
        "id": null,
        "links": [],
        "panels": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "victoriametrics_uid"
            },
            "description": "Overall CPU busy percentage (averaged across all cores)",
            "fieldConfig": {
              "defaults": {
                "color": {
                  "mode": "thresholds"
                },
                "decimals": 1,
                "mappings": [
                  {
                    "options": {
                      "match": "null",
                      "result": {
                        "text": "N/A"
                      }
                    },
                    "type": "special"
                  }
                ],
                "max": 100,
                "min": 0,
                "thresholds": {
                  "mode": "absolute",
                  "steps": [
                    {
                      "color": "rgba(50, 172, 45, 0.97)"
                    },
                    {
                      "color": "rgba(237, 129, 40, 0.89)",
                      "value": 85
                    },
                    {
                      "color": "rgba(245, 54, 54, 0.9)",
                      "value": 95
                    }
                  ]
                },
                "unit": "percent"
              },
              "overrides": []
            },
            "gridPos": {
              "h": 4,
              "w": 3,
              "x": 0,
              "y": 0
            },
            "id": 20,
            "options": {
              "minVizHeight": 75,
              "minVizWidth": 75,
              "orientation": "auto",
              "reduceOptions": {
                "calcs": [
                  "lastNotNull"
                ],
                "fields": "",
                "values": false
              },
              "showThresholdLabels": false,
              "showThresholdMarkers": true,
              "sizing": "auto"
            },
            "pluginVersion": "11.6.1",
            "targets": [
              {
                "editorMode": "code",
                "exemplar": false,
                "expr": "100 * (1 - avg(rate(node_cpu_seconds_total{mode=\"idle\", instance=\"$$node\"}[$$__rate_interval])))",
                "hide": false,
                "instant": true,
                "intervalFactor": 1,
                "legendFormat": "",
                "range": false,
                "refId": "A",
                "step": 240
              }
            ],
            "title": "CPU Busy",
            "type": "gauge"
          }
        ],
        "refresh": "1m",
        "schemaVersion": 41,
        "tags": [
          "linux"
        ],
        "templating": {
          "list": [
            {
              "current": {},
              "includeAll": false,
              "label": "Datasource",
              "name": "DS_PROMETHEUS",
              "options": [],
              "query": "prometheus",
              "refresh": 1,
              "regex": "",
              "type": "datasource"
            },
            {
              "current": {},
              "datasource": {
                "type": "prometheus",
                "uid": "victoriametrics_uid"
              },
              "definition": "",
              "includeAll": false,
              "label": "Job",
              "name": "job",
              "options": [],
              "query": {
                "query": "label_values(node_uname_info, job)",
                "refId": "Prometheus-job-Variable-Query"
              },
              "refresh": 1,
              "regex": "",
              "sort": 1,
              "type": "query"
            },
            {
              "current": {},
              "datasource": {
                "type": "prometheus",
                "uid": "victoriametrics_uid"
              },
              "definition": "label_values(node_uname_info{job=\"$$job\"}, instance)",
              "includeAll": false,
              "label": "Instance",
              "name": "node",
              "options": [],
              "query": {
                "query": "label_values(node_uname_info{job=\"$$job\"}, instance)",
                "refId": "Prometheus-node-Variable-Query"
              },
              "refresh": 1,
              "regex": "",
              "sort": 1,
              "type": "query"
            }
          ]
        },
        "time": {
          "from": "now-24h",
          "to": "now"
        },
        "timepicker": {},
        "timezone": "browser",
        "title": "Node Exporter Full",
        "uid": "node_exporter_full",
        "version": 1,
        "weekStart": ""
      }
  prometheus.yml:
    name: my-media-stack_prometheus.yml
    content: |
      global:
        scrape_interval: 15s
        evaluation_interval: 15s
        external_labels:
          cluster: 'homelab'
          replica: 'prometheus'

      # Alertmanager configuration
      alerting:
        alertmanagers:
          - static_configs:
              - targets:
                - alertmanager:9093

      # Load rules once and periodically evaluate them according to the global 'evaluation_interval'.
      rule_files:
        - "alert.rules"

      # Remote write configuration for VictoriaMetrics
      remote_write:
        - url: http://victoriametrics:8428/api/v1/write
          queue_config:
            max_samples_per_send: 10000
            capacity: 20000
            max_shards: 30
          write_relabel_configs:
            - source_labels: [__name__]
              regex: 'go_.*'
              action: drop

      # A scrape configuration containing exactly one endpoint to scrape:
      scrape_configs:
        # The job name is added as a label `job=<job_name>` to any timeseries scraped from this config.
        - job_name: 'prometheus'
          static_configs:
            - targets: ['localhost:9090']

        - job_name: 'victoriametrics'
          static_configs:
            - targets: ['victoriametrics:8428']
          scrape_interval: 5s
          metrics_path: /metrics

        - job_name: 'node-exporter'
          static_configs:
            - targets: ['node-exporter:9100']
          scrape_interval: 5s

        - job_name: 'cadvisor'
          static_configs:
            - targets: ['cadvisor:8080']
          scrape_interval: 5s

        - job_name: 'grafana'
          static_configs:
            - targets: ['grafana:3000']
          scrape_interval: 15s

        - job_name: 'traefik'
          static_configs:
            - targets: ['traefik:8080']
          scrape_interval: 5s

        - job_name: 'crowdsec'
          static_configs:
            - targets: ['crowdsec:6060']
          scrape_interval: 15s

        - job_name: 'docker'
          static_configs:
            - targets: ['host.docker.internal:9323']
          scrape_interval: 15s

        - job_name: 'flaresolverr'
          static_configs:
            - targets: ['flaresolverr:8191']
          scrape_interval: 15s
          metrics_path: /metrics

        - job_name: 'redis'
          static_configs:
            - targets: ['redis:6379']
          scrape_interval: 15s

        - job_name: 'mongodb'
          static_configs:
            - targets: ['mongodb:27017']
          scrape_interval: 30s

        - job_name: 'homepage'
          static_configs:
            - targets: ['homepage:3000']
          scrape_interval: 30s

        - job_name: 'portainer'
          static_configs:
            - targets: ['portainer:9000']
          scrape_interval: 30s

        - job_name: 'searxng'
          static_configs:
            - targets: ['searxng:8080']
          scrape_interval: 30s

        - job_name: 'code-server'
          static_configs:
            - targets: ['code-server:8443']
          scrape_interval: 30s

        - job_name: 'dozzle'
          static_configs:
            - targets: ['dozzle:8080']
          scrape_interval: 30s

        - job_name: 'bolabaden-nextjs'
          static_configs:
            - targets: ['bolabaden-nextjs:3000']
          scrape_interval: 30s

        # Blackbox exporter for endpoint monitoring
        - job_name: 'blackbox'
          metrics_path: /probe
          params:
            module: [http_2xx]
          static_configs:
            - targets:
              - https://grafana.bolabaden.org
              - https://victoriametrics.bolabaden.org
              - https://prometheus.bolabaden.org
              - https://traefik.bolabaden.org
              - https://crowdsec.bolabaden.org
              - https://flaresolverr.bolabaden.org
              - https://homepage.bolabaden.org
              - https://portainer.bolabaden.org
              - https://searxng.bolabaden.org
              - https://code-server.bolabaden.org
              - https://dozzle.bolabaden.org
              - https://bolabaden.org
          relabel_configs:
            - source_labels: [__address__]
              target_label: __param_target
            - source_labels: [__param_target]
              target_label: instance
            - target_label: __address__
              replacement: blackbox-exporter:9115
  promtail.yaml-override:
    name: my-media-stack_promtail.yaml-override
    content: |
      server:
        http_listen_port: 9080
        grpc_listen_port: 0

      positions:
        filename: /tmp/positions.yaml

      clients:
        - url: http://loki:3100/loki/api/v1/push

      scrape_configs:
        # Scrape Docker container logs via TCP proxy
        - job_name: docker
          docker_sd_configs:
            - host: tcp://dockerproxy-ro:2375
          relabel_configs:
            - source_labels: ["__meta_docker_container_name"]
              target_label: "container"
              regex: "/(.*)"
              replacement: "$$1"

        # Scrape host syslog and other files
        - job_name: system-logs
          static_configs:
            - targets:
                - localhost
              labels:
                job: varlogs
                __path__: /var/log/*.log

        # Scrape custom mounted logs
        - job_name: extra-logs
          static_configs:
            - targets:
                - localhost
              labels:
                job: extralogs
                __path__: /mnt/extra-logs/*.log
  rclone-mounts.json:
    name: my-media-stack_rclone-mounts.json
    content: |
      [
        {
          "fs": "pm:",
          "mountPoint": "/mnt/remote/premiumize",
          "LogLevel": "DEBUG",
          "mainOpt": {
            "LogLevel": "DEBUG"
          },
          "mountOpt": {
            "AllowNonEmpty": true,
            "AllowOther": true,
            "AttrTimeout": "87600h",
            "DirCacheTime": "60s",
            "DirPerms": "0777",
            "ExtraFlags": [
              "--config=/config/rclone/rclone.conf",
              "--log-level=DEBUG",
              "--log-file=/config/rclone/rclone.log"
            ],
            "FilePerms": "0666",
            "GID": 1000,
            "PollInterval": "30s",
            "UID": 1000
          },
          "vfsOpt": {
            "BufferSize": "128M",
            "CacheMaxAge": "2m",
            "CacheMaxSize": "100G",
            "CacheMode": "full",
            "CachePollInterval": "30s",
            "ChunkSize": "2M",
            "ChunkSizeLimit": "64M",
            "DiskSpaceTotalSize": "1T",
            "ExtraOptions": [
              "--vfs-refresh",
              "--transfers=16",
              "--checkers=16",
              "--multi-thread-streams=4",
              "--cache-dir=/mnt/remote/cache/realdebrid"
            ],
            "FastFingerprint": true,
            "MinFreeSpace": "1G",
            "ReadAhead": "128M",
            "ReadWait": "40ms"
          }
        },
        {
          "fs": "alldebrid:",
          "mountPoint": "/mnt/remote/alldebrid",
          "LogLevel": "DEBUG",
          "mainOpt": {
            "ExtraFlags": [
              "--cutoff-mode=cautious",
              "--network-mode",
              "--config=/config/rclone/rclone.conf",
              "--log-level=DEBUG",
              "--log-file=/config/rclone/rclone.log"
            ],
            "LogLevel": "DEBUG",
            "MultiThreadStreams": 0,
            "TPSLimit": 12,
            "Transfers": 8
          },
          "mountOpt": {
            "AllowNonEmpty": true,
            "AllowOther": true,
            "DirPerms": "0777",
            "ExtraFlags": [
              "--cache-dir=/mnt/remote/cache/alldebrid"
            ],
            "FilePerms": "0666",
            "VolumeName": "AllDebrid"
          },
          "vfsOpt": {
            "BufferSize": "128M",
            "CacheMaxAge": "1h",
            "CacheMaxSize": "100G",
            "CacheMode": "full",
            "ChunkSize": "128M",
            "ChunkSizeLimit": "0",
            "DirCacheTime": "160h",
            "ExtraOptions": [],
            "ReadAhead": "0"
          }
        }
      ]
  rclone.conf:
    name: my-media-stack_rclone.conf
    content: "[gcache]\ntype = cache\nremote = gdrive:x-san\nplex_url = http://plex:32400\nplex_username = boden.crouch@gmail.com\nplex_password = h4L0m4St3R327\nchunk_size = 16M\nplex_token = 18rS6gM-Wec22uq5Gw-U\ndb_path = /config/rclone/rclone-cache\nchunk_path = /config/rclone/rclone-cache\ninfo_age = 2d\nchunk_total_size = 20G\ndb_purge = true\n\n[gdrive]\ntype = drive\nclient_id = \nclient_secret = \nscope = drive\nroot_folder_id = \nservice_account_file = \ntoken = \nuse_trash = false\nchunk_size = 16M\n\n[gcrypt]\ntype = crypt\nremote = gcache:\nfilename_encryption = standard\ndirectory_name_encryption = true\npassword = \npassword2 = \n[plex]\ntype = webdav\nurl = https://plex.bolabaden.org/\nvendor = other\nuser = brunner56\n\n[zurg]\ntype = webdav\nurl = http://zurg:9999/dav\nvendor = other\npacer_min_sleep = 0\n\n[zurghttp]\ntype = http\nurl = http://zurg:9999/http/\nno_head = false\nno_slash = false\n\n[zurg-remote]\ntype = webdav\nurl = https://zurg.bolabaden.org/dav\nvendor = other\npacer_min_sleep = 0\n\n[zurghttp-remote]\ntype = http\nurl = https://zurg.bolabaden.org/http/\nno_head = false\nno_slash = false\n\n[alldebrid]\ntype = webdav\nurl = https://webdav.debrid.it/\nvendor = other\nuser = \n\n[pm]\ntype = premiumizeme\ntoken: {\"access_token\":\"\",\"token_type\":\"Bearer\",\"refresh_token\":\"\",\"expiry\":\"2035-08-29T06:37:06.7039897-05:00\",\"expires_in\":315360000}\n"
  session_manager.py:
    name: my-media-stack_session_manager.py
    file: /home/ubuntu/my-media-stack/projects/kotor/kotorscript-session-manager/session_manager.py
  session_manager_index.html:
    name: my-media-stack_session_manager_index.html
    file: /home/ubuntu/my-media-stack/projects/kotor/kotorscript-session-manager/index.html
  session_manager_waiting.html:
    name: my-media-stack_session_manager_waiting.html
    file: /home/ubuntu/my-media-stack/projects/kotor/kotorscript-session-manager/waiting.html
  traefik-dashboard.json:
    name: my-media-stack_traefik-dashboard.json
    content: |
      {
        "annotations": {
          "list": [
            {
              "builtIn": 1,
              "datasource": "-- Grafana --",
              "enable": true,
              "hide": true,
              "iconColor": "rgba(0, 211, 255, 1)",
              "name": "Annotations & Alerts",
              "type": "dashboard"
            }
          ]
        },
        "description": "Traefik reverse proxy monitoring",
        "editable": true,
        "gnetId": null,
        "graphTooltip": 0,
        "id": null,
        "panels": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "victoriametrics_uid"
            },
            "fieldConfig": {
              "defaults": {
                "color": {
                  "mode": "palette-classic"
                },
                "custom": {
                  "axisBorderShow": false,
                  "axisCenteredZero": false,
                  "axisColorMode": "text",
                  "axisLabel": "",
                  "axisPlacement": "auto",
                  "barAlignment": 0,
                  "barWidthFactor": 0.6,
                  "drawStyle": "line",
                  "fillOpacity": 10,
                  "gradientMode": "none",
                  "hideFrom": {
                    "legend": false,
                    "tooltip": false,
                    "viz": false
                  },
                  "insertNulls": false,
                  "lineInterpolation": "linear",
                  "lineWidth": 1,
                  "pointSize": 5,
                  "scaleDistribution": {
                    "type": "linear"
                  },
                  "showPoints": "never",
                  "spanNulls": false,
                  "stacking": {
                    "group": "A",
                    "mode": "none"
                  },
                  "thresholdsStyle": {
                    "mode": "off"
                  }
                },
                "mappings": [],
                "thresholds": {
                  "mode": "absolute",
                  "steps": [
                    {
                      "color": "green"
                    },
                    {
                      "color": "red",
                      "value": 80
                    }
                  ]
                },
                "unit": "reqps"
              },
              "overrides": []
            },
            "gridPos": {
              "h": 8,
              "w": 12,
              "x": 0,
              "y": 0
            },
            "id": 1,
            "options": {
              "legend": {
                "calcs": [],
                "displayMode": "list",
                "placement": "bottom",
                "showLegend": true
              },
              "tooltip": {
                "hideZeros": false,
                "mode": "multi",
                "sort": "none"
              }
            },
            "targets": [
              {
                "expr": "sum(rate(traefik_service_requests_total[5m])) by (service)",
                "format": "time_series",
                "intervalFactor": 1,
                "legendFormat": "{{service}}",
                "refId": "A"
              }
            ],
            "title": "Request Rate by Service",
            "type": "timeseries"
          }
        ],
        "refresh": "30s",
        "schemaVersion": 27,
        "style": "dark",
        "tags": [
          "traefik",
          "proxy"
        ],
        "time": {
          "from": "now-1h",
          "to": "now"
        },
        "timezone": "",
        "title": "Traefik Monitoring",
        "uid": "traefik_monitoring",
        "version": 1
      }
  traefik-dynamic.yaml:
    name: my-media-stack_traefik-dynamic.yaml
    content: "# yaml-language-server: $$schema=https://www.schemastore.org/traefik-v3-file-provider.json\nhttp:\n  routers:\n    catchall:\n      entryPoints:\n        - web\n        - websecure\n      service: noop@internal\n      rule: Host(`bolabaden.org`) || Host(`beatapostapita.bolabaden.org`) || HostRegexp(`^(.+)$$`)\n      priority: 1\n      middlewares:\n        - traefikerrorreplace@file\n  services:\n    nginx-traefik-extensions:\n      loadBalancer:\n        servers:\n          - url: http://nginx-traefik-extensions:80\n    bolabaden-nextjs:\n      loadBalancer:\n        servers:\n          - url: http://bolabaden-nextjs:3000\n  middlewares:\n    traefikerrorreplace:\n      plugin:\n        traefikerrorreplace:\n          matchStatus:\n            - 418\n          replaceStatus: 404\n    bolabaden-error-pages:\n      errors:\n        status:\n          - 400-599\n        service: bolabaden-nextjs@file\n        query: /api/error/{status}\n    nginx-auth:\n      forwardAuth:\n        address: http://nginx-traefik-extensions:80/auth\n        trustForwardHeader: true\n        authResponseHeaders: [\"X-Auth-Method\", \"X-Auth-Passed\", \"X-Middleware-Name\"]\n    strip-www:\n      redirectRegex:\n        regex: '^(http|https)?://www\\.(.+)$$'\n        replacement: '$$1://$$2'\n        permanent: false\n    crowdsec:\n      plugin:\n        bouncer:\n          # Enable the plugin (default: false)\n          enabled: true\n\n          # Log level (default: INFO, expected: INFO, DEBUG, ERROR)\n          logLevel: INFO\n\n          # File path to write logs (default: \"\")\n          logFilePath: \"\"\n\n          # Interval in seconds between metrics updates to Crowdsec (default: 600, <=0 disables metrics)\n          metricsUpdateIntervalSeconds: 600\n\n          # Mode for Crowdsec integration (default: live, expected: none, live, stream, alone, appsec)\n          crowdsecMode: live\n\n          # Enable Crowdsec Appsec Server (WAF) (default: false)\n          crowdsecAppsecEnabled: false\n\n          # Crowdsec Appsec Server host and port (default: \"crowdsec:7422\")\n          crowdsecAppsecHost: crowdsec:7422\n\n          # Crowdsec Appsec Server path (default: \"/\")\n          crowdsecAppsecPath: /\n\n          # Block request when Crowdsec Appsec Server returns 500 (default: true)\n          crowdsecAppsecFailureBlock: true\n\n          # Block request when Crowdsec Appsec Server is unreachable (default: true)\n          crowdsecAppsecUnreachableBlock: true\n\n          # Transmit only the first number of bytes to Crowdsec Appsec Server (default: 10485760 = 10MB)\n          crowdsecAppsecBodyLimit: 10485760\n\n          # Scheme for Crowdsec LAPI (default: http, expected: http, https)\n          crowdsecLapiScheme: http\n\n          # Crowdsec LAPI host and port (default: \"crowdsec:8080\")\n          crowdsecLapiHost: crowdsec:8080\n\n          # Crowdsec LAPI path (default: \"/\")\n          crowdsecLapiPath: /\n\n          # Crowdsec LAPI key for the bouncer (default: \"\")\n          crowdsecLapiKey: MZPvRnuiuYvU2BCsz7mrmaMTC58yGdTzBzZf2HMcls8\n\n          # Disable TLS verification for Crowdsec LAPI (default: false)\n          crowdsecLapiTlsInsecureVerify: false\n\n          # PEM-encoded CA for Crowdsec LAPI (default: \"\")\n          crowdsecLapiTlsCertificateAuthority: \"\"\n\n          # PEM-encoded client certificate for the Bouncer (default: \"\")\n          crowdsecLapiTlsCertificateBouncer: \"\"\n\n          # PEM-encoded client key for the Bouncer (default: \"\")\n          crowdsecLapiTlsCertificateBouncerKey: \"\"\n\n          # Name of the header in response when requests are cancelled (default: \"\")\n          remediationHeadersCustomName: \"\"\n\n          # Name of the header where real client IP is retrieved (default: \"X-Forwarded-For\")\n          forwardedHeadersCustomName: \"X-Forwarded-For\"      \n\n          # Enable Redis cache (default: false)\n          redisCacheEnabled: false\n\n          # Redis hostname and port (default: \"redis:6379\")\n          redisCacheHost: redis:6379\n\n          # Redis password (default: \"\")\n          redisCachePassword: \"\"\n\n          # Redis database selection (default: \"\")\n          redisCacheDatabase: \"\"\n\n          # Block request when Redis is unreachable (default: true, adds 1s delay)\n          redisCacheUnreachableBlock: true\n\n          # Default timeout in seconds for contacting Crowdsec LAPI (default: 10)\n          httpTimeoutSeconds: 10\n\n          # Interval between LAPI fetches in stream mode (default: 60)\n          updateIntervalSeconds: 60\n\n          # Max failures before blocking traffic in stream/alone mode (default: 0, -1 = never block)\n          updateMaxFailure: 0\n\n          # Maximum decision duration in live mode (default: 60)\n          defaultDecisionSeconds: 60\n\n          # HTTP status code for banned user (default: 403)\n          remediationStatusCode: 403\n\n          # CAPI Machine ID (used only in alone mode)\n          crowdsecCapiMachineId: \"\"\n\n          # CAPI Password (used only in alone mode)\n          crowdsecCapiPassword: \"\"\n\n          # CAPI Scenarios (used only in alone mode)\n          crowdsecCapiScenarios: []\n\n          # Captcha provider (expected: hcaptcha, recaptcha, turnstile)\n          captchaProvider: \"\"\n\n          # Captcha site key\n          captchaSiteKey: \"\"\n\n          # Captcha secret key\n          captchaSecretKey: \"\"\n\n          # Grace period after captcha validation before revalidation required (default: 1800s = 30m)\n          captchaGracePeriodSeconds: 1800\n\n          # Path to captcha template (default: /captcha.html)\n          captchaHTMLFilePath: /captcha.html\n\n          # Path to ban HTML file (default: \"\", disabled if empty)\n          banHTMLFilePath: \"\"\n\n          # List of trusted proxies in front of Traefik (default: [])\n          # As can be seen in the middleware declaration, we are actively defining all private class IPv4/IPv6 subnets as trusted IPs.\n          # This is necessary, as we want to trust our Traefik reverse proxy's HTTP headers like X-Forwarded-For and X-Real-IP.\n          # Those headers typically define the real IP address of our website visitors and threat actors, used by CrowdSec for decision making and banning.\n          forwardedHeadersTrustedIPs: &trustedIps\n            - \"127.0.0.1/32\"    # Loopback addresses\n            - \"10.0.0.0/8\"      # RFC1918 private network\n            - \"100.64.0.0/10\"   # Carrier-grade NAT (RFC6598)\n            - \"127.0.0.0/8\"     # Loopback addresses\n            - \"169.254.0.0/16\"  # Link-local addresses (RFC3927)\n            - \"172.16.0.0/12\"   # RFC1918 private network\n            - \"192.168.0.0/16\"  # RFC1918 private network\n            - \"::1/128\"         # IPv6 loopback address\n            - \"2002::/16\"       # 6to4 IPv6 addresses\n            - \"fc00::/7\"        # Unique local IPv6 unicast (RFC4193)\n            - \"fe80::/10\"       # IPv6 link-local addresses\n\n          # List of client IPs to trust (default: [])\n          clientTrustedIPs: *trustedIps\n"
  traefik-failover-dynamic.conf.tmpl:
    name: my-media-stack_traefik-failover-dynamic.conf.tmpl
    content: |
      # yaml-language-server: $$schema=https://www.schemastore.org/traefik-v3-file-provider.json
      http:
        routers:
      {{- range $$c := . }}
        {{- if eq (index $$c.Labels "traefik.enable") "true" }}
          {{- $$name := trimPrefix "/" $$c.Name }}
          {{ $$name }}-with-failover:
            service: {{ $$name }}-with-failover@file
            rule: Host(`{{ $$name }}.bolabaden.org`)
          {{ $$name }}-direct:
            service: {{ $$name }}-direct@file
            rule: Host(`{{ $$name }}.beatapostapita.bolabaden.org`)
        {{- end }}
      {{- end }}

        services:
      {{- range $$c := . }}
        {{- if eq (index $$c.Labels "traefik.enable") "true" }}
          {{- $$name := trimPrefix "/" $$c.Name }}
          {{- /* Get healthcheck path from container labels */}}
          {{- $$healthPath := "/" }}
          {{- if index $$c.Labels "kuma.healthcheck.path" }}
            {{- $$healthPath = index $$c.Labels "kuma.healthcheck.path" }}
          {{- end }}

          {{- /* Get healthcheck interval from container labels */}}
          {{- $$healthInterval := "5m" }}
          {{- if index $$c.Labels "kuma.healthcheck.interval" }}
            {{- $$healthInterval = index $$c.Labels "kuma.healthcheck.interval" }}
          {{- end }}

          {{- /* Get healthcheck timeout from container labels */}}
          {{- $$healthTimeout := "15s" }}
          {{- if index $$c.Labels "kuma.healthcheck.timeout" }}
            {{- $$healthTimeout = index $$c.Labels "kuma.healthcheck.timeout" }}
          {{- end }}

          {{ $$name }}-direct:
            loadBalancer:
              servers:
                - url: http://{{ $$name }}:{{ (index $$c.Addresses 0).Port }}

          {{ $$name }}-with-failover:
            loadBalancer:
              servers:
                - url: http://{{ $$name }}:{{ (index $$c.Addresses 0).Port }}
                - url: https://{{ $$name }}.micklethefickle.bolabaden.org
                - url: https://{{ $$name }}.beatapostapita.bolabaden.org
                - url: https://{{ $$name }}.vractormania.bolabaden.org
              #  - url: https://{{ $$name }}.arnialtrashlid.bolabaden.org
              #  - url: https://{{ $$name }}.wizard-pc.bolabaden.org
              #  - url: https://{{ $$name }}.wizard-pc-wsl.bolabaden.org
              #  - url: https://{{ $$name }}.wizard-laptop.bolabaden.org
              #  - url: https://{{ $$name }}.cloudserver1.bolabaden.org
              #  - url: https://{{ $$name }}.cloudserver2.bolabaden.org
              #  - url: https://{{ $$name }}.cloudserver3.bolabaden.org
              healthCheck:
                path: "{{ $$healthPath }}"
                interval: "{{ $$healthInterval }}"
                timeout: "{{ $$healthTimeout }}"
        {{- end }}
      {{- end }}
  victoriametrics-dashboard.json:
    name: my-media-stack_victoriametrics-dashboard.json
    content: |
      {
        "annotations": {
          "list": [
            {
              "builtIn": 1,
              "datasource": {
                "type": "datasource",
                "uid": "grafana"
              },
              "enable": true,
              "hide": true,
              "iconColor": "rgba(0, 211, 255, 1)",
              "name": "Annotations & Alerts",
              "target": {
                "limit": 100,
                "matchAny": false,
                "tags": [],
                "type": "dashboard"
              },
              "type": "dashboard"
            }
          ]
        },
        "description": "Overview for single-node VictoriaMetrics",
        "editable": true,
        "fiscalYearStartMonth": 0,
        "graphTooltip": 1,
        "id": null,
        "panels": [
          {
            "datasource": {
              "type": "prometheus",
              "uid": "victoriametrics_uid"
            },
            "description": "How many datapoints are in storage",
            "fieldConfig": {
              "defaults": {
                "color": {
                  "mode": "thresholds"
                },
                "mappings": [],
                "thresholds": {
                  "mode": "absolute",
                  "steps": [
                    {
                      "color": "green",
                      "value": null
                    }
                  ]
                },
                "unit": "short"
              },
              "overrides": []
            },
            "gridPos": {
              "h": 2,
              "w": 5,
              "x": 0,
              "y": 0
            },
            "id": 26,
            "options": {
              "colorMode": "value",
              "graphMode": "area",
              "justifyMode": "auto",
              "orientation": "horizontal",
              "reduceOptions": {
                "calcs": [
                  "lastNotNull"
                ],
                "fields": "",
                "values": false
              },
              "showPercentChange": false,
              "textMode": "auto",
              "wideLayout": true
            },
            "targets": [
              {
                "datasource": {
                  "type": "prometheus",
                  "uid": "victoriametrics_uid"
                },
                "editorMode": "code",
                "exemplar": false,
                "expr": "sum(vm_rows{job=~\"$$job\", instance=~\"$$instance\", type!~\"indexdb.*\"})",
                "format": "time_series",
                "instant": true,
                "interval": "",
                "intervalFactor": 1,
                "legendFormat": "",
                "refId": "A"
              }
            ],
            "title": "Total datapoints",
            "type": "stat"
          }
        ],
        "refresh": "",
        "schemaVersion": 40,
        "tags": [
          "victoriametrics"
        ],
        "templating": {
          "list": [
            {
              "current": {},
              "datasource": {
                "type": "prometheus",
                "uid": "victoriametrics_uid"
              },
              "definition": "label_values(vm_app_version{version=~\"victoria-metrics-.*\"}, job)",
              "includeAll": false,
              "name": "job",
              "options": [],
              "query": {
                "query": "label_values(vm_app_version{version=~\"victoria-metrics-.*\"}, job)",
                "refId": "VictoriaMetrics-job-Variable-Query"
              },
              "refresh": 1,
              "regex": "",
              "type": "query"
            },
            {
              "allValue": ".*",
              "current": {},
              "datasource": {
                "type": "prometheus",
                "uid": "victoriametrics_uid"
              },
              "definition": "label_values(vm_app_version{job=~\"$$job\"}, instance)",
              "includeAll": true,
              "multi": true,
              "name": "instance",
              "options": [],
              "query": {
                "query": "label_values(vm_app_version{job=~\"$$job\"}, instance)",
                "refId": "VictoriaMetrics-instance-Variable-Query"
              },
              "refresh": 1,
              "regex": "",
              "type": "query"
            }
          ]
        },
        "time": {
          "from": "now-3h",
          "to": "now"
        },
        "timezone": "",
        "title": "VictoriaMetrics",
        "uid": "victoriametrics_single",
        "version": 1
      }
  warp-monitor.sh:
    name: my-media-stack_warp-monitor.sh
    content: "#!/usr/bin/env bash\nset -euo pipefail\n\n# Configurable via env\nDOCKER_CMD=\"$${DOCKER_CMD:-docker -H $${DOCKER_HOST:-unix:///var/run/docker.sock}}\"\nCHECK_IMAGE=\"$${CHECK_IMAGE:-curlimages/curl}\"   # image that includes curl\nNETWORK=\"$${NETWORK:-warp-nat-net}\"\nSLEEP_INTERVAL=\"$${SLEEP_INTERVAL:-5}\"                  # seconds between checks\n\n# Healthcheck command to run inside the ephemeral container.\n# This mirrors your warp-healthcheck logic: exit 0 when WARP active, nonzero otherwise.\nHEALTHCHECK_INSIDE='sh -c \"if curl -s --max-time 4 https://cloudflare.com/cdn-cgi/trace | grep -qE \\\"^warp=on|warp=plus$$\\\"; then echo WARP_OK && exit 0; else echo WARP_NOT_OK && exit 1; fi\"'\n\necho \"warp-monitor: checking WARP via ephemeral container on network '$${NETWORK}'.\"\necho \"Using image: $${CHECK_IMAGE}\"\nprev_ok=1  # assume healthy initially so we don't run setup at startup\nfail_count=0\nRETRY_SETUP_AFTER=\"$${RETRY_SETUP_AFTER:-12}\"  # retry setup after N consecutive failures\n\nwhile true; do\n  echo \"[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] running health probe...\"\n  if $${DOCKER_CMD} run --rm --network \"$${NETWORK}\" --entrypoint sh \"$${CHECK_IMAGE}\" -c \"$${HEALTHCHECK_INSIDE}\"; then\n    # check succeeded\n    if [[ \"$${prev_ok}\" -eq 0 ]]; then\n      echo \"[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] health probe recovered -> marking healthy\"\n    fi\n    prev_ok=1\n    fail_count=0\n  else\n    echo \"[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] health probe failed (consecutive failures: $$((fail_count + 1)))\"\n    fail_count=$$((fail_count + 1))\n    \n    # Run setup on first failure OR after N consecutive failures\n    if [[ \"$${prev_ok}\" -eq 1 ]] || [[ \"$${fail_count}\" -ge \"$${RETRY_SETUP_AFTER}\" ]]; then\n    if [[ \"$${prev_ok}\" -eq 1 ]]; then\n      echo \"[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] detected healthy->unhealthy transition; running /usr/local/bin/warp-nat-setup.sh\"\n      else\n        echo \"[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] still unhealthy after $$fail_count failures; retrying /usr/local/bin/warp-nat-setup.sh\"\n        fail_count=0  # reset counter after retry\n      fi\n      # Run setup, but do not let its failure kill the monitor. Log failures.\n      if /usr/local/bin/warp-nat-setup.sh; then\n        echo \"[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] warp-nat-setup.sh completed\"\n      else\n        echo \"[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] warp-nat-setup.sh failed (exit nonzero).\"\n      fi\n      # mark as unhealthy until probe says otherwise\n      prev_ok=0\n      # Wait a little before probing again to avoid tight loops\n      sleep \"$${SLEEP_INTERVAL}\"\n      # continue to next iteration (which will probe again and wait for recovery)\n    else\n      echo \"[$$(date -u +'%Y-%m-%dT%H:%M:%SZ')] still unhealthy ($$fail_count failures); will retry setup after $$RETRY_SETUP_AFTER consecutive failures\"\n    fi\n  fi\n\n  sleep \"$${SLEEP_INTERVAL}\"\ndone\n"
  warp-nat-setup.sh:
    name: my-media-stack_warp-nat-setup.sh
    content: "#!/bin/bash\nset -xe\n\n# Defaults (configurable via env)\nDOCKER_HOST=\"$${DOCKER_HOST:-unix:///var/run/docker.sock}\"\nROUTER_CONTAINER_NAME=\"$${ROUTER_CONTAINER_NAME:-warp_router}\"\nDOCKER_NETWORK_NAME=\"$${DOCKER_NETWORK_NAME:-warp-nat-net}\"\nWARP_CONTAINER_NAME=\"$${WARP_CONTAINER_NAME:-warp-nat-gateway}\"\nHOST_VETH_IP=\"$${HOST_VETH_IP:-169.254.100.1}\"\nCONT_VETH_IP=\"$${CONT_VETH_IP:-169.254.100.2}\"\nROUTING_TABLE=\"$${ROUTING_TABLE:-warp-nat-routing}\"\nVETH_HOST=\"$${VETH_HOST:-veth-warp}\" \n\n# VETH_CONT is derived from VETH_HOST\nVETH_CONT=\"$${VETH_HOST#veth-}-nat-cont\"\nDOCKER=\"docker -H $$DOCKER_HOST\"\nDEFAULT_DOCKER_NETWORK_NAME=\"warp-nat-net\"\n\necho \"==========================================\"\necho \"Starting WARP NAT setup script\"\necho \"==========================================\"\n\n# ==========================================\n# PHASE 1: COMPLETE CLEANUP OF OLD STATE\n# ==========================================\necho \"\"\necho \"Phase 1: Cleaning up any existing configuration...\"\n\n# Remove old veth interfaces\nif ip link show \"$$VETH_HOST\" >/dev/null 2>&1; then\n    echo \"Removing old veth interface: $$VETH_HOST\"\n    ip link del \"$$VETH_HOST\" 2>/dev/null || true\nfi\n\n# Get the subnet to clean up rules (try to get from network if it exists)\nCLEANUP_SUBNET=\"$${WARP_NAT_NET_SUBNET:-10.0.2.0/24}\"\nif $$DOCKER network inspect \"$$DOCKER_NETWORK_NAME\" >/dev/null 2>&1; then\n    EXISTING_SUBNET=$$($$DOCKER network inspect -f '{{(index .IPAM.Config 0).Subnet}}' \"$$DOCKER_NETWORK_NAME\" 2>/dev/null | tr -d '[:space:]')\n    if [[ -n \"$$EXISTING_SUBNET\" ]]; then\n        CLEANUP_SUBNET=\"$$EXISTING_SUBNET\"\n    fi\nfi\n\n# Remove old routing rules for this subnet\necho \"Removing old routing rules for $$CLEANUP_SUBNET\"\nwhile ip rule del from \"$$CLEANUP_SUBNET\" table \"$$ROUTING_TABLE\" 2>/dev/null; do\n    echo \"  Removed routing rule\"\ndone\n\n# Remove old iptables failsafe rules\necho \"Removing old failsafe iptables rules\"\nwhile iptables -D FORWARD -s \"$$CLEANUP_SUBNET\" -j DROP -m comment --comment \"warp-nat-failsafe\" 2>/dev/null; do\n    echo \"  Removed failsafe rule\"\ndone\n\n# Remove old NAT rules on host\necho \"Removing old NAT rules on host\"\niptables -t nat -D POSTROUTING -s \"$$CLEANUP_SUBNET\" ! -d \"$$CLEANUP_SUBNET\" -j MASQUERADE 2>/dev/null || true\n\necho \"Phase 1 cleanup complete\"\n\n# Pick a free routing table id dynamically (start at 110)\npick_table_id() {\n    local id=110\n    while grep -q \"^$$id \" /etc/iproute2/rt_tables 2>/dev/null; do\n        id=$$((id+1))\n    done\n    echo $$id\n}\n\n# ==========================================\n# PHASE 2: SETUP ROUTING TABLE\n# ==========================================\necho \"\"\necho \"Phase 2: Setting up routing table...\"\n\n# Get existing routing table ID if name exists, else pick new and add\nif grep -q \" $$ROUTING_TABLE$$\" /etc/iproute2/rt_tables 2>/dev/null; then\n    ROUTING_TABLE_ID=$$(awk \"/ $$ROUTING_TABLE\\$$/ {print \\$$1}\" /etc/iproute2/rt_tables)\n    echo \"Routing table exists: $$ROUTING_TABLE (ID: $$ROUTING_TABLE_ID)\"\nelse\n    ROUTING_TABLE_ID=$$(pick_table_id)\n    echo \"$$ROUTING_TABLE_ID $$ROUTING_TABLE\" >> /etc/iproute2/rt_tables\n    echo \"Created routing table: $$ROUTING_TABLE (ID: $$ROUTING_TABLE_ID)\"\nfi\n\n# ==========================================\n# PHASE 3: ENSURE NETWORK EXISTS AND DISCOVER WARP CONTAINER\n# ==========================================\n# This phase discovers the actual network name using Docker Compose naming conventions.\n# Unlike the old approach that required the router container to exist and inspected its\n# connected networks, this method:\n# - Works independently of container state (more robust during setup/teardown)\n# - Understands Compose prefixes network names with stack name (e.g., \"stack_network\")\n# - Supports multiple WARP instances by using stack-specific naming\n# - Is deterministic and idempotent (doesn't depend on container connection state)\necho \"\"\necho \"Phase 3: Ensuring network exists and discovering WARP container...\"\n\n# Get stack name from compose labels to construct proper network name\n# Tries router container first, falls back to warp container, then empty string\nSTACK_NAME=\"$$(\n    $$DOCKER inspect -f '{{ index .Config.Labels \"com.docker.compose.project\" }}' \"$$ROUTER_CONTAINER_NAME\" 2>/dev/null \\\n    || $$DOCKER inspect -f '{{ index .Config.Labels \"com.docker.compose.project\" }}' \"$$WARP_CONTAINER_NAME\" 2>/dev/null \\\n    || echo \"\"\n)\"\n\n# NETWORK DISCOVERY (Non-Destructive Approach)\n# ================================================\n# OLD BEHAVIOR (removed): Would forcefully disconnect ALL containers, delete network, recreate it,\n#                         then reconnect containers - causing service disruption every time script ran.\n# NEW BEHAVIOR: Simply discovers and uses existing network without any modifications.\n# BENEFITS:\n#   - Zero downtime: containers stay connected, no service interruption\n#   - True idempotency: can run multiple times safely without state changes\n#   - Faster: no time wasted on disconnect/reconnect operations\n#   - Simpler: no complex gw_priority preservation logic needed\n#   - Multi-instance safe: different stacks can manage independent networks\n# NETWORK LIFECYCLE: Managed by warp-net-init service, not this routing script.\n\n# Check if network exists using Compose naming pattern (stack_network) first\n# This allows multiple stacks to have their own warp-nat-net without conflicts\nACTUAL_NETWORK_NAME=\"\"\nif [[ -n \"$$STACK_NAME\" ]]; then\n    if $$DOCKER network inspect \"$${STACK_NAME}_$$DOCKER_NETWORK_NAME\" >/dev/null 2>&1; then\n        ACTUAL_NETWORK_NAME=\"$${STACK_NAME}_$$DOCKER_NETWORK_NAME\"\n    fi\nfi\n\n# Fallback: try plain network name (for external networks or single-stack deployments)\nif [[ -z \"$$ACTUAL_NETWORK_NAME\" ]]; then\n    if $$DOCKER network inspect \"$$DOCKER_NETWORK_NAME\" >/dev/null 2>&1; then\n        ACTUAL_NETWORK_NAME=\"$$DOCKER_NETWORK_NAME\"\n    fi\nfi\n\n# Create network only if it doesn't exist (idempotent approach)\n# NOTE: This does NOT recreate/modify existing networks, avoiding disruption to connected containers.\n# Previous behavior: forcefully recreated network every time, disconnecting/reconnecting all containers.\n# New behavior: uses existing network as-is. To change network config, manually delete network first.\n# Benefits: No container disruption, no \"container not connected\" errors, faster, more deterministic.\nif [[ -z \"$$ACTUAL_NETWORK_NAME\" ]]; then\n    echo \"Network $$DOCKER_NETWORK_NAME does not exist, creating it...\"\n    BRIDGE_OPT_NAME=\"br_$$DOCKER_NETWORK_NAME\"\n    $$DOCKER network create \\\n        --driver=bridge \\\n        --attachable \\\n        -o com.docker.network.bridge.name=\"$$BRIDGE_OPT_NAME\" \\\n        -o com.docker.network.bridge.enable_ip_masquerade=false \\\n        --subnet=\"$${WARP_NAT_NET_SUBNET:-10.0.2.0/24}\" \\\n        --gateway=\"$${WARP_NAT_NET_GATEWAY:-10.0.2.1}\" \\\n        \"$$DOCKER_NETWORK_NAME\"\n    ACTUAL_NETWORK_NAME=\"$$DOCKER_NETWORK_NAME\"\n    echo \"Created network: $$ACTUAL_NETWORK_NAME\"\nelse\n    echo \"Found existing network: $$ACTUAL_NETWORK_NAME\"\nfi\n\necho \"Found network: $$ACTUAL_NETWORK_NAME\"\n\n# Dynamically get network subnet\nDOCKER_NET=\"$$($$DOCKER network inspect -f '{{(index .IPAM.Config 0).Subnet}}' \"$$ACTUAL_NETWORK_NAME\" 2>/dev/null | tr -d '[:space:]')\"\nif [[ -z \"$$DOCKER_NET\" ]]; then\n    echo \"Error: Could not determine subnet for network $$ACTUAL_NETWORK_NAME\"\n    exit 1\nfi\necho \"Network subnet: $$DOCKER_NET\"\n\n# Dynamically get the actual bridge name from the network\nBRIDGE_NAME=\"$$($$DOCKER network inspect -f '{{index .Options \"com.docker.network.bridge.name\"}}' \"$$ACTUAL_NETWORK_NAME\" 2>/dev/null)\"\nif [[ -z \"$$BRIDGE_NAME\" || \"$$BRIDGE_NAME\" == \"<no value>\" ]]; then\n    # Fallback: construct bridge name from network ID (Docker's default pattern)\n    NETWORK_ID=\"$$($$DOCKER network inspect -f '{{.Id}}' \"$$ACTUAL_NETWORK_NAME\" 2>/dev/null | cut -c1-12)\"\n    BRIDGE_NAME=\"br-$$NETWORK_ID\"\n    echo \"Bridge name not explicitly set, using Docker default: $$BRIDGE_NAME\"\nelse\n    echo \"Bridge device: $$BRIDGE_NAME\"\nfi\n\n# Verify the bridge exists\nif ! ip link show \"$$BRIDGE_NAME\" >/dev/null 2>&1; then\n    echo \"Error: Bridge $$BRIDGE_NAME does not exist\"\n    echo \"Available bridges:\"\n    ip link show type bridge\n    exit 1\nfi\n\n# Get WARP container PID\nwarp_pid=\"$$($$DOCKER inspect -f '{{.State.Pid}}' $$WARP_CONTAINER_NAME 2>/dev/null || echo \\\"\\\")\"\nif [[ -z \"$$warp_pid\" || \"$$warp_pid\" == \"0\" ]]; then\n    echo \"Error: $$WARP_CONTAINER_NAME container not found or not running\"\n    exit 1\nfi\n\nif [[ ! -e \"/proc/$$warp_pid/ns/net\" ]]; then\n    echo \"Error: $$WARP_CONTAINER_NAME container network namespace not ready\"\n    exit 1\nfi\n\necho \"Found WARP container PID: $$warp_pid\"\n\n# Clean orphan NAT rules inside warp container (for any stale subnets)\necho \"Cleaning orphan NAT rules inside WARP container...\"\nnsenter -t \"$$warp_pid\" -n iptables -t nat -S POSTROUTING 2>/dev/null | grep -- '-j MASQUERADE' | while read -r rule; do\n    s_net=$$(echo \"$$rule\" | sed -n 's/.*-s \\([^ ]*\\) -j MASQUERADE.*/\\1/p')\n    if [[ -z \"$$s_net\" ]]; then continue; fi\n    if [[ \"$$s_net\" == \"$$DOCKER_NET\" ]]; then continue; fi\n    echo \"  Removing orphan NAT rule inside warp: $$s_net\"\n        del_rule=$$(echo \"$$rule\" | sed 's/^-A/-D/')\n        nsenter -t \"$$warp_pid\" -n iptables -t nat $$del_rule 2>/dev/null || true\ndone\n\n# Set up cleanup function for error handling\ncleanup() {\n    echo \" Error occurred. Rolling back changes...\"\n\n    # Remove host veth if it was created\n    if ip link show \"$$VETH_HOST\" >/dev/null 2>&1; then\n        echo \"Removing veth interface: $$VETH_HOST\"\n        ip link del \"$$VETH_HOST\" 2>/dev/null || true\n    fi\n\n    # Remove ip rule if it was added\n    if ip rule show | grep -q \"from $$DOCKER_NET lookup $$ROUTING_TABLE\"; then\n        echo \"Removing routing rule: from $$DOCKER_NET lookup $$ROUTING_TABLE\"\n        ip rule del from \"$$DOCKER_NET\" table \"$$ROUTING_TABLE\" 2>/dev/null || true\n    fi\n\n    # Remove specific routes from routing table if they exist\n    if ip route show table \"$$ROUTING_TABLE\" | grep -q \"^default via $$CONT_VETH_IP dev $$VETH_HOST\"; then\n        echo \"Removing default route from $$ROUTING_TABLE\"\n        ip route del default via \"$$CONT_VETH_IP\" dev \"$$VETH_HOST\" table \"$$ROUTING_TABLE\" 2>/dev/null || true\n    fi\n    if ip route show table \"$$ROUTING_TABLE\" | grep -q \"^$$DOCKER_NET dev $$BRIDGE_NAME\"; then\n        echo \"Removing network route from $$ROUTING_TABLE\"\n        ip route del \"$$DOCKER_NET\" dev \"$$BRIDGE_NAME\" table \"$$ROUTING_TABLE\" 2>/dev/null || true\n    fi\n\n    # Remove NAT rule on host if it exists\n    if iptables -t nat -C POSTROUTING -s \"$$DOCKER_NET\" ! -d \"$$DOCKER_NET\" -j MASQUERADE 2>/dev/null; then\n        echo \"Removing NAT rule on host\"\n        iptables -t nat -D POSTROUTING -s \"$$DOCKER_NET\" ! -d \"$$DOCKER_NET\" -j MASQUERADE 2>/dev/null || true\n    fi\n\n    # Remove NAT rule inside warp container if it exists\n    if [[ -n \"$$warp_pid\" ]] && [[ -e \"/proc/$$warp_pid/ns/net\" ]]; then\n        if nsenter -t \"$$warp_pid\" -n iptables -t nat -C POSTROUTING -s \"$$DOCKER_NET\" -j MASQUERADE 2>/dev/null; then\n            echo \"Removing NAT rule inside WARP container\"\n            nsenter -t \"$$warp_pid\" -n iptables -t nat -D POSTROUTING -s \"$$DOCKER_NET\" -j MASQUERADE 2>/dev/null || true\n        fi\n    fi\n    \n    # CRITICAL: Ensure failsafe DROP rule is in place to prevent IP leaks\n    if ! iptables -C FORWARD -s \"$$DOCKER_NET\" -j DROP -m comment --comment \"warp-nat-failsafe\" 2>/dev/null; then\n        echo \"Re-enabling failsafe DROP rule to prevent IP leaks\"\n        iptables -I FORWARD -s \"$$DOCKER_NET\" -j DROP -m comment --comment \"warp-nat-failsafe\" 2>/dev/null || true\n    fi\n}\n\n# ==========================================\n# PHASE 4: CRITICAL SETUP WITH FAILSAFE\n# ==========================================\necho \"\"\necho \"Phase 4: Setting up VETH tunnel and routing...\"\n\n# Trap any error in the critical section\ntrap cleanup ERR\n\n# CRITICAL FAILSAFE: Block all traffic from warp-nat-net by default\n# This prevents IP leaks if setup fails. Rule will be removed at the end if setup succeeds.\necho \"Installing failsafe DROP rule to prevent IP leaks during setup\"\n# Add failsafe rule at the top of FORWARD chain (already cleaned in Phase 1)\niptables -I FORWARD -s \"$$DOCKER_NET\" -j DROP -m comment --comment \"warp-nat-failsafe\"\n\n# Create veth pair (host side: $$VETH_HOST, container side: $$VETH_CONT)\necho \"Creating veth pair: $$VETH_HOST <-> $$VETH_CONT\"\nip link add \"$$VETH_HOST\" type veth peer name \"$$VETH_CONT\"\n\n# Move container end into warp namespace\necho \"Moving $$VETH_CONT into WARP container namespace\"\nip link set \"$$VETH_CONT\" netns \"$$warp_pid\"\n\n# Configure host end of veth\necho \"Configuring host veth: $$VETH_HOST ($$HOST_VETH_IP/30)\"\nip addr add \"$$HOST_VETH_IP/30\" dev \"$$VETH_HOST\"\nip link set \"$$VETH_HOST\" up\n\n# Configure container end of veth\necho \"Configuring container veth: $$VETH_CONT ($$CONT_VETH_IP/30)\"\nnsenter -t \"$$warp_pid\" -n ip addr add \"$$CONT_VETH_IP/30\" dev \"$$VETH_CONT\"\nnsenter -t \"$$warp_pid\" -n ip link set \"$$VETH_CONT\" up\nnsenter -t \"$$warp_pid\" -n sysctl -w net.ipv4.ip_forward=1 >/dev/null\n#nsenter -t \"$$warp_pid\" -n sysctl -w net.ipv4.conf.all.rp_filter=2\n#nsenter -t \"$$warp_pid\" -n sysctl -w net.ipv4.conf.default.rp_filter=2\n\n# Setup NAT inside warp container\necho \"Setting up NAT inside WARP container for $$DOCKER_NET\"\nnsenter -t \"$$warp_pid\" -n iptables -t nat -C POSTROUTING -s \"$$DOCKER_NET\" -j MASQUERADE 2>/dev/null || \\\nnsenter -t \"$$warp_pid\" -n iptables -t nat -A POSTROUTING -s \"$$DOCKER_NET\" -j MASQUERADE\n\n# Setup routing rules\necho \"Adding routing rule: from $$DOCKER_NET lookup $$ROUTING_TABLE\"\nip rule add from \"$$DOCKER_NET\" table \"$$ROUTING_TABLE\"\n\n# Setup routing table entries (delete specific routes first if they exist)\necho \"Configuring routes in routing table $$ROUTING_TABLE\"\n\n# Remove existing network route if present\nif ip route show table \"$$ROUTING_TABLE\" | grep -q \"^$$DOCKER_NET dev $$BRIDGE_NAME\"; then\n    echo \"  Removing existing network route for $$DOCKER_NET\"\n    ip route del \"$$DOCKER_NET\" dev \"$$BRIDGE_NAME\" table \"$$ROUTING_TABLE\" 2>/dev/null || true\nfi\n\n# Remove existing default route if present\nif ip route show table \"$$ROUTING_TABLE\" | grep -q \"^default via $$CONT_VETH_IP dev $$VETH_HOST\"; then\n    echo \"  Removing existing default route\"\n    ip route del default via \"$$CONT_VETH_IP\" dev \"$$VETH_HOST\" table \"$$ROUTING_TABLE\" 2>/dev/null || true\nfi\n\n# Add the routes\necho \"  Adding network route: $$DOCKER_NET dev $$BRIDGE_NAME\"\nip route add \"$$DOCKER_NET\" dev \"$$BRIDGE_NAME\" table \"$$ROUTING_TABLE\"\necho \"  Adding default route: default via $$CONT_VETH_IP dev $$VETH_HOST\"\nip route add default via \"$$CONT_VETH_IP\" dev \"$$VETH_HOST\" table \"$$ROUTING_TABLE\"\n\n# Setup NAT on host\necho \"Setting up NAT on host for $$DOCKER_NET\"\niptables -t nat -C POSTROUTING -s \"$$DOCKER_NET\" ! -d \"$$DOCKER_NET\" -j MASQUERADE 2>/dev/null || \\\niptables -t nat -A POSTROUTING -s \"$$DOCKER_NET\" ! -d \"$$DOCKER_NET\" -j MASQUERADE\n\n# CRITICAL: Remove failsafe DROP rule now that routing is properly configured\necho \"Removing failsafe DROP rule - routing is now active\"\nwhile iptables -D FORWARD -s \"$$DOCKER_NET\" -j DROP -m comment --comment \"warp-nat-failsafe\" 2>/dev/null; do \n    echo \"  Removed failsafe rule\"\ndone\n\n# Disable error trap now that setup completed successfully\ntrap - ERR\n\n# ==========================================\n# SETUP COMPLETE\n# ==========================================\necho \"\"\necho \"==========================================\"\necho \" WARP NAT setup complete\"\necho \"==========================================\"\necho \"Network:        $$DOCKER_NETWORK_NAME\"\necho \"Subnet:         $$DOCKER_NET\"\necho \"Veth host:      $$VETH_HOST ($$HOST_VETH_IP)\"\necho \"Veth container: $$VETH_CONT ($$CONT_VETH_IP)\"\necho \"Routing table:  $$ROUTING_TABLE (ID: $$ROUTING_TABLE_ID)\"\necho \"Bridge:         $$BRIDGE_NAME\"\necho \"==========================================\"\n"
  watchtower-config.json:
    name: my-media-stack_watchtower-config.json
    file: /root/.docker/config.json
x-common-env:
  PGID: "999"
  PUID: "1001"
  TZ: America/Chicago
  UMASK: "002"
