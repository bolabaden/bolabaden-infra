# yaml-language-server: $schema=https://raw.githubusercontent.com/compose-spec/compose-spec/master/schema/compose-spec.json



networks:
  nginx_net:
    #external: true  # docker network create --driver=bridge --attachable nginx_net -o com.docker.network.bridge.name=br_nginx_traefik --subnet=${NGINX_TRAEFIK_SUBNET:-10.0.8.0/24} --gateway=${NGINX_TRAEFIK_GATEWAY:-10.0.8.1}
    attachable: true
    driver_opts:
      com.docker.network.bridge.name: br_nginx_net
    ipam:
      config:
        - subnet: ${NGINX_TRAEFIK_SUBNET:-10.0.8.0/24}
          gateway: ${NGINX_TRAEFIK_GATEWAY:-10.0.8.1}



secrets:
  cloudflare-api-token:
    file: ${SECRETS_PATH:?}/cf-api-token.txt
  cloudflare-api-key:
    file: ${SECRETS_PATH:?}/cf-api-key.txt
  nginx-auth-api-key:
    file: ${SECRETS_PATH:?}/nginx-auth-api-key.txt
  tinyauth-secret:
    file: ${SECRETS_PATH:?}/tinyauth-secret.txt
  tinyauth-google-client-secret:
    file: ${SECRETS_PATH:?}/tinyauth-google-client-secret.txt
  tinyauth-github-client-secret:
    file: ${SECRETS_PATH:?}/tinyauth-github-client-secret.txt
  crowdsec-lapi-key:
    file: ${SECRETS_PATH:?}/crowdsec-lapi-key.txt
  sudo-password:
    file: ${SECRETS_PATH:?}/sudo-password.txt



# reminder: everything in docker-compose.yml's `configs:` section must have dollar signs ($) escaped with another dollar sign ($$).
# Single dollar sign specifies that we're expecting docker to resolve the variable itself at deploy time (e.g. when `docker compose up` is ran).
# Double dollar sign specifies a literal dollar sign in the configuration content.
configs:
      # Log bind mounts into crowdsec
  crowdsec-auth.log:
    file: ${CONFIG_PATH:-./volumes}/traefik/crowdsec/var/log/auth.log
  crowdsec-syslog:
    file: ${CONFIG_PATH:-./volumes}/traefik/crowdsec/var/log/syslog
  crowdsec-victoriametrics.yaml:
    content: |
      type: http
      name: http_victoriametrics
      log_level: debug
      format: >
        {{- range $$Alert := . -}}
        {{- $$traefikRouters := GetMeta . "traefik_router_name" -}}
        {{- range .Decisions -}}
        {"metric":{"__name__":"cs_lapi_decision","instance":"my-instance","country":"{{$$Alert.Source.Cn}}","asname":"{{$$Alert.Source.AsName}}","asnumber":"{{$$Alert.Source.AsNumber}}","latitude":"{{$$Alert.Source.Latitude}}","longitude":"{{$$Alert.Source.Longitude}}","iprange":"{{$$Alert.Source.Range}}","scenario":"{{.Scenario}}","type":"{{.Type}}","duration":"{{.Duration}}","scope":"{{.Scope}}","ip":"{{.Value}}","traefik_routers":{{ printf "%q" ($$traefikRouters | uniq | join ",")}}},"values": [1],"timestamps":[{{now|unixEpoch}}000]}
        {{- end }}
        {{- end -}}
      url: http://victoriametrics:${VICTORIAMETRICS_PORT:-8428}/api/v1/import
      method: POST
      headers:
        Content-Type: application/json
        # if you use vmauth as proxy, please uncomment next line and add your token
        # If you would like to add authentication, please read about vmauth.
        # https://docs.victoriametrics.com/victoriametrics/vmauth/?ref=blog.lrvt.de#bearer-token-auth-proxy
        # It's basically another Docker container service, which acts as proxy in front of VictoriaMetrics and enforces Bearer HTTP Authentication.
        # Authorization: "${VICTORIAMETRICS_AUTH_TOKEN:-}"
  crowdsec-profiles.yaml:
    # If you are already using other custom notification channels, make sure to only add `http_victoriametrics` to the mix.
    # Your already existing notification channels should remain unchanged.
    content: |
      name: default_ip_remediation
      #debug: true
      filters:
      - Alert.Remediation == true && Alert.GetScope() == "Ip"
      decisions:
      - type: ban
        duration: 4h
      #duration_expr: Sprintf('%dh', (GetDecisionsCount(Alert.GetValue()) + 1) * 4)
      #notifications:
      #   - email_default         # Set the required email parameters in /etc/crowdsec/notifications/email.yaml before enabling this.
      #   - http_victoriametrics  # Set the required http parameters in /etc/crowdsec/notifications/http.yaml before enabling this.
      #   - slack_default         # Set the webhook in /etc/crowdsec/notifications/slack.yaml before enabling this.
      #   - splunk_default        # Set the splunk url and token in /etc/crowdsec/notifications/splunk.yaml before enabling this.
      #   - http_default          # Set the required http parameters in /etc/crowdsec/notifications/http.yaml before enabling this.
      on_success: break
      ---
      name: default_range_remediation
      #debug: true
      filters:
      - Alert.Remediation == true && Alert.GetScope() == "Range"
      decisions:
      - type: ban
        duration: 4h
      #duration_expr: Sprintf('%dh', (GetDecisionsCount(Alert.GetValue()) + 1) * 4)
      #notifications:
      #   - email_default         # Set the required email parameters in /etc/crowdsec/notifications/email.yaml before enabling this.
      #   - http_victoriametrics  # Set the required http parameters in /etc/crowdsec/notifications/http.yaml before enabling this.
      #   - slack_default         # Set the webhook in /etc/crowdsec/notifications/slack.yaml before enabling this.
      #   - splunk_default        # Set the splunk url and token in /etc/crowdsec/notifications/splunk.yaml before enabling this.
      #   - http_default          # Set the required http parameters in /etc/crowdsec/notifications/http.yaml before enabling this.
      on_success: break
  crowdsec-acquis.yaml:
    content: |
      filenames:
        - /var/log/auth.log
        - /var/log/syslog
      labels:
        type: syslog
      ---
      poll_without_inotify: false
      filenames:
        - ${TRAEFIK_INTERNAL_LOG_DIR:-/var/log/traefik}/*.log
      #  - ${TRAEFIK_INTERNAL_LOG_DIR:-/var/log/traefik}/access.log
      labels:
        type: traefik
  crowdsec-email.yaml:
    content: |
      type: email           # Don't change
      name: email_default   # Must match the registered plugin in the profile

      # One of "trace", "debug", "info", "warn", "error", "off"
      log_level: info

      # group_wait:         # Time to wait collecting alerts before relaying a message to this plugin, eg "30s"
      # group_threshold:    # Amount of alerts that triggers a message before <group_wait> has expired, eg "10"
      # max_retry:          # Number of attempts to relay messages to plugins in case of error
      timeout: 20s          # Time to wait for response from the plugin before considering the attempt a failure, eg "10s"

      #-------------------------
      # plugin-specific options

      # The following template receives a list of models.Alert objects
      # The output goes in the email message body
      format: |
        <html><body>
        {{range . -}}
          {{$$alert := . -}}
          {{range .Decisions -}}
            <p><a href="https://www.whois.com/whois/{{.Value}}">{{.Value}}</a> will get <b>{{.Type}}</b> for next <b>{{.Duration}}</b> for triggering <b>{{.Scenario}}</b> on machine <b>{{$$alert.MachineID}}</b>.</p> <p><a href="https://app.crowdsec.net/cti/{{.Value}}">CrowdSec CTI</a></p>
          {{end -}}
        {{end -}}
        </body></html>

      smtp_host: ${CROWDSEC_SMTP_HOST:-smtp.gmail.com}  # example: smtp.gmail.com
      smtp_username: ${CROWDSEC_SMTP_USERNAME:-$ACME_RESOLVER_EMAIL}
      smtp_password: ${CROWDSEC_SMTP_PASSWORD:-$GMAIL_APP_PASSWORD}
      smtp_port: ${CROWDSEC_SMTP_PORT:-587}   # Common values are any of [25, 465, 587, 2525]
      auth_type: ${CROWDSEC_SMTP_AUTH_TYPE:-login}   # Valid choices are "none", "crammd5", "login", "plain"
      sender_name: "CrowdSec"
      sender_email: ${CROWDSEC_SENDER_EMAIL:-${CROWDSEC_SMTP_USERNAME}}
      email_subject: "CrowdSec Security Alert"
      receiver_emails:
        - ${CROWDSEC_RECEIVER_EMAIL:-admin@localhost}
        - ${ACME_RESOLVER_EMAIL:?}
      # - email1@gmail.com
      # - email2@gmail.com

      # One of "ssltls", "starttls", "none"
      encryption_type: "ssltls"

      # If you need to set the HELO hostname:
      # helo_host: "localhost"

      # If the email server is hitting the default timeouts (10 seconds), you can increase them here
      #
      # connect_timeout: 10s
      # send_timeout: 10s

      ---

      # type: email
      # name: email_second_notification
      # ...
  crowdsec-http.yaml:
    content: |
      type: http          # Don't change
      name: http_default  # Must match the registered plugin in the profile

      # One of "trace", "debug", "info", "warn", "error", "off"
      log_level: ${CROWDSEC_HTTP_LOG_LEVEL:-info}

      # group_wait:         # Time to wait collecting alerts before relaying a message to this plugin, eg "30s"
      # group_threshold:    # Amount of alerts that triggers a message before <group_wait> has expired, eg "10"
      # max_retry:          # Number of attempts to relay messages to plugins in case of error
      # timeout:            # Time to wait for response from the plugin before considering the attempt a failure, eg "10s"

      #-------------------------
      # plugin-specific options

      # The following template receives a list of models.Alert objects
      # The output goes in the http request body
      format: |
        {{.|toJson}}

      # The plugin will make requests to this url, eg:  https://www.cloudflare.com/
      url: ${CROWDSEC_HTTP_URL:-https://grafana.$DOMAIN/api/annotations}

      # Any of the http verbs: "POST", "GET", "PUT"...
      method: ${CROWDSEC_HTTP_METHOD:-POST}

      # headers:
      #   Authorization: token 0x64312313
      #   Content-Type: application/json

      skip_tls_verification: ${CROWDSEC_HTTP_SKIP_TLS_VERIFICATION:-false}  # true or false. Default is false

      ---

      # type: http
      # name: http_second_notification
      # ...
  crowdsec-slack.yaml:
    content: |
      type: slack           # Don't change
      name: slack_default   # Must match the registered plugin in the profile

      # One of "trace", "debug", "info", "warn", "error", "off"
      log_level: info

      # group_wait:         # Time to wait collecting alerts before relaying a message to this plugin, eg "30s"
      # group_threshold:    # Amount of alerts that triggers a message before <group_wait> has expired, eg "10"
      # max_retry:          # Number of attempts to relay messages to plugins in case of error
      # timeout:            # Time to wait for response from the plugin before considering the attempt a failure, eg "10s"

      #-------------------------
      # plugin-specific options

      # The following template receives a list of models.Alert objects
      # The output goes in the slack message
      format: |
        {{range . -}}
        {{$$alert := . -}}
        {{range .Decisions -}}
        {{if $$alert.Source.Cn -}}
        :flag-{{$$alert.Source.Cn}}: <https://www.whois.com/whois/{{.Value}}|{{.Value}}> will get {{.Type}} for next {{.Duration}} for triggering {{.Scenario}} on machine '{{$$alert.MachineID}}'. <https://app.crowdsec.net/cti/{{.Value}}|CrowdSec CTI>{{end}}
        {{if not $$alert.Source.Cn -}}
        :pirate_flag: <https://www.whois.com/whois/{{.Value}}|{{.Value}}> will get {{.Type}} for next {{.Duration}} for triggering {{.Scenario}} on machine '{{$$alert.MachineID}}'.  <https://app.crowdsec.net/cti/{{.Value}}|CrowdSec CTI>{{end}}
        {{end -}}
        {{end -}}


      webhook: ${CROWDSEC_SLACK_WEBHOOK_URL:-<SLACK_WEBHOOK_URL>}

      # API request data as defined by the Slack webhook API.
      #channel: <CHANNEL_NAME>
      #username: <USERNAME>
      #icon_emoji: <ICON_EMOJI>
      #icon_url: <ICON_URL>

      ---

      # type: slack
      # name: slack_second_notification
      # ...
  crowdsec-splunk.yaml:
    content: |
      type: splunk          # Don't change
      name: ${CROWDSEC_SPLUNK_FIRST_NOTIFICATION_NAME:-splunk_default}  # Must match the registered plugin in the profile

      # One of "trace", "debug", "info", "warn", "error", "off"
      log_level: ${CROWDSEC_SPLUNK_FIRST_NOTIFICATION_LOG_LEVEL:-info}

      # group_wait:         # Time to wait collecting alerts before relaying a message to this plugin, eg "30s"
      # group_threshold:    # Amount of alerts that triggers a message before <group_wait> has expired, eg "10"
      # max_retry:          # Number of attempts to relay messages to plugins in case of error
      # timeout:            # Time to wait for response from the plugin before considering the attempt a failure, eg "10s"

      #-------------------------
      # plugin-specific options

      # The following template receives a list of models.Alert objects
      # The output goes in the splunk notification
      format: |
        {{.|toJson}}

      url: ${CROWDSEC_SPLUNK_FIRST_NOTIFICATION_HTTP_URL:-<SPLUNK_HTTP_URL>}
      token: ${CROWDSEC_SPLUNK_FIRST_NOTIFICATION_TOKEN:-<SPLUNK_TOKEN>}

      ---

      # type: splunk
      # name: splunk_second_notification
      # ...
  crowdsec-sentinel.yaml:
    content: |
      type: sentinel          # Don't change
      name: ${CROWDSEC_SENTINEL_FIRST_NOTIFICATION_NAME:-sentinel_default}  # Must match the registered plugin in the profile

      # One of "trace", "debug", "info", "warn", "error", "off"
      log_level: ${CROWDSEC_SENTINEL_FIRST_NOTIFICATION_LOG_LEVEL:-info}
      # group_wait:         # Time to wait collecting alerts before relaying a message to this plugin, eg "30s"
      # group_threshold:    # Amount of alerts that triggers a message before <group_wait> has expired, eg "10"
      # max_retry:          # Number of attempts to relay messages to plugins in case of error
      # timeout:            # Time to wait for response from the plugin before considering the attempt a failure, eg "10s"

      #-------------------------
      # plugin-specific options

      # The following template receives a list of models.Alert objects
      # The output goes in the http request body
      format: |
        {{.|toJson}}

      customer_id: ${CROWDSEC_SENTINEL_FIRST_NOTIFICATION_CUSTOMER_ID:-XXX-XXX}
      shared_key: ${CROWDSEC_SENTINEL_FIRST_NOTIFICATION_SHARED_KEY:-XXXXXXX}
  crowdsec-file.yaml:
    content: |
      # Don't change this
      type: file

      name: ${CROWDSEC_FILE_FIRST_NOTIFICATION_NAME:-file_default}  # this must match with the registered plugin in the profile
      log_level: ${CROWDSEC_FILE_FIRST_NOTIFICATION_LOG_LEVEL:-info}  # Options include: trace, debug, info, warn, error, off

      # This template render all events as ndjson
      format: |
        {{range . -}}
        { "time": "{{.StopAt}}", "program": "crowdsec", "alert": {{. | toJson }} }
        {{ end -}}

      group_wait: ${CROWDSEC_FILE_FIRST_NOTIFICATION_GROUP_WAIT:-30s}  # duration to wait collecting alerts before sending to this plugin
      group_threshold: ${CROWDSEC_FILE_FIRST_NOTIFICATION_GROUP_THRESHOLD:-10}  # if alerts exceed this, then the plugin will be sent the message

      # Use full path EG /tmp/crowdsec_alerts.json or %TEMP%\crowdsec_alerts.json
      log_path: "${CROWDSEC_FILE_FIRST_NOTIFICATION_LOG_PATH:-/tmp/crowdsec_alerts.json}"
      rotate:
        enabled: ${CROWDSEC_FILE_FIRST_NOTIFICATION_ROTATE_ENABLED:-true}  # Change to false if you want to handle log rotate on system basis
        max_size: ${CROWDSEC_FILE_FIRST_NOTIFICATION_MAX_SIZE:-500}  # in MB
        max_files: ${CROWDSEC_FILE_FIRST_NOTIFICATION_MAX_FILES:-5}
        max_age: ${CROWDSEC_FILE_FIRST_NOTIFICATION_MAX_AGE:-5}
        compress: ${CROWDSEC_FILE_FIRST_NOTIFICATION_COMPRESS:-true}
  crowdsec-config.yaml:
    content: |
      common:
        log_media: stdout
        log_level: info
        log_dir: /var/log/
      config_paths:
        config_dir: /etc/crowdsec/
        data_dir: /var/lib/crowdsec/data/
        simulation_path: /etc/crowdsec/simulation.yaml
        hub_dir: /etc/crowdsec/hub/
        index_path: /etc/crowdsec/hub/.index.json
        notification_dir: /etc/crowdsec/notifications/
        plugin_dir: /usr/local/lib/crowdsec/plugins/
      crowdsec_service:
        acquisition_path: /etc/crowdsec/acquis.yaml
        acquisition_dir: /etc/crowdsec/acquis.d
        parser_routines: 1
      plugin_config:
        user: nobody
        group: nobody
      cscli:
        output: human
      db_config:
        log_level: info
        type: sqlite
        db_path: /var/lib/crowdsec/data/crowdsec.db
        flush:
          max_items: 5000
          max_age: 7d
        use_wal: false
      api:
        client:
          insecure_skip_verify: false
          credentials_path: /etc/crowdsec/local_api_credentials.yaml
        server:
          log_level: info
          listen_uri: 0.0.0.0:${CROWDSEC_LAPI_PORT:-8080}
          profiles_path: /etc/crowdsec/profiles.yaml
          trusted_ips: # IP ranges, or IPs which can have admin API access
            - 127.0.0.1
            - ::1
          online_client: # Central API credentials (to push signals and receive bad IPs)
            credentials_path: /etc/crowdsec//online_api_credentials.yaml
          enable: true
      prometheus:
        enabled: true
        level: full
        listen_addr: 0.0.0.0
        listen_port: ${PROMETHEUS_PORT:-6060}
  traefik-failover-dynamic.conf.tmpl:
    content: |
      # yaml-language-server: $$schema=https://www.schemastore.org/traefik-v3-file-provider.json
      http:
        routers:
      {{- range $$c := . }}
        {{- if eq (index $$c.Labels "traefik.enable") "true" }}
          {{- $$name := trimPrefix "/" $$c.Name }}
          {{ $$name }}-with-failover:
            service: {{ $$name }}-with-failover@file
            rule: Host(`{{ $$name }}.$DOMAIN`)
          {{ $$name }}-direct:
            service: {{ $$name }}-direct@file
            rule: Host(`{{ $$name }}.$TS_HOSTNAME.$DOMAIN`)
        {{- end }}
      {{- end }}

        services:
      {{- range $$c := . }}
        {{- if eq (index $$c.Labels "traefik.enable") "true" }}
          {{- $$name := trimPrefix "/" $$c.Name }}
          {{- /* Get healthcheck path from container labels */}}
          {{- $$healthPath := "/" }}
          {{- if index $$c.Labels "kuma.healthcheck.path" }}
            {{- $$healthPath = index $$c.Labels "kuma.healthcheck.path" }}
          {{- end }}

          {{- /* Get healthcheck interval from container labels */}}
          {{- $$healthInterval := "5m" }}
          {{- if index $$c.Labels "kuma.healthcheck.interval" }}
            {{- $$healthInterval = index $$c.Labels "kuma.healthcheck.interval" }}
          {{- end }}

          {{- /* Get healthcheck timeout from container labels */}}
          {{- $$healthTimeout := "15s" }}
          {{- if index $$c.Labels "kuma.healthcheck.timeout" }}
            {{- $$healthTimeout = index $$c.Labels "kuma.healthcheck.timeout" }}
          {{- end }}

          {{ $$name }}-direct:
            loadBalancer:
              servers:
                - url: http://{{ $$name }}:{{ (index $$c.Addresses 0).Port }}

          {{ $$name }}-with-failover:
            loadBalancer:
              servers:
                - url: http://{{ $$name }}:{{ (index $$c.Addresses 0).Port }}
                - url: https://{{ $$name }}.micklethefickle.$DOMAIN
                - url: https://{{ $$name }}.beatapostapita.$DOMAIN
                - url: https://{{ $$name }}.vractormania.$DOMAIN
              #  - url: https://{{ $$name }}.arnialtrashlid.$DOMAIN
              #  - url: https://{{ $$name }}.wizard-pc.$DOMAIN
              #  - url: https://{{ $$name }}.wizard-pc-wsl.$DOMAIN
              #  - url: https://{{ $$name }}.wizard-laptop.$DOMAIN
              #  - url: https://{{ $$name }}.cloudserver1.$DOMAIN
              #  - url: https://{{ $$name }}.cloudserver2.$DOMAIN
              #  - url: https://{{ $$name }}.cloudserver3.$DOMAIN
              healthCheck:
                path: "{{ $$healthPath }}"
                interval: "{{ $$healthInterval }}"
                timeout: "{{ $$healthTimeout }}"
        {{- end }}
      {{- end }}

  traefik-dynamic.yaml:
    content: |
      # yaml-language-server: $$schema=https://www.schemastore.org/traefik-v3-file-provider.json
      http:
        routers:
          catchall:
            entryPoints:
              - web
              - websecure
            service: noop@internal
            rule: Host(`$DOMAIN`) || Host(`$TS_HOSTNAME.$DOMAIN`) || HostRegexp(`^(.+)$$`)
            priority: 1
            middlewares:
              - traefikerrorreplace@file
        services:
          nginx-traefik-extensions:
            loadBalancer:
              servers:
                - url: http://nginx-traefik-extensions:80
          bolabaden-nextjs:
            loadBalancer:
              servers:
                - url: http://bolabaden-nextjs:3000
        middlewares:
          traefikerrorreplace:
            plugin:
              traefikerrorreplace:
                matchStatus:
                  - 418
                replaceStatus: 404
          bolabaden-error-pages:
            errors:
              status:
                - 400-599
              service: bolabaden-nextjs@file
              query: /api/error/{status}
          nginx-auth:
            forwardAuth:
              address: http://nginx-traefik-extensions:80/auth
              trustForwardHeader: true
              authResponseHeaders: ["X-Auth-Method", "X-Auth-Passed", "X-Middleware-Name"]
          strip-www:
            redirectRegex:
              regex: '^(http|https)?://www\.(.+)$$'
              replacement: '$$1://$$2'
              permanent: false
          crowdsec:
            plugin:
              bouncer:
                # Enable the plugin (default: false)
                enabled: ${CROWDSEC_BOUNCER_ENABLED:-false}

                # Log level (default: INFO, expected: INFO, DEBUG, ERROR)
                logLevel: ${CROWDSEC_BOUNCER_LOG_LEVEL:-INFO}

                # File path to write logs (default: "")
                logFilePath: "${CROWDSEC_BOUNCER_LOG_FILE_PATH:-}"

                # Interval in seconds between metrics updates to Crowdsec (default: 600, <=0 disables metrics)
                metricsUpdateIntervalSeconds: ${CROWDSEC_BOUNCER_METRICS_UPDATE_INTERVAL_SECONDS:-600}

                # Mode for Crowdsec integration (default: live, expected: none, live, stream, alone, appsec)
                crowdsecMode: ${CROWDSEC_BOUNCER_MODE:-live}

                # Enable Crowdsec Appsec Server (WAF) (default: false)
                crowdsecAppsecEnabled: ${CROWDSEC_APPSEC_ENABLED:-false}

                # Crowdsec Appsec Server host and port (default: "crowdsec:7422")
                crowdsecAppsecHost: ${CROWDSEC_APPSEC_HOST:-crowdsec:7422}

                # Crowdsec Appsec Server path (default: "/")
                crowdsecAppsecPath: ${CROWDSEC_APPSEC_PATH:-/}

                # Block request when Crowdsec Appsec Server returns 500 (default: true)
                crowdsecAppsecFailureBlock: ${CROWDSEC_APPSEC_FAILURE_BLOCK:-true}

                # Block request when Crowdsec Appsec Server is unreachable (default: true)
                crowdsecAppsecUnreachableBlock: ${CROWDSEC_APPSEC_UNREACHABLE_BLOCK:-true}

                # Transmit only the first number of bytes to Crowdsec Appsec Server (default: 10485760 = 10MB)
                crowdsecAppsecBodyLimit: ${CROWDSEC_APPSEC_BODY_LIMIT:-10485760}

                # Scheme for Crowdsec LAPI (default: http, expected: http, https)
                crowdsecLapiScheme: ${CROWDSEC_LAPI_SCHEME:-http}

                # Crowdsec LAPI host and port (default: "crowdsec:8080")
                crowdsecLapiHost: ${CROWDSEC_LAPI_HOST:-crowdsec:8080}

                # Crowdsec LAPI path (default: "/")
                crowdsecLapiPath: ${CROWDSEC_LAPI_PATH:-/}

                # Crowdsec LAPI key for the bouncer (default: "")
                crowdsecLapiKey: ${CROWDSEC_LAPI_KEY}

                # Disable TLS verification for Crowdsec LAPI (default: false)
                crowdsecLapiTlsInsecureVerify: ${CROWDSEC_LAPI_TLS_INSECURE_VERIFY:-false}

                # PEM-encoded CA for Crowdsec LAPI (default: "")
                crowdsecLapiTlsCertificateAuthority: "${CROWDSEC_LAPI_TLS_CA:-}"

                # PEM-encoded client certificate for the Bouncer (default: "")
                crowdsecLapiTlsCertificateBouncer: "${CROWDSEC_LAPI_TLS_CERT:-}"

                # PEM-encoded client key for the Bouncer (default: "")
                crowdsecLapiTlsCertificateBouncerKey: "${CROWDSEC_LAPI_TLS_KEY:-}"

                # Name of the header in response when requests are cancelled (default: "")
                remediationHeadersCustomName: "${CROWDSEC_BOUNCER_REMEDIATION_HEADER_NAME:-}"

                # Name of the header where real client IP is retrieved (default: "X-Forwarded-For")
                forwardedHeadersCustomName: "${CROWDSEC_BOUNCER_FORWARDED_HEADER_NAME:-X-Forwarded-For}"      

                # Enable Redis cache (default: false)
                redisCacheEnabled: ${CROWDSEC_BOUNCER_REDIS_ENABLED:-false}

                # Redis hostname and port (default: "redis:6379")
                redisCacheHost: ${CROWDSEC_BOUNCER_REDIS_HOST:-redis:6379}

                # Redis password (default: "")
                redisCachePassword: "${CROWDSEC_BOUNCER_REDIS_PASSWORD:-}"

                # Redis database selection (default: "")
                redisCacheDatabase: "${CROWDSEC_BOUNCER_REDIS_DB:-}"

                # Block request when Redis is unreachable (default: true, adds 1s delay)
                redisCacheUnreachableBlock: ${CROWDSEC_BOUNCER_REDIS_UNREACHABLE_BLOCK:-true}

                # Default timeout in seconds for contacting Crowdsec LAPI (default: 10)
                httpTimeoutSeconds: ${CROWDSEC_BOUNCER_HTTP_TIMEOUT_SECONDS:-10}

                # Interval between LAPI fetches in stream mode (default: 60)
                updateIntervalSeconds: ${CROWDSEC_BOUNCER_UPDATE_INTERVAL_SECONDS:-60}

                # Max failures before blocking traffic in stream/alone mode (default: 0, -1 = never block)
                updateMaxFailure: ${CROWDSEC_BOUNCER_UPDATE_MAX_FAILURE:-0}

                # Maximum decision duration in live mode (default: 60)
                defaultDecisionSeconds: ${CROWDSEC_BOUNCER_DEFAULT_DECISION_SECONDS:-60}

                # HTTP status code for banned user (default: 403)
                remediationStatusCode: ${CROWDSEC_BOUNCER_REMEDIATION_STATUS_CODE:-403}

                # CAPI Machine ID (used only in alone mode)
                crowdsecCapiMachineId: ${CROWDSEC_CAPI_MACHINE_ID:-""}

                # CAPI Password (used only in alone mode)
                crowdsecCapiPassword: "${CROWDSEC_CAPI_PASSWORD:-}"

                # CAPI Scenarios (used only in alone mode)
                crowdsecCapiScenarios: ${CROWDSEC_CAPI_SCENARIOS:-[]}

                # Captcha provider (expected: hcaptcha, recaptcha, turnstile)
                captchaProvider: "${CROWDSEC_BOUNCER_CAPTCHA_PROVIDER:-}"

                # Captcha site key
                captchaSiteKey: "${CROWDSEC_BOUNCER_CAPTCHA_SITE_KEY:-}"

                # Captcha secret key
                captchaSecretKey: "${CROWDSEC_BOUNCER_CAPTCHA_SECRET_KEY:-}"

                # Grace period after captcha validation before revalidation required (default: 1800s = 30m)
                captchaGracePeriodSeconds: ${CROWDSEC_BOUNCER_CAPTCHA_GRACE_PERIOD_SECONDS:-1800}

                # Path to captcha template (default: /captcha.html)
                captchaHTMLFilePath: ${CROWDSEC_BOUNCER_CAPTCHA_HTML_FILE_PATH:-/captcha.html}

                # Path to ban HTML file (default: "", disabled if empty)
                banHTMLFilePath: "${CROWDSEC_BOUNCER_BAN_HTML_FILE_PATH:-}"

                # List of trusted proxies in front of Traefik (default: [])
                # As can be seen in the middleware declaration, we are actively defining all private class IPv4/IPv6 subnets as trusted IPs.
                # This is necessary, as we want to trust our Traefik reverse proxy's HTTP headers like X-Forwarded-For and X-Real-IP.
                # Those headers typically define the real IP address of our website visitors and threat actors, used by CrowdSec for decision making and banning.
                forwardedHeadersTrustedIPs: &trustedIps
                  - "127.0.0.1/32"    # Loopback addresses
                  - "10.0.0.0/8"      # RFC1918 private network
                  - "100.64.0.0/10"   # Carrier-grade NAT (RFC6598)
                  - "127.0.0.0/8"     # Loopback addresses
                  - "169.254.0.0/16"  # Link-local addresses (RFC3927)
                  - "172.16.0.0/12"   # RFC1918 private network
                  - "192.168.0.0/16"  # RFC1918 private network
                  - "::1/128"         # IPv6 loopback address
                  - "2002::/16"       # 6to4 IPv6 addresses
                  - "fc00::/7"        # Unique local IPv6 unicast (RFC4193)
                  - "fe80::/10"       # IPv6 link-local addresses

                # List of client IPs to trust (default: [])
                clientTrustedIPs: *trustedIps
  nginx-traefik-extensions.conf:
    content: |
      user nginx;
      worker_processes auto;

      error_log /dev/stderr warn;
      pid /var/run/nginx.pid;

      events {
          worker_connections 1024;
          use epoll;
          multi_accept on;
      }

      http {
          include /etc/nginx/mime.types;
          default_type application/octet-stream;
          log_format main '\n\r$$time_iso8601 | $$status | $$remote_addr | $$http_host | $$request | $${request_time}ms | '
                          'auth_method="$$auth_method" | $$http_user_agent | '
                          'request_method=$$request_method | '
                          'request_uri=$$request_uri | '
                          'query_string=$$query_string | '
                          'content_type=$$content_type | '
                          'server_protocol=$$server_protocol | '
                          'request_scheme=$$scheme | '
                          '\n\rheaders: {'
                            '"accept":"$$http_accept",'
                            '"accept_encoding":"$$http_accept_encoding",'
                            '"cookie":"$$http_cookie",'
                            '"x_forwarded_for":"$$http_x_forwarded_for",'
                            '"x_forwarded_port":"$$http_x_forwarded_port",'
                            '"x_forwarded_proto":"$$http_x_forwarded_proto",'
                            '"x_forwarded_host":"$$http_x_forwarded_host",'
                            '"x_real_ip":"$$http_x_real_ip",'
                            '"x_api_key":"$$http_x_api_key",'
                          '}';

          # Output all access logs to stdout for Docker console visibility
          access_log /dev/stdout main;
          error_log /dev/stderr warn;

          # Basic settings
          sendfile on;
          tcp_nopush on;
          tcp_nodelay on;
          keepalive_timeout 65;
          types_hash_max_size 2048;
          server_tokens off;
          
          # Fix for long API keys in map module
          map_hash_bucket_size 128;

          # Rate limiting zones
          limit_req_zone $$binary_remote_addr zone=auth:10m rate=10r/s;

          set_real_ip_from ${CROWDSEC_GF_SUBNET:-10.0.6.0/24};
          set_real_ip_from ${BACKEND_SUBNET:-10.0.7.0/24};
          real_ip_header X-Forwarded-For;
          real_ip_recursive on;

          geo $$ip_whitelisted {
              default 0;
              ${CROWDSEC_GF_SUBNET:-10.0.6.0/24} 1;
              ${BACKEND_SUBNET:-10.0.7.0/24}     1;
          }

          map $$http_x_api_key $$api_key_valid {
              default 0;
              "${NGINX_AUTH_API_KEY:?}" 1;
              # Add more API keys here as needed
          }

          upstream tinyauth {
              server auth:3000;
          }

          server {
              listen 80 default_server;
              server_name _;

              set $$auth_passed 0;
              set $$auth_method "none";

              if ($$api_key_valid = 1) {
                  set $$auth_passed 1;
                  set $$auth_method "api_key";
              }

              if ($$ip_whitelisted = 1) {
                  set $$auth_passed 1;
                  set $$auth_method "ip_whitelist";
              }

              location /auth {
                  limit_req zone=auth burst=20 nodelay;
                  if ($$auth_passed = 1) {
                      add_header X-Auth-Method "$$auth_method" always;
                      add_header X-Auth-Passed "true" always;
                      return 200 "OK";
                  }

                  proxy_pass http://tinyauth/api/auth/traefik;
                  proxy_pass_request_body off;
                  proxy_set_header Content-Length "";
                  proxy_set_header X-Original-URI $$http_x_original_uri;
                  proxy_set_header X-Original-Method $$http_x_original_method;
                  proxy_set_header X-Real-IP $$remote_addr;
                  proxy_set_header X-Forwarded-For $$proxy_add_x_forwarded_for;
                  proxy_set_header X-Forwarded-Proto $$scheme;
                  proxy_set_header X-Forwarded-Host $$http_x_forwarded_host;
                  add_header X-Auth-Method "tinyauth" always;
                  access_log /dev/stdout main;
              }

              location /health {
                  access_log /dev/stdout main;
                  return 200 "nginx service healthy\n";
                  add_header Content-Type text/plain;
              }

              location / {
                  access_log /dev/stdout main;
                  return 200 "nginx service healthy\n";
                  add_header Content-Type text/plain;
              }
          }
      }



services:
  cloudflare-ddns:
    image: docker.io/favonia/cloudflare-ddns:1  # - "1" for the latest stable version whose major version is 1
    #image: docker.io/favonia/cloudflare-ddns  # - "latest" for the latest stable version (which could become 2.x.y in the future and break things)
    container_name: cloudflare-ddns
    network_mode: host  # This bypasses network isolation and makes IPv6 easier (optional; see below)
    read_only: true  # Make the container filesystem read-only (optional but recommended)
    cap_drop: [all]  # Drop all Linux capabilities (optional but recommended)
    security_opt: [no-new-privileges:true]  # Another protection to restrict superuser privileges (optional but recommended)
    secrets:
      - cloudflare-api-token
    environment:
      TZ: ${TZ:-America/Chicago}
      # The value of CLOUDFLARE_API_TOKEN should be an API token (not an API key), which can be obtained from the API Tokens page.
      # Use the Edit zone DNS template to create a token. The less secure API key authentication is deliberately not supported.
      CLOUDFLARE_API_TOKEN_FILE: /run/secrets/cloudflare-api-token

      # There is an optional feature (available since version 1.14.0) that lets you maintain a WAF list of detected IP addresses.
      # To use this feature, edit the token and grant it the Account - Account Filter Lists - Edit permission.
      # If you only need to update WAF lists, not DNS records, you can remove the Zone - DNS - Edit permission.
      # Refer to the detailed documentation below for information on updating WAF lists.
      #CLOUDFLARE_WAF_LIST_TOKEN: $CLOUDFLARE_WAF_LIST_TOKEN  # Your Cloudflare WAF list token

      # The value of DOMAINS should be a list of fully qualified domain names (FQDNs) separated by commas.
      # For example, DOMAINS=example.org,www.example.org,example.io instructs the updater to manage the domains example.org, www.example.org, and example.io.
      # These domains do not have to share the same DNS zone---the updater will take care of the DNS zones behind the scene.
      DOMAINS: $TS_HOSTNAME.$DOMAIN,*.$TS_HOSTNAME.$DOMAIN  # Your domains (separated by commas)

      # PROXIED=true enables Cloudflare proxying (caches webpages, hides your IP).
      # Remove PROXIED=true to expose your real IP or if your traffic isn't HTTP(S).
      # Default: false. Accepts any boolean (true, false, 0, 1).
      # Advanced: PROXIED can be a domain-based boolean expression, e.g.:
      #   PROXIED=is(example.org)           # only example.org proxied
      #   PROXIED=sub(example.org)          # only subdomains of example.org proxied
      #   PROXIED=!is(example.org)          # all except example.org proxied
      #   PROXIED=is(a.org)||is(b.org)      # only a.org and b.org proxied
      # See https://pkg.go.dev/strconv#ParseBool for more advanced usage.
      PROXIED: is($DOMAIN)||is(*.$DOMAIN)

      # The time-to-live (TTL) (in seconds) of new DNS records.
      # Default is 1, meaning 'automatic' in CloudFlare.
      TTL: 1

      # The comment of new DNS records.
      RECORD_COMMENT: 'Updated by Cloudflare DDNS on server `$TS_HOSTNAME.$DOMAIN`'

      # üß™ The text description of new WAF lists.
      #WAF_LIST_DESCRIPTION: 

      # The Healthchecks ping URL to ping when the updater successfully updates
      # IP addresses, such as `https://hc-ping.com/<uuid>` or `https://hc-ping.com/<project-ping-key>/<name-slug>`.
      # ‚ö†Ô∏è The ping schedule should match the update schedule specified by UPDATE_CRON.
      # ü§ñ The updater can work with any server following the [same Healthchecks protocol](https://healthchecks.io/docs/http_api/)
      # This includes self-hosted instances of [Healthchecks](https://github.com/healthchecks/healthchecks).
      # Both UUID and Slug URLs are supported, and the updater works regardless whether the POST-only mode is enabled.
      #HEALTHCHECKS: 

      # The Uptime Kuma‚Äôs Push URL to ping when the updater successfully updates IP addresses, such as `https://<host>/push/<id>`.
      # You can directly copy the ‚ÄúPush URL‚Äù from the Uptime Kuma configuration page.
      # ‚ö†Ô∏è The ‚ÄúHeartbeat Interval‚Äù should match the update schedule specified by `UPDATE_CRON`.
      #UPTIMEKUMA: 'https://uptimekuma.$DOMAIN/push/1234567890'

      # Newline-separated [shoutrrr URLs](https://containrrr.dev/shoutrrr/latest/services/overview/) to which the updater sends notifications of IP address changes and other events.
      # Each shoutrrr URL represents a notification service; for example, discord://<token>@<id> means sending messages to Discord.
      # See https://github.com/containrrr/shoutrrr for more information.
      #SHOUTRRR: discord://$SHOUTRRR_DISCORD_TOKEN@$SHOUTRRR_DISCORD_CHANNEL_ID

      # The updater, by default, will attempt to update DNS records for both IPv4 and IPv6, and there is no
      # harm in leaving the automatic detection on even if your network does not work for one of them.
      # However, if you want to disable IPv6 entirely (perhaps to avoid seeing the detection errors), add IP6_PROVIDER=none.
      #IP6_PROVIDER: 'none'
    restart: always
  nginx-traefik-extensions:
    # üîπüîπ Nginx Authentication Middleware üîπüîπ
    # Acts as forwardAuth service for traefik middleware
    image: docker.io/nginx:alpine
    container_name: nginx-traefik-extensions
    hostname: nginx-traefik-extensions
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - backend
      - nginx_net
    configs:
      - source: nginx-traefik-extensions.conf
        target: /etc/nginx/nginx.conf
        mode: 0444
    volumes:
      - ${CONFIG_PATH:-./volumes}/traefik/nginx-middlewares/auth:/etc/nginx/auth:ro
#      - ${CONFIG_PATH:-./volumes}/traefik/nginx-middlewares/cache:/var/cache/nginx
#      - ${CONFIG_PATH:-./volumes}/traefik/nginx-middlewares/logs:/var/log/nginx
    environment:
      TZ: ${TZ:-America/Chicago}
      NGINX_ACCESS_LOG: /dev/stdout
      NGINX_ERROR_LOG: /dev/stderr
      NGINX_LOG_LEVEL: debug
    labels:
      # https://doc.traefik.io/traefik/reference/routing-configuration/http/middlewares/forwardauth/
      # 'nginx-auth@docker' traefik middleware
      traefik.http.middlewares.nginx-auth.forwardAuth.address: http://nginx-traefik-extensions:80/auth"
      traefik.http.middlewares.nginx-auth.forwardAuth.trustForwardHeader: true
      traefik.http.middlewares.nginx-auth.forwardAuth.authResponseHeaders: '["X-Auth-Method", "X-Auth-Passed", "X-Middleware-Name"]'
    cpus: 1
    mem_limit: 1G
    mem_reservation: 6M
    healthcheck:
      # test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:80/health > /dev/null 2>&1 && wget -qO- https://127.0.0.1:443/health > /dev/null 2>&1 || exit 1"]
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:80/health > /dev/null 2>&1 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 20s
    restart: always
  tinyauth:
    # üîπüîπ TinyAuth üîπüîπ
    image: ghcr.io/steveiliop56/tinyauth:latest
    container_name: tinyauth
    hostname: auth
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - backend
      - publicnet
    secrets:
      - tinyauth-secret
      - tinyauth-google-client-secret
      - tinyauth-github-client-secret
    expose:
      - 3000
    volumes:
      - ${CONFIG_PATH:-./volumes}/traefik/tinyauth:/data
    environment:
      SECRET_FILE: /run/secrets/tinyauth-secret
      APP_URL: ${TINYAUTH_APP_URL:-https://auth.$DOMAIN}
      USERS: $TINYAUTH_USERS
      # Google OAuth Configuration
      GOOGLE_CLIENT_ID: ${TINYAUTH_GOOGLE_CLIENT_ID:?}
      GOOGLE_CLIENT_SECRET_FILE: /run/secrets/tinyauth-google-client-secret
      # GitHub OAuth Configuration
      GITHUB_CLIENT_ID: ${TINYAUTH_GITHUB_CLIENT_ID:?}
      GITHUB_CLIENT_SECRET_FILE: /run/secrets/tinyauth-github-client-secret
      # Additional recommended settings
      SESSION_EXPIRY: ${TINYAUTH_SESSION_EXPIRY:-604800} # 2 weeks session expiry
      COOKIE_SECURE: ${TINYAUTH_COOKIE_SECURE:-true} # Send cookie only with HTTPS
      APP_TITLE: ${TINYAUTH_APP_TITLE:-$DOMAIN} # Customize login page title
      LOGIN_MAX_RETRIES: ${TINYAUTH_LOGIN_MAX_RETRIES:-15} # Maximum login attempts before account lockout
      LOGIN_TIMEOUT: ${TINYAUTH_LOGIN_TIMEOUT:-300} # Lock account for 5 minutes after too many failed attempts
      OAUTH_AUTO_REDIRECT: ${TINYAUTH_OAUTH_AUTO_REDIRECT:-none} # Options: none, github, google, or generic
      # Uncomment below if you want to restrict OAuth login to specific users
      OAUTH_WHITELIST: $TINYAUTH_OAUTH_WHITELIST  # e.g. user1,user2,/^admin.>
    labels:
      traefik.enable: true
      traefik.http.services.tinyauth.loadbalancer.server.port: 3000
      traefik.http.routers.tinyauth.rule: Host(`auth.$DOMAIN`) || Host(`auth.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.middlewares.tinyauth.forwardAuth.address: http://auth:3000/api/auth/traefik
      homepage.group: Security
      homepage.name: TinyAuth
      homepage.icon: https://tinyauth.app/img/logo.png
      homepage.href: https://auth.$DOMAIN/
      homepage.description: Centralized login service (email, Google, GitHub) used by Traefik auth middleware and apps
      kuma.tinyauth.http.name: auth.$TS_HOSTNAME.$DOMAIN
      kuma.tinyauth.http.url: https://auth.$DOMAIN
      kuma.tinyauth.http.interval: 60
    restart: always
  crowdsec:
    # üîπüîπ CrowdSec üîπüîπ
    # Highly recommend this guide: https://blog.lrvt.de/configuring-crowdsec-with-traefik/
    image: docker.io/crowdsecurity/crowdsec:latest
    container_name: crowdsec
    hostname: crowdsec
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - backend
      - default
    expose:
      - ${CROWDSEC_LAPI_PORT:-8080}    # HTTP API for bouncers
      - ${CROWDSEC_APPSEC_PORT:-7422}  # AppSec WAF endpoint
      - ${PROMETHEUS_PORT:-6060}       # Metrics endpoint for prometheus
    environment:
      UID: ${PUID:-1001}
      GID: ${PGID:-999}
      COLLECTIONS: crowdsecurity/appsec-crs crowdsecurity/appsec-generic-rules crowdsecurity/appsec-virtual-patching crowdsecurity/whitelist-good-actors crowdsecurity/base-http-scenarios crowdsecurity/http-cve crowdsecurity/linux crowdsecurity/sshd
    configs:
      # CrowdSec files are typically access restricted (644) by the root user.
      # If your log files are stored onto an NFS share, you may want to use poll_without_inotify: true for each log source as outlined by CrowdSec here.
      # https://docs.crowdsec.net/docs/data_sources/file/?ref=blog.lrvt.de#poll_without_inotify
      - source: crowdsec-config.yaml
        target: /etc/crowdsec/config.yaml
        mode: 0644
      - source: crowdsec-acquis.yaml
        target: /etc/crowdsec/acquis.yaml
        mode: 0644
      - source: crowdsec-profiles.yaml
        target: /etc/crowdsec/profiles.yaml
        mode: 0644
      # docker exec crowdsec cscli notifications list
      - source: crowdsec-victoriametrics.yaml
        target: /etc/crowdsec/notifications/victoriametrics.yaml
        mode: 0644
      - source: crowdsec-email.yaml       # docker exec crowdsec cscli notifications test email_default
        target: /etc/crowdsec/email.yaml
        mode: 0644
      - source: crowdsec-file.yaml        # docker exec crowdsec cscli notifications test file_default
        target: /etc/crowdsec/file.yaml
        mode: 0644
      - source: crowdsec-http.yaml        # docker exec crowdsec cscli notifications test http_default
        target: /etc/crowdsec/http.yaml
        mode: 0644
      - source: crowdsec-sentinel.yaml    # docker exec crowdsec cscli notifications test sentinel_default
        target: /etc/crowdsec/sentinel.yaml
        mode: 0644
      - source: crowdsec-slack.yaml       # docker exec crowdsec cscli notifications test slack_default
        target: /etc/crowdsec/http.yaml
        mode: 0644
      - source: crowdsec-splunk.yaml      # docker exec crowdsec cscli notifications test splunk_default
        target: /etc/crowdsec/http.yaml
        mode: 0644
      # Log bind mounts into crowdsec
      - source: crowdsec-auth.log
        target: /var/log/auth.log
        mode: 0644
      - source: crowdsec-syslog
        target: /var/log/syslog
        mode: 0644
    volumes:
      # CrowdSec container data
      - ${CONFIG_PATH:-./volumes}/traefik/crowdsec/data:/var/lib/crowdsec/data:rw
      - ${CONFIG_PATH:-./volumes}/traefik/crowdsec/etc/crowdsec:/etc/crowdsec:rw
      - ${CONFIG_PATH:-./volumes}/traefik/crowdsec/plugins:/usr/local/lib/crowdsec/plugins:rw
      # Log bind mounts into crowdsec
      - ${CONFIG_PATH:-./volumes}/traefik/logs:/var/log/traefik:ro
    healthcheck:
      test: ["CMD-SHELL", "cscli version"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
    restart: always
  traefik:
    # üîπüîπ Traefik üîπüîπ
    # https://doc.traefik.io
    depends_on:
      crowdsec:
        condition: service_started
      dockerproxy-ro:
        condition: service_healthy
    image: docker.io/traefik:latest
    container_name: traefik
    hostname: traefik
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - default
      - nginx_net
      - publicnet
    secrets:
      - cloudflare-api-key
    expose:
      - 8080          # Dashboard API
    ports:
      - 80:80         # HTTP
      - 443:443       # HTTPS
      - 443:443/udp   # HTTPS/3
    #  - 853:853       # DNS over TLS
    #  - 3000:3000     # Ping
    cap_add:
      - NET_ADMIN
    volumes:
    #  - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro
      - ${CONFIG_PATH:-./volumes}/traefik/dynamic:/traefik/dynamic
      - ${CONFIG_PATH:-./volumes}/traefik/certs:/certs
      - ${CONFIG_PATH:-./volumes}/traefik/plugins-local:/plugins-local
      - ${CONFIG_PATH:-./volumes}/traefik/logs:/var/log/traefik:rw
    #      - /etc/localtime:/etc/localtime:ro
    configs:
      - source: traefik-dynamic.yaml
        target: /traefik/dynamic/core.yaml
        mode: 0644
    environment:
      DOCKER_HOST: tcp://dockerproxy-ro:2375
      LETS_ENCRYPT_EMAIL: ${ACME_RESOLVER_EMAIL:?}
      CLOUDFLARE_EMAIL: ${CLOUDFLARE_EMAIL:?}
      CLOUDFLARE_API_KEY_FILE: /run/secrets/cloudflare-api-key
      CLOUDFLARE_ZONE_ID: ${CLOUDFLARE_ZONE_ID:?}
    command:
      - '--accessLog=true'
      - '--accessLog.bufferingSize=0'
      - '--accessLog.fields.headers.defaultMode=drop'
      - '--accessLog.fields.headers.names.User-Agent=keep'
      - '--accessLog.fields.names.StartUTC=drop'
      - '--accessLog.filePath=/var/log/traefik/traefik.log'
      - '--accessLog.filters.statusCodes=100-999'
      - '--accessLog.format=json'
      - '--metrics.prometheus.buckets=0.1,0.3,1.2,5.0'
      - '--api.dashboard=true'
      - '--api.debug=true'
      - '--api.disableDashboardAd=true'
      - '--api.insecure=false'
      - '--api=true'
      # use staging for testing/development/debugging to avoid rate limiting
      #- '--certificatesResolvers.letsencrypt.acme.caServer=https://acme-staging-v02.api.letsencrypt.org/directory'
      - '--certificatesResolvers.letsencrypt.acme.caServer=${TRAEFIK_CA_SERVER:-https://acme-v02.api.letsencrypt.org/directory}'
      - '--certificatesResolvers.letsencrypt.acme.dnsChallenge=${TRAEFIK_DNS_CHALLENGE:-true}'
      - '--certificatesResolvers.letsencrypt.acme.dnsChallenge.provider=cloudflare'
      - '--certificatesResolvers.letsencrypt.acme.dnsChallenge.resolvers=${TRAEFIK_DNS_RESOLVERS:-1.1.1.1,1.0.0.1}'
      - '--certificatesResolvers.letsencrypt.acme.email=${ACME_RESOLVER_EMAIL:?}'
      - '--certificatesResolvers.letsencrypt.acme.httpChallenge=${TRAEFIK_HTTP_CHALLENGE:-false}'
      - '--certificatesResolvers.letsencrypt.acme.httpChallenge.entryPoint=web'
      - '--certificatesResolvers.letsencrypt.acme.tlsChallenge=${TRAEFIK_TLS_CHALLENGE:-false}'
      - '--certificatesResolvers.letsencrypt.acme.storage=/certs/acme.json'
      #- '--certificatesResolvers.letsencrypt.acme.dnsChallenge.propagation.disableChecks=true'
      #- '--certificatesResolvers.letsencrypt.acme.dnsChallenge.propagation.delayBeforeChecks=60'
      #- '--entryPoints.dot.address=:853'
      #- '--entryPoints.ping.address=:3000'
      - '--entryPoints.web.address=:80'
      # Redirect HTTP to HTTPS
      - '--entryPoints.web.http.redirections.entryPoint.scheme=https'
      - '--entryPoints.web.http.redirections.entryPoint.to=websecure'
      # Ensures that the Traefik logs obtain the correct IP address of site visitors, defined in CloudFlare's `CF_CONNECTING_IP` HTTP header.
      - '--entryPoints.web.forwardedHeaders.trustedIPs=103.21.244.0/22,103.22.200.0/22,103.31.4.0/22,104.16.0.0/13,104.24.0.0/14,108.162.192.0/18,131.0.72.0/22,141.101.64.0/18,162.158.0.0/15,172.64.0.0/13,173.245.48.0/20,188.114.96.0/20,190.93.240.0/20,197.234.240.0/22,198.41.128.0/17,2400:cb00::/32,2405:8100::/32,2405:b500::/32,2606:4700::/32,2803:f800::/32,2a06:98c0::/29,2c0f:f248::/32'
      - '--entryPoints.websecure.forwardedHeaders.trustedIPs=103.21.244.0/22,103.22.200.0/22,103.31.4.0/22,104.16.0.0/13,104.24.0.0/14,108.162.192.0/18,131.0.72.0/22,141.101.64.0/18,162.158.0.0/15,172.64.0.0/13,173.245.48.0/20,188.114.96.0/20,190.93.240.0/20,197.234.240.0/22,198.41.128.0/17,2400:cb00::/32,2405:8100::/32,2405:b500::/32,2606:4700::/32,2803:f800::/32,2a06:98c0::/29,2c0f:f248::/32'
      - '--entryPoints.websecure.address=:443'
      - '--entryPoints.websecure.http.encodeQuerySemiColons=true'
      - '--entryPoints.websecure.http.middlewares=bolabaden-error-pages@file,crowdsec@file,strip-www@file'
      - '--entryPoints.websecure.http.tls=true'
      - '--entryPoints.websecure.http.tls.certResolver=letsencrypt'
      - '--entryPoints.websecure.http.tls.domains[0].main=$DOMAIN'
      - '--entryPoints.websecure.http.tls.domains[0].sans=www.$DOMAIN,*.$DOMAIN,*.$TS_HOSTNAME.$DOMAIN'
      - '--entryPoints.websecure.http2.maxConcurrentStreams=100'
      - '--entryPoints.websecure.http3'
      - '--global.checkNewVersion=true'
      - '--global.sendAnonymousUsage=false'
      - '--log.level=INFO'
      - '--ping=true'
      - '--providers.docker=true'
      - '--providers.docker.endpoint=tcp://dockerproxy-ro:2375'
      - '--providers.docker.network=${STACK_NAME:?}_publicnet'
      - '--providers.docker.defaultRule=Host(`{{ normalize .ContainerName }}.$DOMAIN`) || Host(`{{ normalize .Name }}.$DOMAIN`) || Host(`{{ normalize .ContainerName }}.$TS_HOSTNAME.$DOMAIN`) || Host(`{{ normalize .Name }}.$TS_HOSTNAME.$DOMAIN`)'
      - '--providers.docker.exposedByDefault=false'
      - '--providers.file.directory=/traefik/dynamic/'
      - '--providers.file.watch=true'
      - '--experimental.plugins.bouncer.modulename=github.com/maxlerebourg/crowdsec-bouncer-traefik-plugin'
      - '--experimental.plugins.bouncer.version=v1.4.5'
      - '--experimental.plugins.traefikerrorreplace.modulename=github.com/PseudoResonance/traefikerrorreplace'
      - '--experimental.plugins.traefikerrorreplace.version=v1.0.1'
      # TLS-specific option within the ServersTransport configuration.
      # When set to true, it disables the verification of the backend server's TLS certificate chain and hostname.
      # By default, this option is false, meaning Traefik would validate the certificate to ensure it is issued by a trusted certificate authority (CA), has not expired, and matches the expected hostname.
      # Setting it to true bypasses these checks, allowing connections to proceed even with self-signed, expired, or mismatched certificates.
      # This is useful for testing or when you need to connect to services that use self-signed certificates.
      # however, in our configuration, this is REQUIRED to be true (avoids errors like 'ERR 500 Internal Server Error error="tls: failed to verify certificate: x509: cannot validate certificate for 10.76.0.2 because it doesn't contain any IP SANs"') in stremio
      - '--serversTransport.insecureSkipVerify=true'
    labels:
      traefik.enable: true
      traefik.http.routers.traefik.service: api@internal
      traefik.http.routers.traefik.rule: Host(`traefik.$DOMAIN`) || Host(`traefik.$TS_HOSTNAME.$DOMAIN`)
      traefik.http.services.traefik.loadbalancer.server.port: 8080
#      traefik.http.middlewares.legalbar.plugin.bodyreplace.replacements[0].search: </body>
#      traefik.http.middlewares.legalbar.plugin.bodyreplace.replacements[0].replace: <div style='position:fixed;bottom:0;width:100%;background:#222;color:#fff;text-align:center;padding:6px;font-size:14px;z-index:9999;'><a href="https://$DOMAIN/privacy-policy" style="color:#fff;margin:0 10px;">Privacy Policy</a> | <a href="https://$DOMAIN/terms" style="color:#fff;margin:0 10px;">Terms</a> | <a href="https://$DOMAIN/cookies-policy" style="color:#fff;margin:0 10px;">Cookies Policy</a> ‚Äî ¬© $DOMAIN</div></body>
      homepage.group: Infrastructure
      homepage.name: Traefik
      homepage.icon: traefik.png
      homepage.href: https://traefik.$DOMAIN/dashboard
      homepage.widget.type: traefik
      homepage.description: Reverse proxy entrypoint for all services with TLS, Cloudflare integration, and auth middleware
      kuma.traefik.http.name: traefik.$TS_HOSTNAME.$DOMAIN
      kuma.traefik.http.url: https://traefik.$DOMAIN/dashboard
      kuma.traefik.http.interval: 20
    healthcheck:
      test: ["CMD-SHELL", "traefik healthcheck --ping"]
      interval: 10s
      timeout: 3s
      retries: 3
      start_period: 10s
    restart: always
  whoami:
    # üîπüîπ Whoami üîπüîπ
    image: docker.io/traefik/whoami:latest
    container_name: whoami
    hostname: whoami
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - backend
      - publicnet
    expose:
      - 80
    labels:
      traefik.enable: true
      traefik.http.routers.whoami.service: whoami@docker
      traefik.http.services.whoami.loadBalancer.server.port: 80
#      traefik.http.routers.whoami.middlewares: legalbar@file
#      traefik.http.middlewares.legalbar.plugin.bodyreplace.replacements[0].search: </body>
#      traefik.http.middlewares.legalbar.plugin.bodyreplace.replacements[0].replace: <div style='position:fixed;bottom:0;width:100%;background:#222;color:#fff;text-align:center;padding:6px;font-size:14px;z-index:9999;'><a href="https://$DOMAIN/privacy-policy" style="color:#fff;margin:0 10px;">Privacy Policy</a> | <a href="https://$DOMAIN/terms" style="color:#fff;margin:0 10px;">Terms</a> | <a href="https://$DOMAIN/cookies-policy" style="color:#fff;margin:0 10px;">Cookies Policy</a> ‚Äî ¬© $DOMAIN</div></body>
      homepage.group: Web Services
      homepage.name: whoami
      homepage.icon: whoami.png
      homepage.href: https://whoami.$DOMAIN
      homepage.description: Request echo service used to verify reverse-proxy, headers, and auth middleware
      kuma.whoami.http.name: whoami.$TS_HOSTNAME.$DOMAIN
      kuma.whoami.http.url: https://whoami.$DOMAIN
      kuma.whoami.http.interval: 60
    restart: always
  docker-gen-failover:
    image: docker.io/nginxproxy/docker-gen
    container_name: docker-gen-failover
    networks:
      - backend
      - default
    configs:
      - source: traefik-failover-dynamic.conf.tmpl
        target: /templates/traefik-failover-dynamic.conf.tmpl
        mode: 0400
    volumes:
    #  - ${DOCKER_SOCKET:-/var/run/docker.sock}:/tmp/docker.sock:ro
      - ${CONFIG_PATH:-./volumes}/traefik/dynamic:/traefik/dynamic
    command: |
      -endpoint tcp://dockerproxy-rw:2375
      -only-exposed
      -include-stopped
      -event-filter event=start
      -event-filter event=create
      -event-filter event=expose
      -event-filter event=update
      -event-filter event=connect
      -event-filter label=traefik.enable=true
      -container-filter label=traefik.enable=true
      -watch /templates/traefik-failover-dynamic.conf.tmpl /traefik/dynamic/failover-fallbacks.yaml
    restart: no
  logrotate-traefik:
    # üîπüîπ Logrotate for Traefik logs üîπüîπ
    build:
      context: ${SRC_PATH:-./projects}/reverse_proxy/logrotate-traefik
      dockerfile: Dockerfile
    image: docker.io/bolabaden/logrotate-traefik
    container_name: logrotate-traefik
    network_mode: none
    volumes:
      - ${CONFIG_PATH:-./volumes}/traefik/logs:/var/log/traefik
    environment:
      TZ: ${TZ:-America/Chicago}
      # All other environment variables use container defaults
      # Override as needed:
      # LOG_LEVEL: info
      # LOGROTATE_MAXSIZE: 10M
      # LOGROTATE_MAXCOUNT: 20
      # STATUS_CODES: 100-999
    cpus: 0.1
    mem_limit: 64M
    mem_reservation: 8M
    restart: always
  autokuma:
    image: ghcr.io/bigboot/autokuma
    container_name: autokuma
    hostname: autokuma
    extra_hosts:
      - host.docker.internal:host-gateway
    networks:
      - backend
    volumes:
      - ${DOCKER_SOCKET:-/var/run/docker.sock}:/var/run/docker.sock:ro  # Read-only access to Docker
    environment:
      # --- Uptime Kuma connection settings ---
      AUTOKUMA__KUMA__URL: https://uptimekuma.$DOMAIN  # kuma.url: The URL AutoKuma should use to connect to Uptime Kuma
      AUTOKUMA__KUMA__USERNAME: ${AUTOKUMA__KUMA__USERNAME:-admin}  # kuma.username: The username for logging into Uptime Kuma (required unless auth is disabled)
      AUTOKUMA__KUMA__PASSWORD: ${AUTOKUMA__KUMA__PASSWORD:?}  # kuma.password: The password for logging into Uptime Kuma (required unless auth is disabled)
      AUTOKUMA__KUMA__CALL_TIMEOUT: ${AUTOKUMA__KUMA__CALL_TIMEOUT:-5}  # kuma.call_timeout: The timeout for executing calls to the Uptime Kuma server
      AUTOKUMA__KUMA__CONNECT_TIMEOUT: ${AUTOKUMA__KUMA__CONNECT_TIMEOUT:-5}  # kuma.connect_timeout: The timeout for the initial connection to Uptime Kuma
    restart: always
